This chapter is an entension of chapter 6 regarding decision trees. In this chapter we learn about random forests, ensemble learning, bagging methods, boosting methods and stacking trees as well. Ensemble learning is the ability to train a classifier by using the predictions of multiple classifiers (SVMs, Logistic Regression, Decision Trees, etc.) and using these to predict the class that gets the most votes in the form of *hard voting* classification. Bagging is the process of random sampling with replacement for a predictor in a model and then simply training the predictor with different random subsets of the training set. There is a section on how this can be implemented in scikit-learn too. Random forests is an unsupervised learning algorithm with the purpose of solving classification problems. Typically, random forests are ensembles of decision trees using a bagging method to achieve the end goal of the model. The section on boosting in this chapter talks about how to improve ensemble learning but combining many weak learners into a strong one. One boosting algorithm is Adaboost which starts by training a base decision tree, then increasing the weights of misclassified instances. From here it then trains a second classifier, updates the weights and continues this process for however many iterations. Finally, the section on stacking in this chapter talks about using a model to aggregate predictions in a blender instead of using a function like hard voting to achieve higher accuracy on predicted values. 
The jupyter notebook in this section goes in more depth on the topics discussed above. 
