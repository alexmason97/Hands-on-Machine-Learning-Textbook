This chapter is a deeper dive into creating Deep Neural Network and goes over a broad list of categories of topics for Deep NNs. The chapter opens with discussing a common issue with neural networks: The vanishing/exploding gradients problem where a network cannot diverge or the loss on the model just explodes as a result of an "Exploding Gradient." We then talk about different activation functions like softmax and tanh and even about one of the more useful techniques between layers: Batch Normalization. We discuss a wide range of optimizers like AdaGrad, RMSProp and Adam/Nadam optimizers and even do some examples on transfer learning from other common larger models and simply hiding the top layers, training with the new layers you are adding and they retraining the top layers of the model. Finally, we talk about other regularization methods like Dropout, l1 & l2 regularizatoin and Max-Norm Regularization.
More details on the content in this chapter can be found in the jupyter notebook for this chapter. 
