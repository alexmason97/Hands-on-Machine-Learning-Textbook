{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "413fb0ae",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf27202",
   "metadata": {},
   "source": [
    "### Ensure GPU Env is working correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e30c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efef50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a090ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "879b27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d87b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd805c89",
   "metadata": {},
   "source": [
    "## Perceptrons "
   ]
  },
  {
   "attachments": {
    "perceptron.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "f9914835",
   "metadata": {},
   "source": [
    "The perceptron is a the simplest part of a neural network. it is linear machine learning algorithm for binary classification tasks. An example of a perceptron is shown below: ![perceptron.png](attachment:perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305da15",
   "metadata": {},
   "source": [
    "We will now use sklearn to implement a perceptron to classifier the iris dataset with petal length and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0730c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length and width\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron(max_iter=1000, tol=1e-2)\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b74901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "347441b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "a = -per_clf.coef_[0][0] / per_clf.coef_[0][1]\n",
    "b = -per_clf.intercept_ / per_clf.coef_[0][1]\n",
    "axes = [0, 5, 0, 2]\n",
    "\n",
    "x0, x1 = np.meshgrid(\n",
    "        np.linspace(axes[0], axes[1], 1000).reshape(-1, 1),\n",
    "        np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
    "    )\n",
    "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
    "y_predict = per_clf.predict(X_new)\n",
    "zz = y_predict.reshape(x0.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(X[y==0, 0], X[y==0, 1], \"bs\", label=\"Not Iris-Setosa\")\n",
    "plt.plot(X[y==1, 0], X[y==1, 1], \"yo\", label=\"Iris-Setosa\")\n",
    "\n",
    "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\", linewidth=3)\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
    "\n",
    "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=14)\n",
    "plt.axis(axes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc1fbf",
   "metadata": {},
   "source": [
    "## Plots of Different Activation Functions in Neural Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for neural nets\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative_of_func(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56e7e1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth = 1, label = \"Step\")\n",
    "plt.plot(z, sigmoid(z), \"g--\", linewidth = 2, label = \"Sigmoid\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth = 2, label = \"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth = 2, label = \"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation Functions\", fontsize=15)\n",
    "plt.axis([-5, 5, -1.5, 1.5])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative_of_func(np.sign, z), \"r-\", linewidth=1, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative_of_func(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
    "plt.plot(z, derivative_of_func(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative_of_func(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573ae183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heaviside(z):\n",
    "    return (z >= 0).astype(z.dtype)\n",
    "\n",
    "def mlp_xor(x1, x2, activation=heaviside):\n",
    "    return activation(-activation(x1 + x2 - 1.5) + activation(x1 + x2 - 0.5) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9746d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1s = np.linspace(-0.2, 1.2, 100)\n",
    "x2s = np.linspace(-0.2, 1.2, 100)\n",
    "x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "z1 = mlp_xor(x1, x2, activation=heaviside)\n",
    "z2 = mlp_xor(x1, x2, activation=sigmoid)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(x1, x2, z1)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: heaviside\", fontsize=14)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(x1, x2, z2)\n",
    "plt.plot([0, 1], [0, 1], \"gs\", markersize=20)\n",
    "plt.plot([0, 1], [1, 0], \"y^\", markersize=20)\n",
    "plt.title(\"Activation function: sigmoid\", fontsize=14)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff997e39",
   "metadata": {},
   "source": [
    "## Building an Image Classifier in Keras Shell in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbd4cb",
   "metadata": {},
   "source": [
    "We will build an image classifer using a different dataset. This one is from keras and is a fashion dataset with images split in pixels accordingly based on the piece of clothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa7723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cda51d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of dataset\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dce09886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21409fcb",
   "metadata": {},
   "source": [
    "We'll do a train-val-test split now and then divide our values by 255 to scale the pixel ratios as they are 255 pixel images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ecb5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80569922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18e9a4",
   "metadata": {},
   "source": [
    "The lables are the class IDs (represented as uint8), from 0 to 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a4fedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 7, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c2239a",
   "metadata": {},
   "source": [
    "Now we need to create corresponding class names for this set, so lets do that in an array right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3a8d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37263b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e6431f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Shapes of Validation and Test sets\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a156c455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 50 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 5\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2897222",
   "metadata": {},
   "source": [
    "Now time to build the model. We'll create a sequential model with a first layer to convert each input image to a 1D array. Then, we will follow it up with 2 Dense layers with relu activatoins functions and finally a output dense layer with 10 neurons with a softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82c049e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "443b7c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1cf6d5c3358>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cf7e4a4ef0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cf7e4a4e80>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cf787fc588>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers # model layers array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b524b44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5916f6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e8d31",
   "metadata": {},
   "source": [
    "The reason it shows question marks is because we didn't set batch sizes for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d215b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "True\n",
      "[[-0.04072303  0.02818586 -0.05906986 ... -0.07144554  0.04997285\n",
      "   0.01521179]\n",
      " [-0.03956556 -0.02049073 -0.04370243 ... -0.03947478  0.04447182\n",
      "   0.05297169]\n",
      " [-0.04291168 -0.00898109  0.06424204 ...  0.04203054 -0.07276922\n",
      "  -0.00648141]\n",
      " ...\n",
      " [-0.00180912  0.00553776 -0.04868077 ... -0.06696551  0.07227893\n",
      "   0.04407913]\n",
      " [ 0.03341646  0.02270722  0.03876131 ...  0.03101095 -0.00564861\n",
      "   0.0074928 ]\n",
      " [ 0.04312358  0.04764709  0.04154848 ...  0.036351   -0.01424\n",
      "   0.04911801]]\n"
     ]
    }
   ],
   "source": [
    "# Some other things that can be viewed\n",
    "hidden1 = model.layers[1]\n",
    "print(hidden1.name)\n",
    "print(model.get_layer(hidden1.name) is hidden1)\n",
    "weights, biases = hidden1.get_weights()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32830416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a95aaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df88d2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1cd290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb5063",
   "metadata": {},
   "source": [
    "The above code is equal to the following: \n",
    "\n",
    "```python\n",
    "model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=[keras.metrics.sparse_categorical_accuracy])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50765145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 57s 1ms/sample - loss: 0.7314 - accuracy: 0.7543 - val_loss: 0.5194 - val_accuracy: 0.8254\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.4909 - accuracy: 0.8296 - val_loss: 0.4763 - val_accuracy: 0.8332\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.4449 - accuracy: 0.8437 - val_loss: 0.4307 - val_accuracy: 0.8536\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.4159 - accuracy: 0.8547 - val_loss: 0.3960 - val_accuracy: 0.8662\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3946 - accuracy: 0.8625 - val_loss: 0.3899 - val_accuracy: 0.8606\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3777 - accuracy: 0.8661 - val_loss: 0.3863 - val_accuracy: 0.8632\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3658 - accuracy: 0.8697 - val_loss: 0.3656 - val_accuracy: 0.8740\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.3527 - accuracy: 0.8747 - val_loss: 0.3846 - val_accuracy: 0.8658\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.3427 - accuracy: 0.8783 - val_loss: 0.3494 - val_accuracy: 0.8734\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3335 - accuracy: 0.8809 - val_loss: 0.3584 - val_accuracy: 0.8702\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.3255 - accuracy: 0.8833 - val_loss: 0.3367 - val_accuracy: 0.8774\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.3185 - accuracy: 0.8860 - val_loss: 0.3414 - val_accuracy: 0.8754\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.3098 - accuracy: 0.8885 - val_loss: 0.3330 - val_accuracy: 0.8776\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.3027 - accuracy: 0.8911 - val_loss: 0.3410 - val_accuracy: 0.8766\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.2966 - accuracy: 0.8934 - val_loss: 0.3262 - val_accuracy: 0.8834\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.2908 - accuracy: 0.8950 - val_loss: 0.3243 - val_accuracy: 0.8848\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 3s 55us/sample - loss: 0.2837 - accuracy: 0.8979 - val_loss: 0.3236 - val_accuracy: 0.8854\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 3s 54us/sample - loss: 0.2801 - accuracy: 0.8980 - val_loss: 0.3339 - val_accuracy: 0.8792\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.2730 - accuracy: 0.9018 - val_loss: 0.3212 - val_accuracy: 0.8832\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.2687 - accuracy: 0.9034 - val_loss: 0.3112 - val_accuracy: 0.8888\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.2632 - accuracy: 0.9062 - val_loss: 0.3048 - val_accuracy: 0.8914\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 3s 58us/sample - loss: 0.2585 - accuracy: 0.9063 - val_loss: 0.3343 - val_accuracy: 0.8780\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.2537 - accuracy: 0.9083 - val_loss: 0.3146 - val_accuracy: 0.8860\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.2495 - accuracy: 0.9105 - val_loss: 0.3000 - val_accuracy: 0.8902\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 3s 56us/sample - loss: 0.2458 - accuracy: 0.9121 - val_loss: 0.3089 - val_accuracy: 0.8862\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.2418 - accuracy: 0.9122 - val_loss: 0.2923 - val_accuracy: 0.8914\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 3s 60us/sample - loss: 0.2366 - accuracy: 0.9144 - val_loss: 0.3007 - val_accuracy: 0.8880\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 3s 57us/sample - loss: 0.2340 - accuracy: 0.9158 - val_loss: 0.3002 - val_accuracy: 0.8944\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 3s 53us/sample - loss: 0.2289 - accuracy: 0.9174 - val_loss: 0.3022 - val_accuracy: 0.8928\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 3s 59us/sample - loss: 0.2258 - accuracy: 0.9190 - val_loss: 0.2960 - val_accuracy: 0.8938\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fe3f8e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'epochs': 30,\n",
       " 'steps': 1719,\n",
       " 'samples': 55000,\n",
       " 'verbose': 0,\n",
       " 'do_validation': True,\n",
       " 'metrics': ['loss', 'accuracy', 'val_loss', 'val_accuracy']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a51c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "print(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e8af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64777058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c39dcb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.3356 - accuracy: 0.8843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.335556380045414, 0.8843]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f53662ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9dde764f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c512e910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbccd2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x172.8 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7.2, 2.4))\n",
    "for index, image in enumerate(X_new):\n",
    "    plt.subplot(1, 3, index + 1)\n",
    "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.axis('off')\n",
    "    plt.title(class_names[y_test[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f90b6",
   "metadata": {},
   "source": [
    "## Building a Regression MLP (multilayer perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1cac10",
   "metadata": {},
   "source": [
    "Now we will build a regression neural net with the california housing dataset from scikit learn with a keras sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83a810e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b7acf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/25\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 2.2280 - val_loss: 1.9954\n",
      "Epoch 2/25\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.7575 - val_loss: 0.6885\n",
      "Epoch 3/25\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.6750 - val_loss: 0.6646\n",
      "Epoch 4/25\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.6343 - val_loss: 0.6084\n",
      "Epoch 5/25\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.6001 - val_loss: 0.5848\n",
      "Epoch 6/25\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5707 - val_loss: 0.5912\n",
      "Epoch 7/25\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5463 - val_loss: 0.5428\n",
      "Epoch 8/25\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.5249 - val_loss: 0.5172\n",
      "Epoch 9/25\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.5074 - val_loss: 0.5006\n",
      "Epoch 10/25\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4926 - val_loss: 0.4625\n",
      "Epoch 11/25\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4799 - val_loss: 0.4502\n",
      "Epoch 12/25\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4698 - val_loss: 0.4435\n",
      "Epoch 13/25\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4612 - val_loss: 0.4320\n",
      "Epoch 14/25\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.4539 - val_loss: 0.4284\n",
      "Epoch 15/25\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4477 - val_loss: 0.4185\n",
      "Epoch 16/25\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4426 - val_loss: 0.4134\n",
      "Epoch 17/25\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4381 - val_loss: 0.4124\n",
      "Epoch 18/25\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4340 - val_loss: 0.4076\n",
      "Epoch 19/25\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4306 - val_loss: 0.4119\n",
      "Epoch 20/25\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4274 - val_loss: 0.4017\n",
      "Epoch 21/25\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4245 - val_loss: 0.4086\n",
      "Epoch 22/25\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4219 - val_loss: 0.3977\n",
      "Epoch 23/25\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4196 - val_loss: 0.4041\n",
      "Epoch 24/25\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4175 - val_loss: 0.3953\n",
      "Epoch 25/25\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4153 - val_loss: 0.3933\n",
      "5160/5160 [==============================] - 0s 34us/sample - loss: 0.4059\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=25, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2469566c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adb4ef3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5472129],\n",
       "       [1.6834185],\n",
       "       [3.4440737]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "005bb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 34us/sample - loss: 0.4059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40587604313857795"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33a721",
   "metadata": {},
   "source": [
    "## Functional API to Build a Wide & Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bde3892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "attachments": {
    "Wide_n_DeepNet.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "db7cdb77",
   "metadata": {},
   "source": [
    "The above code looks something like this: ![Wide_n_DeepNet.PNG](attachment:Wide_n_DeepNet.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9b47bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 30)           270         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           930         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 38)           0           input_1[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            39          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 1,239\n",
      "Trainable params: 1,239\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a47be968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 2.1009 - val_loss: 4.4723\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.7627 - val_loss: 1.2052\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.6644 - val_loss: 0.8405\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.6160 - val_loss: 0.5817\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5755 - val_loss: 0.5622\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.5471 - val_loss: 0.5304\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.5229 - val_loss: 0.4911\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.5033 - val_loss: 0.4691\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4873 - val_loss: 0.4706\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4733 - val_loss: 0.4464\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4618 - val_loss: 0.4402\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4516 - val_loss: 0.4341\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4425 - val_loss: 0.4390\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4348 - val_loss: 0.4147\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4280 - val_loss: 0.4207\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4222 - val_loss: 0.4325\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4170 - val_loss: 0.3957\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4125 - val_loss: 0.4581\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4089 - val_loss: 0.4130\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4054 - val_loss: 0.4344\n",
      "5160/5160 [==============================] - 0s 37us/sample - loss: 0.3935\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e514f2f",
   "metadata": {},
   "source": [
    "Now we will build a model with a subset of the features being passed through the wide path and a different subset in the deep path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c5fdfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e33bb3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 2.0597 - val_loss: 1.3105\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.8339 - val_loss: 1.0524\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.7042 - val_loss: 0.7389\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.6500 - val_loss: 0.6261\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.6137 - val_loss: 0.5757\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.5862 - val_loss: 0.5518\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.5630 - val_loss: 0.5347\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.5439 - val_loss: 0.5172\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.5271 - val_loss: 0.5004\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.5129 - val_loss: 0.4866\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.5007 - val_loss: 0.4741\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4906 - val_loss: 0.4567\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4816 - val_loss: 0.4474\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.4740 - val_loss: 0.4398\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.4674 - val_loss: 0.4326\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4618 - val_loss: 0.4294\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.4569 - val_loss: 0.4251\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.4524 - val_loss: 0.4239\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.4483 - val_loss: 0.4215\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4449 - val_loss: 0.4188\n",
      "5160/5160 [==============================] - 0s 36us/sample - loss: 0.4403\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "41f00eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44028191714323767"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test #performance so far"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a58cb9",
   "metadata": {},
   "source": [
    "### Multiple Outputs in a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8788085b",
   "metadata": {},
   "source": [
    "Now we will add extra outputs to a neural net. In this case, anauxiliary output for regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59835d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aeaf93",
   "metadata": {},
   "source": [
    "Now we will need to pass each output with its own loss function and loss weights. We will do this with a stochastic gradient descent optimizer and retrain the model with labels for each output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f5be83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "afa5fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 2s 133us/sample - loss: 2.7251 - main_output_loss: 2.5069 - aux_output_loss: 4.6844 - val_loss: 1.5818 - val_main_output_loss: 1.3529 - val_aux_output_loss: 3.6388\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 1s 86us/sample - loss: 1.0886 - main_output_loss: 0.8679 - aux_output_loss: 3.0728 - val_loss: 0.9003 - val_main_output_loss: 0.7288 - val_aux_output_loss: 2.4430\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.8339 - main_output_loss: 0.6786 - aux_output_loss: 2.2319 - val_loss: 0.7616 - val_main_output_loss: 0.6246 - val_aux_output_loss: 1.9929\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.7389 - main_output_loss: 0.6198 - aux_output_loss: 1.8087 - val_loss: 0.7172 - val_main_output_loss: 0.5983 - val_aux_output_loss: 1.7851\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.6869 - main_output_loss: 0.5875 - aux_output_loss: 1.5813 - val_loss: 0.6663 - val_main_output_loss: 0.5561 - val_aux_output_loss: 1.6570\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 1s 91us/sample - loss: 0.6525 - main_output_loss: 0.5636 - aux_output_loss: 1.4527 - val_loss: 0.6459 - val_main_output_loss: 0.5458 - val_aux_output_loss: 1.5450\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.6273 - main_output_loss: 0.5448 - aux_output_loss: 1.3692 - val_loss: 0.6189 - val_main_output_loss: 0.5268 - val_aux_output_loss: 1.4457\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 1s 86us/sample - loss: 0.6074 - main_output_loss: 0.5288 - aux_output_loss: 1.3135 - val_loss: 0.5977 - val_main_output_loss: 0.5119 - val_aux_output_loss: 1.3683\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 1s 88us/sample - loss: 0.5912 - main_output_loss: 0.5156 - aux_output_loss: 1.2719 - val_loss: 0.5711 - val_main_output_loss: 0.4890 - val_aux_output_loss: 1.3081\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 1s 92us/sample - loss: 0.5782 - main_output_loss: 0.5051 - aux_output_loss: 1.2379 - val_loss: 0.5524 - val_main_output_loss: 0.4740 - val_aux_output_loss: 1.2564\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 1s 97us/sample - loss: 0.5665 - main_output_loss: 0.4950 - aux_output_loss: 1.2092 - val_loss: 0.5378 - val_main_output_loss: 0.4625 - val_aux_output_loss: 1.2138\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.5564 - main_output_loss: 0.4870 - aux_output_loss: 1.1833 - val_loss: 0.5247 - val_main_output_loss: 0.4518 - val_aux_output_loss: 1.1798\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.5475 - main_output_loss: 0.4794 - aux_output_loss: 1.1597 - val_loss: 0.5166 - val_main_output_loss: 0.4461 - val_aux_output_loss: 1.1494\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.5396 - main_output_loss: 0.4732 - aux_output_loss: 1.1380 - val_loss: 0.5080 - val_main_output_loss: 0.4394 - val_aux_output_loss: 1.1241\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.5325 - main_output_loss: 0.4675 - aux_output_loss: 1.1170 - val_loss: 0.4990 - val_main_output_loss: 0.4319 - val_aux_output_loss: 1.1015\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 0.5259 - main_output_loss: 0.4627 - aux_output_loss: 1.0969 - val_loss: 0.4931 - val_main_output_loss: 0.4276 - val_aux_output_loss: 1.0817\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.5197 - main_output_loss: 0.4581 - aux_output_loss: 1.0773 - val_loss: 0.4873 - val_main_output_loss: 0.4232 - val_aux_output_loss: 1.0629\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.5142 - main_output_loss: 0.4537 - aux_output_loss: 1.0573 - val_loss: 0.4827 - val_main_output_loss: 0.4200 - val_aux_output_loss: 1.0460\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 1s 91us/sample - loss: 0.5089 - main_output_loss: 0.4500 - aux_output_loss: 1.0395 - val_loss: 0.4764 - val_main_output_loss: 0.4150 - val_aux_output_loss: 1.0274\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 1s 93us/sample - loss: 0.5035 - main_output_loss: 0.4459 - aux_output_loss: 1.0212 - val_loss: 0.4721 - val_main_output_loss: 0.4119 - val_aux_output_loss: 1.0126\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92acd1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 45us/sample - loss: 0.4947 - main_output_loss: 0.4391 - aux_output_loss: 0.9991\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74eda2b",
   "metadata": {},
   "source": [
    "## Building a Python Subclass with the Keras API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5f4b1",
   "metadata": {},
   "source": [
    "Now we will similar to what we did above with a wide and deep model only this time we will build it using the keras API. We build a python class to help us with this task and run a model object to do the same step we just previously did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ed70004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8b63c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x000001CF7EAA6D68>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "WARNING: AutoGraph could not transform <bound method WideAndDeepModel.call of <__main__.WideAndDeepModel object at 0x000001CF7EAA6D68>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: No module named 'tensorflow_core.estimator'\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 120us/sample - loss: 2.7301 - output_1_loss: 2.3970 - output_2_loss: 5.7196 - val_loss: 3.7088 - val_output_1_loss: 3.5199 - val_output_2_loss: 5.3958\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 1.2456 - output_1_loss: 0.9514 - output_2_loss: 3.8925 - val_loss: 1.4109 - val_output_1_loss: 0.8791 - val_output_2_loss: 6.1931\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.9340 - output_1_loss: 0.7366 - output_2_loss: 2.7132 - val_loss: 1.3088 - val_output_1_loss: 0.6696 - val_output_2_loss: 7.0569\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.8017 - output_1_loss: 0.6543 - output_2_loss: 2.1272 - val_loss: 1.3299 - val_output_1_loss: 0.6564 - val_output_2_loss: 7.3867\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.7302 - output_1_loss: 0.6070 - output_2_loss: 1.8458 - val_loss: 1.2567 - val_output_1_loss: 0.5949 - val_output_2_loss: 7.2081\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 89us/sample - loss: 0.6836 - output_1_loss: 0.5710 - output_2_loss: 1.6966 - val_loss: 1.1998 - val_output_1_loss: 0.5716 - val_output_2_loss: 6.8491\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.6472 - output_1_loss: 0.5403 - output_2_loss: 1.6078 - val_loss: 1.1523 - val_output_1_loss: 0.6180 - val_output_2_loss: 5.9569\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.6196 - output_1_loss: 0.5172 - output_2_loss: 1.5400 - val_loss: 0.9844 - val_output_1_loss: 0.4763 - val_output_2_loss: 5.5530\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.5966 - output_1_loss: 0.4976 - output_2_loss: 1.4880 - val_loss: 0.9558 - val_output_1_loss: 0.4904 - val_output_2_loss: 5.1418\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.5771 - output_1_loss: 0.4808 - output_2_loss: 1.4424 - val_loss: 0.8609 - val_output_1_loss: 0.4515 - val_output_2_loss: 4.5430\n",
      "5160/5160 [==============================] - 0s 48us/sample - loss: 0.5599 - output_1_loss: 0.4672 - output_2_loss: 1.4034\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit((X_train_A, X_train_B), (y_train, y_train), epochs=10,\n",
    "                    validation_data=((X_valid_A, X_valid_B), (y_valid, y_valid)))\n",
    "total_loss, main_loss, aux_loss = model.evaluate((X_test_A, X_test_B), (y_test, y_test))\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e587b7d",
   "metadata": {},
   "source": [
    "## Being able to save and restore a model with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcc533f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by building a simple sequential model with 1 hidden layer\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bc31dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 1.9729 - val_loss: 1.6628\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.6958 - val_loss: 0.6909\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.5713 - val_loss: 0.5416\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.5321 - val_loss: 0.4906\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.5054 - val_loss: 0.4636\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4850 - val_loss: 0.4464\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4690 - val_loss: 0.4372\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4564 - val_loss: 0.4334\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.4462 - val_loss: 0.4275\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4382 - val_loss: 0.4271\n",
      "5160/5160 [==============================] - 0s 34us/sample - loss: 0.4326\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7a46970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to simply save a keras neural network model\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4b942bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to simply load the model into the notebook from the file directory\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d4ad3f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68682075],\n",
       "       [1.4693861 ],\n",
       "       [3.330238  ]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the model to predict\n",
    "model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7c117",
   "metadata": {},
   "source": [
    "As another useful resouce, you can also save and load the weights from the model to reuse as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b33dd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bd326fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1cf4cf74e10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"my_keras_weights.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab14be",
   "metadata": {},
   "source": [
    "## Using Callbacks while training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53ed4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple model \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5656099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/10\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 2.1072 - val_loss: 1.7776\n",
      "Epoch 2/10\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.7669 - val_loss: 0.8134\n",
      "Epoch 3/10\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.6748 - val_loss: 0.6226\n",
      "Epoch 4/10\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.6362 - val_loss: 0.5938\n",
      "Epoch 5/10\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.6070 - val_loss: 0.5740\n",
      "Epoch 6/10\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.5814 - val_loss: 0.5441\n",
      "Epoch 7/10\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.5581 - val_loss: 0.5219\n",
      "Epoch 8/10\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.5366 - val_loss: 0.5030\n",
      "Epoch 9/10\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.5175 - val_loss: 0.4793\n",
      "Epoch 10/10\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4997 - val_loss: 0.4616\n",
      "5160/5160 [==============================] - 0s 55us/sample - loss: 0.4810\n"
     ]
    }
   ],
   "source": [
    "# use model checkpoints to save progress of epoch training in case system crashes or something else happens\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True) #NOTE: We only save the best results from training\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef5a91da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4809646786645401"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cab3e5f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.4840 - val_loss: 0.4462\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.4702 - val_loss: 0.4380\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.4588 - val_loss: 0.4261\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4486 - val_loss: 0.4235\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4400 - val_loss: 0.4196\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4328 - val_loss: 0.4174\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4267 - val_loss: 0.4316\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4211 - val_loss: 0.4410\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4164 - val_loss: 0.4392\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.4121 - val_loss: 0.4433\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.4080 - val_loss: 0.4448\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4045 - val_loss: 0.4409\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4013 - val_loss: 0.4487\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3985 - val_loss: 0.4410\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3953 - val_loss: 0.4387\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3927 - val_loss: 0.4537\n",
      "5160/5160 [==============================] - 0s 35us/sample - loss: 0.4223\n"
     ]
    }
   ],
   "source": [
    "#using early stopping if no improvement after 10 epochs\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                  restore_best_weights=True) \n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb])\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c7d78",
   "metadata": {},
   "source": [
    "### Building our own class to print out the validation/training ratio for our loss function for callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3cdf2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4145df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "10752/11610 [==========================>...] - ETA: 0s - loss: 0.4257\n",
      "val/train: 0.99\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.4264 - val_loss: 0.4229\n"
     ]
    }
   ],
   "source": [
    "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
    "history = model.fit(X_train, y_train, epochs=1,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[val_train_ratio_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68367e56",
   "metadata": {},
   "source": [
    "## Introduction to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ac92999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aeebfca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2022_01_09-18_15_00'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04a98495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f0fb8a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 107us/sample - loss: 2.3796 - val_loss: 1.0007\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 87us/sample - loss: 0.8962 - val_loss: 0.7174\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.6933 - val_loss: 0.6088\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.6158 - val_loss: 0.5594\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.5720 - val_loss: 0.5541\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.5387 - val_loss: 0.5434\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.5122 - val_loss: 0.4762\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.4907 - val_loss: 0.4674\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.4730 - val_loss: 0.4450\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.4586 - val_loss: 0.4684\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.4468 - val_loss: 0.4351\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.4370 - val_loss: 0.4684\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.4293 - val_loss: 0.4100\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 86us/sample - loss: 0.4220 - val_loss: 0.4640\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.4165 - val_loss: 0.4049\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.4112 - val_loss: 0.3882\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.4067 - val_loss: 0.3823\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.4029 - val_loss: 0.3768\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3992 - val_loss: 0.3820\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.3965 - val_loss: 0.4101\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.3943 - val_loss: 0.3778\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 76us/sample - loss: 0.3906 - val_loss: 0.5540\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.3903 - val_loss: 0.4832\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.3873 - val_loss: 0.3895\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3852 - val_loss: 0.3632\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3829 - val_loss: 0.3714\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.3811 - val_loss: 0.3827\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3790 - val_loss: 0.4259\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3783 - val_loss: 0.4181\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.3761 - val_loss: 0.4673\n"
     ]
    }
   ],
   "source": [
    "# Logging our callbacks to the tensorboard object using our directory\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc21ecd",
   "metadata": {},
   "source": [
    "Now we will load tensorboard manually into our jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f081f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1808), started 0:27:28 ago. (Use '!kill 1808' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5137c04b0895f3c0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5137c04b0895f3c0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d4727f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_logs\\\\run_2022_01_09-18_15_29'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_logdir2 = get_run_logdir()\n",
    "run_logdir2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b9c4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    \n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(learning_rate=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a129de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/30\n",
      "11610/11610 [==============================] - 1s 113us/sample - loss: 0.5392 - val_loss: 2.2308\n",
      "Epoch 2/30\n",
      "11610/11610 [==============================] - 1s 74us/sample - loss: 0.4373 - val_loss: 0.3594\n",
      "Epoch 3/30\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.5042 - val_loss: 0.3959\n",
      "Epoch 4/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3746 - val_loss: 0.3394\n",
      "Epoch 5/30\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3566 - val_loss: 0.3280\n",
      "Epoch 6/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.3462 - val_loss: 0.8118\n",
      "Epoch 7/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3482 - val_loss: 0.4881\n",
      "Epoch 8/30\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.3361 - val_loss: 0.3877\n",
      "Epoch 9/30\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.3331 - val_loss: 0.3106\n",
      "Epoch 10/30\n",
      "11610/11610 [==============================] - 1s 83us/sample - loss: 0.3258 - val_loss: 0.3250\n",
      "Epoch 11/30\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.3224 - val_loss: 0.2956\n",
      "Epoch 12/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3170 - val_loss: 0.3050\n",
      "Epoch 13/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3112 - val_loss: 0.3043\n",
      "Epoch 14/30\n",
      "11610/11610 [==============================] - 1s 82us/sample - loss: 0.3079 - val_loss: 0.3193\n",
      "Epoch 15/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.3046 - val_loss: 0.3014\n",
      "Epoch 16/30\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.3042 - val_loss: 0.3406\n",
      "Epoch 17/30\n",
      "11610/11610 [==============================] - 1s 79us/sample - loss: 0.3011 - val_loss: 0.3323\n",
      "Epoch 18/30\n",
      "11610/11610 [==============================] - 1s 81us/sample - loss: 0.2954 - val_loss: 0.4243\n",
      "Epoch 19/30\n",
      "11610/11610 [==============================] - 1s 88us/sample - loss: 0.2975 - val_loss: 0.3138\n",
      "Epoch 20/30\n",
      "11610/11610 [==============================] - 1s 92us/sample - loss: 0.2949 - val_loss: 0.3461\n",
      "Epoch 21/30\n",
      "11610/11610 [==============================] - 1s 85us/sample - loss: 0.2912 - val_loss: 0.2763\n",
      "Epoch 22/30\n",
      "11610/11610 [==============================] - 1s 86us/sample - loss: 0.2906 - val_loss: 0.3100\n",
      "Epoch 23/30\n",
      "11610/11610 [==============================] - 1s 77us/sample - loss: 0.2927 - val_loss: 0.3080\n",
      "Epoch 24/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.2911 - val_loss: 0.3099\n",
      "Epoch 25/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.2874 - val_loss: 0.3848\n",
      "Epoch 26/30\n",
      "11610/11610 [==============================] - 1s 88us/sample - loss: 0.2836 - val_loss: 0.2820\n",
      "Epoch 27/30\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.2841 - val_loss: 0.4267\n",
      "Epoch 28/30\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.2831 - val_loss: 0.2744\n",
      "Epoch 29/30\n",
      "11610/11610 [==============================] - 1s 84us/sample - loss: 0.2797 - val_loss: 0.4900\n",
      "Epoch 30/30\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 0.2867 - val_loss: 0.6422\n"
     ]
    }
   ],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir2)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b98587",
   "metadata": {},
   "source": [
    "Now we can look again at tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "274c3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1808), started 0:27:57 ago. (Use '!kill 1808' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-df1251e8641c509d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-df1251e8641c509d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ecc40",
   "metadata": {},
   "source": [
    "## Neural Network Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e2a43b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function manually to build us a model with a custom learning rate\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c7202593",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model) # still keras under a scikit learn wrapper for a regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b01a1a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 80us/sample - loss: 1.2291 - val_loss: 3.0417\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.5326 - val_loss: 0.4553\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4683 - val_loss: 0.4181\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.4447 - val_loss: 0.4067\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4320 - val_loss: 0.4244\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4231 - val_loss: 0.4109\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.4180 - val_loss: 0.4328\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4111 - val_loss: 0.4427\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.4072 - val_loss: 0.4067\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4041 - val_loss: 0.4106\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4031 - val_loss: 0.4129\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4004 - val_loss: 0.4240\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3976 - val_loss: 0.4326\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3950 - val_loss: 0.3957\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.3907 - val_loss: 0.4038\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.3901 - val_loss: 0.4160\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3878 - val_loss: 0.3690\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3865 - val_loss: 0.3851\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3833 - val_loss: 0.3805\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3818 - val_loss: 0.3919\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3804 - val_loss: 0.3676\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3781 - val_loss: 0.3831\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3772 - val_loss: 0.3515\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3773 - val_loss: 0.3648\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3747 - val_loss: 0.3505\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3735 - val_loss: 0.3756\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3725 - val_loss: 0.3544\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3705 - val_loss: 0.3432\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.3705 - val_loss: 0.3807\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.3702 - val_loss: 0.3426\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.3672 - val_loss: 0.4032\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3666 - val_loss: 0.3428\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3666 - val_loss: 0.3455\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3649 - val_loss: 0.3807\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3628 - val_loss: 0.3759\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.3628 - val_loss: 0.3723\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3618 - val_loss: 0.3526\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3619 - val_loss: 0.3357\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3596 - val_loss: 0.3397\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3584 - val_loss: 0.3874\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3580 - val_loss: 0.3429\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3594 - val_loss: 0.3358\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3562 - val_loss: 0.3634\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3552 - val_loss: 0.3478\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3562 - val_loss: 0.3427\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3538 - val_loss: 0.3319\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3529 - val_loss: 0.3317\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3528 - val_loss: 0.3705\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 75us/sample - loss: 0.3515 - val_loss: 0.3362\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3507 - val_loss: 0.3313\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3509 - val_loss: 0.3764\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3502 - val_loss: 0.3402\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3484 - val_loss: 0.3879\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.3481 - val_loss: 0.3562\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 65us/sample - loss: 0.3473 - val_loss: 0.3551\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3479 - val_loss: 0.3269\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3457 - val_loss: 0.3413\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.3457 - val_loss: 0.3449\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.3444 - val_loss: 0.4137\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.3446 - val_loss: 0.3802\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.3453 - val_loss: 0.3233\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3433 - val_loss: 0.4048\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3443 - val_loss: 0.5113\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3431 - val_loss: 0.3764\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3413 - val_loss: 0.3229\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3428 - val_loss: 0.3813\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.3421 - val_loss: 0.4976\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3413 - val_loss: 0.4711\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 70us/sample - loss: 0.3432 - val_loss: 0.6588\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3434 - val_loss: 0.5398\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3409 - val_loss: 0.8207\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.3450 - val_loss: 1.1490\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3442 - val_loss: 1.2438\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.3505 - val_loss: 1.4791\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3438 - val_loss: 1.8159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf2b9c1048>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c0f04c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 35us/sample - loss: 0.3446\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "68655131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "911a664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65689814, 1.5495439 , 4.06317   ], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fc757eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.34460847082064133"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a3e395",
   "metadata": {},
   "source": [
    "Now with our created model. lets use grid search to find us optimal parameters we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c278dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 2.0921 - val_loss: 155.7743\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 4.3938 - val_loss: 306.9512\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 4.8927 - val_loss: 892.0719\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 6.2895 - val_loss: 3233.7001\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 21.2285 - val_loss: 8193.9742\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 114.8909 - val_loss: 22572.0240\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 166.0599 - val_loss: 60507.4180\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 908.3290 - val_loss: 164538.3430\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 944.6871 - val_loss: 443900.6007\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 2166.4437 - val_loss: 1219587.4141\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 18644.8864 - val_loss: 3323310.6058\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 10488.1394\n",
      "[CV 1/5] END learning_rate=0.008104201962072255, n_hidden=0, n_neurons=80;, score=-10488.139 total time=   6.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 1.4703 - val_loss: 88.5540\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 2.1566 - val_loss: 320.8628\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 2.1508 - val_loss: 1475.4921\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 10.3843 - val_loss: 3729.4198\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 46.9876 - val_loss: 10238.6614\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 56.3738 - val_loss: 25584.3345\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 105.2652 - val_loss: 71544.8772\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 200.6036 - val_loss: 180202.8842\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 3460.0020 - val_loss: 471851.8127\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 1658.2049 - val_loss: 1247507.5895\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 6130.1393 - val_loss: 3252192.9631\n",
      "2322/2322 [==============================] - 0s 40us/sample - loss: 10193.5480\n",
      "[CV 2/5] END learning_rate=0.008104201962072255, n_hidden=0, n_neurons=80;, score=-10193.548 total time=   6.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 97us/sample - loss: 1.1684 - val_loss: 0.9725\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5239 - val_loss: 9.1464\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5157 - val_loss: 11.6476\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5124 - val_loss: 12.3817\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5043 - val_loss: 15.2936\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5140 - val_loss: 15.2209\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5122 - val_loss: 12.8218\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5091 - val_loss: 13.5019\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5051 - val_loss: 16.1495\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5059 - val_loss: 15.6369\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5032 - val_loss: 14.0074\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 1.0525\n",
      "[CV 3/5] END learning_rate=0.008104201962072255, n_hidden=0, n_neurons=80;, score=-1.053 total time=   7.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 1.4129 - val_loss: 6.1062\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6432 - val_loss: 0.5409\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.5950 - val_loss: 2.1150\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6989 - val_loss: 3.1495\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5619 - val_loss: 1.6373\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6587 - val_loss: 1.4239\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6043 - val_loss: 4.8395\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5484 - val_loss: 3.9698\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6727 - val_loss: 0.5042\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6178 - val_loss: 17.6736\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5778 - val_loss: 0.6991\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6097 - val_loss: 11.3082\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5928 - val_loss: 23.1235\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 0.9736 - val_loss: 47.3081\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6112 - val_loss: 9.0712\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.7235 - val_loss: 29.8241\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7408 - val_loss: 47.4721\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 1.7995 - val_loss: 44.6999\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 1.2121 - val_loss: 95.6930\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4362\n",
      "[CV 4/5] END learning_rate=0.008104201962072255, n_hidden=0, n_neurons=80;, score=-0.436 total time=  11.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 2.4913 - val_loss: 53.1980\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6469 - val_loss: 12.8985\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.9448 - val_loss: 59.6586\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6907 - val_loss: 28.7639\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8673 - val_loss: 160.2441\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.8097 - val_loss: 76.2893\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 2.2586 - val_loss: 116.7374\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1.0864 - val_loss: 150.8866\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 1.6361 - val_loss: 333.9708\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.2736 - val_loss: 304.6601\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 1.4164 - val_loss: 949.2519\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 9.0269 - val_loss: 1533.1127\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 1.5832\n",
      "[CV 5/5] END learning_rate=0.008104201962072255, n_hidden=0, n_neurons=80;, score=-1.583 total time=   7.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 97us/sample - loss: 0.7525 - val_loss: 1.1349\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4026 - val_loss: 0.7763\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3770 - val_loss: 0.7108\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3609 - val_loss: 1.8481\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3920 - val_loss: 0.4195\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3541 - val_loss: 2.0313\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3645 - val_loss: 1.6247\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3562 - val_loss: 0.3269\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3346 - val_loss: 0.3111\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3290 - val_loss: 0.3160\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3262 - val_loss: 0.3147\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3215 - val_loss: 0.3266\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3185 - val_loss: 0.3216\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3139 - val_loss: 0.3550\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3107 - val_loss: 0.3577\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3065 - val_loss: 0.4526\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3619 - val_loss: 0.3129\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3119 - val_loss: 0.3093\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3065 - val_loss: 0.3162\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3033 - val_loss: 0.3128\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3001 - val_loss: 0.3022\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2966 - val_loss: 0.3105\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2928 - val_loss: 0.3080\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2922 - val_loss: 0.3044\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2877 - val_loss: 0.3489\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2872 - val_loss: 0.2951\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2851 - val_loss: 0.3529\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2850 - val_loss: 0.2940\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2847 - val_loss: 0.2950\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2787 - val_loss: 0.2981\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2781 - val_loss: 0.3653\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2787 - val_loss: 0.5252\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2775 - val_loss: 0.3451\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2757 - val_loss: 0.2951\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2729 - val_loss: 0.3057\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2714 - val_loss: 0.2957\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2734 - val_loss: 0.3476\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2710 - val_loss: 0.2707\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2693 - val_loss: 0.3308\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2693 - val_loss: 0.2961\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2674 - val_loss: 0.3041\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2649 - val_loss: 0.2659\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2668 - val_loss: 0.3028\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2648 - val_loss: 0.2740\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2626 - val_loss: 0.2649\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2608 - val_loss: 0.2783\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2618 - val_loss: 0.3420\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2613 - val_loss: 0.2851\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2616 - val_loss: 0.2849\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2611 - val_loss: 0.3231\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2594 - val_loss: 0.2812\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2597 - val_loss: 0.2917\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2595 - val_loss: 0.3002\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2574 - val_loss: 0.3369\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2569 - val_loss: 0.3254\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.2891\n",
      "[CV 1/5] END learning_rate=0.011858962637622806, n_hidden=3, n_neurons=75;, score=-0.289 total time=  35.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 107us/sample - loss: 0.6629 - val_loss: 1.4146\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4052 - val_loss: 1.9192\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3800 - val_loss: 0.4966\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3509 - val_loss: 0.4915\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3440 - val_loss: 0.3749\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3332 - val_loss: 0.4258\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3272 - val_loss: 0.3372\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3221 - val_loss: 0.3629\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3158 - val_loss: 0.3085\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3116 - val_loss: 0.3381\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3087 - val_loss: 0.3181\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3035 - val_loss: 0.3400\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3015 - val_loss: 0.4457\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2998 - val_loss: 0.3064\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2926 - val_loss: 0.3301\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2923 - val_loss: 0.2987\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2903 - val_loss: 0.3803\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2892 - val_loss: 0.3570\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2878 - val_loss: 0.8694\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2894 - val_loss: 0.5625\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2876 - val_loss: 0.3510\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2824 - val_loss: 0.2890\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2789 - val_loss: 0.3540\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2783 - val_loss: 0.2968\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.2767 - val_loss: 0.3065\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2740 - val_loss: 0.3830\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2713 - val_loss: 0.2865\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2713 - val_loss: 0.3124\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2702 - val_loss: 0.3004\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2687 - val_loss: 0.3347\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2693 - val_loss: 0.2855\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2684 - val_loss: 0.2960\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2641 - val_loss: 0.3141\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2637 - val_loss: 0.2925\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2638 - val_loss: 0.2682\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2625 - val_loss: 0.3660\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2612 - val_loss: 0.2840\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2616 - val_loss: 0.3425\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2593 - val_loss: 0.2999\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2579 - val_loss: 0.3667\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2601 - val_loss: 0.2676\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2578 - val_loss: 0.4101\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2572 - val_loss: 0.3316\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2575 - val_loss: 0.4945\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2561 - val_loss: 0.3026\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2557 - val_loss: 0.3531\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2520 - val_loss: 0.3126\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2534 - val_loss: 0.3116\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2529 - val_loss: 0.2851\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2513 - val_loss: 0.2852\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2489 - val_loss: 0.2777\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3306\n",
      "[CV 2/5] END learning_rate=0.011858962637622806, n_hidden=3, n_neurons=75;, score=-0.331 total time=  33.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 100us/sample - loss: 0.7284 - val_loss: 0.4622\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4000 - val_loss: 0.4370\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3703 - val_loss: 1.5251\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3572 - val_loss: 0.5895\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3456 - val_loss: 1.3114\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3353 - val_loss: 1.6566\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3298 - val_loss: 1.6036\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3261 - val_loss: 0.7460\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3203 - val_loss: 0.9929\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3165 - val_loss: 0.8817\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3100 - val_loss: 0.4865\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3077 - val_loss: 0.9600\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.5324\n",
      "[CV 3/5] END learning_rate=0.011858962637622806, n_hidden=3, n_neurons=75;, score=-0.532 total time=   8.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 109us/sample - loss: 0.6847 - val_loss: 2.3034\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4372 - val_loss: 3.9811\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4109 - val_loss: 0.9294\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3810 - val_loss: 0.4795\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3608 - val_loss: 0.3356\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3497 - val_loss: 0.3960\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3452 - val_loss: 0.3478\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3391 - val_loss: 0.3079\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3329 - val_loss: 0.3741\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3286 - val_loss: 0.3101\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3225 - val_loss: 0.3163\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3186 - val_loss: 1.7599\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3222 - val_loss: 0.3636\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3162 - val_loss: 0.3269\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.3125 - val_loss: 0.3295\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3077 - val_loss: 0.3451\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3068 - val_loss: 0.2957\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3033 - val_loss: 0.3470\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3000 - val_loss: 0.2855\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2988 - val_loss: 0.3131\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2976 - val_loss: 0.2886\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2964 - val_loss: 0.2980\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2941 - val_loss: 0.3742\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2925 - val_loss: 0.4516\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2900 - val_loss: 0.2866\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2888 - val_loss: 0.3010\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2865 - val_loss: 0.3877\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2884 - val_loss: 0.3412\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2852 - val_loss: 0.3139\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.2913\n",
      "[CV 4/5] END learning_rate=0.011858962637622806, n_hidden=3, n_neurons=75;, score=-0.291 total time=  19.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 98us/sample - loss: 0.8029 - val_loss: 3.4046\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5200 - val_loss: 0.6997\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3817 - val_loss: 0.7055\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3586 - val_loss: 1.9657\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3631 - val_loss: 3.1439\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3667 - val_loss: 1.9357\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3442 - val_loss: 2.3098\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3468 - val_loss: 0.6061\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3297 - val_loss: 0.3235\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3224 - val_loss: 0.3560\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3185 - val_loss: 0.3336\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3138 - val_loss: 0.3634\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3106 - val_loss: 0.3450\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3061 - val_loss: 0.3311\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3016 - val_loss: 0.3200\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3017 - val_loss: 0.3013\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2977 - val_loss: 0.4178\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2973 - val_loss: 0.3436\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2931 - val_loss: 0.3831\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2902 - val_loss: 0.3430\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2880 - val_loss: 0.2902\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2859 - val_loss: 0.3227\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2854 - val_loss: 0.2872\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2831 - val_loss: 0.2873\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2829 - val_loss: 0.3212\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2796 - val_loss: 0.3001\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2795 - val_loss: 0.2863\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2761 - val_loss: 0.3254\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2754 - val_loss: 0.2814\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2746 - val_loss: 0.2898\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2712 - val_loss: 0.2814\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2712 - val_loss: 0.3305\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2721 - val_loss: 0.2824\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2712 - val_loss: 0.3460\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2690 - val_loss: 0.2939\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2671 - val_loss: 0.3275\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2680 - val_loss: 0.3227\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2699 - val_loss: 0.2893\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2641 - val_loss: 0.3509\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.3697\n",
      "[CV 5/5] END learning_rate=0.011858962637622806, n_hidden=3, n_neurons=75;, score=-0.370 total time=  24.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.6070 - val_loss: 1.2155\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.4406 - val_loss: 9.0296\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4959 - val_loss: 2.2595\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: nan - val_loss: nan\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: nan\n",
      "[CV 1/5] END learning_rate=0.029237254345484322, n_hidden=2, n_neurons=50;, score=nan total time=   7.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 0.5688 - val_loss: 6.8012\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6258 - val_loss: 59.6945\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 0.4164 - val_loss: 54.4709\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6117 - val_loss: 1.1113\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3687 - val_loss: 0.3676\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3500 - val_loss: 0.3262\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3376 - val_loss: 0.3352\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3334 - val_loss: 0.3612\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3280 - val_loss: 0.8035\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3247 - val_loss: 0.7706\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3200 - val_loss: 0.3047\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3138 - val_loss: 0.5771\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3093 - val_loss: 0.3258\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3067 - val_loss: 0.3105\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3035 - val_loss: 0.2935\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3019 - val_loss: 0.3084\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2967 - val_loss: 0.3154\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2950 - val_loss: 0.2871\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2925 - val_loss: 0.3020\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2901 - val_loss: 0.3074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2870 - val_loss: 0.2884\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2859 - val_loss: 0.6147\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2847 - val_loss: 0.3654\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2840 - val_loss: 0.3332\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2815 - val_loss: 0.2833\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2751 - val_loss: 0.3540\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2759 - val_loss: 0.3756\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2733 - val_loss: 0.2939\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.2712 - val_loss: 0.3110\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2704 - val_loss: 0.3032\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2699 - val_loss: 0.3279\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2674 - val_loss: 0.3887\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2656 - val_loss: 0.3278\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2674 - val_loss: 0.3274\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2661 - val_loss: 0.2808\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2614 - val_loss: 0.2769\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2619 - val_loss: 0.2813\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2597 - val_loss: 0.2823\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2593 - val_loss: 0.3324\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2577 - val_loss: 0.2988\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2606 - val_loss: 0.3494\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2576 - val_loss: 0.2702\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2564 - val_loss: 0.2765\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2581 - val_loss: 0.2653\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2517 - val_loss: 0.3037\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2538 - val_loss: 0.2716\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2520 - val_loss: 0.2770\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2521 - val_loss: 0.3848\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2493 - val_loss: 0.2703\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2515 - val_loss: 0.2621\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2500 - val_loss: 0.2888\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2474 - val_loss: 0.2779\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2475 - val_loss: 0.2631\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2465 - val_loss: 0.2885\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2480 - val_loss: 0.2706\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2465 - val_loss: 0.2782\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2427 - val_loss: 0.2679\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2457 - val_loss: 0.4147\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2439 - val_loss: 0.3049\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2428 - val_loss: 0.2800\n",
      "2322/2322 [==============================] - 0s 46us/sample - loss: 0.3309\n",
      "[CV 2/5] END learning_rate=0.029237254345484322, n_hidden=2, n_neurons=50;, score=-0.331 total time=  36.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 0.5820 - val_loss: 0.3931\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4012 - val_loss: 1.2668\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3738 - val_loss: 1.8583\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3528 - val_loss: 0.7432\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3406 - val_loss: 0.5934\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3403 - val_loss: 0.7326\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3326 - val_loss: 0.8344\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3259 - val_loss: 0.5202\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3196 - val_loss: 3.0197\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3183 - val_loss: 0.3344\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3181 - val_loss: 0.3103\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3114 - val_loss: 1.0762\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3072 - val_loss: 1.4327\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3021 - val_loss: 0.7644\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3049 - val_loss: 2.2309\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3003 - val_loss: 0.8093\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2970 - val_loss: 2.0758\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2970 - val_loss: 0.2844\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2948 - val_loss: 2.9734\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2922 - val_loss: 1.6400\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2891 - val_loss: 0.8616\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2870 - val_loss: 1.2695\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2863 - val_loss: 1.9952\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2853 - val_loss: 3.2244\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2856 - val_loss: 1.6042\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2823 - val_loss: 0.4004\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2793 - val_loss: 0.3130\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2803 - val_loss: 0.3355\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3118\n",
      "[CV 3/5] END learning_rate=0.029237254345484322, n_hidden=2, n_neurons=50;, score=-0.312 total time=  17.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 0.8067 - val_loss: 5.9555\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6383 - val_loss: 176.7756\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 2.1082 - val_loss: 0.9585\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4208 - val_loss: 1.0243\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3970 - val_loss: 1.2241\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 0.3956 - val_loss: 0.3817\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3682 - val_loss: 0.3716\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3568 - val_loss: 0.3270\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3937 - val_loss: 0.3859\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3654 - val_loss: 0.3209\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3511 - val_loss: 0.3511\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3471 - val_loss: 0.3355\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3427 - val_loss: 0.3371\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3328 - val_loss: 0.3347\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3335 - val_loss: 0.2990\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3303 - val_loss: 0.3364\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3257 - val_loss: 0.3323\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3208 - val_loss: 0.3763\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3160 - val_loss: 0.4411\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3141 - val_loss: 0.3183\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3137 - val_loss: 0.2994\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3112 - val_loss: 0.2879\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3090 - val_loss: 1.3469\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3058 - val_loss: 0.3076\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3028 - val_loss: 0.3139\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3077 - val_loss: 0.3921\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3028 - val_loss: 0.3025\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3020 - val_loss: 5.3206\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3046 - val_loss: 0.9746\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3016 - val_loss: 0.9994\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3109 - val_loss: 0.4750\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2988 - val_loss: 0.3442\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.2730\n",
      "[CV 4/5] END learning_rate=0.029237254345484322, n_hidden=2, n_neurons=50;, score=-0.273 total time=  20.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 0.5565 - val_loss: 12.2267\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4389 - val_loss: 6.7470\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4655 - val_loss: 1.6964\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3721 - val_loss: 0.3414\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3526 - val_loss: 0.6541\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3491 - val_loss: 0.4856\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3371 - val_loss: 0.8796\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3380 - val_loss: 0.3758\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.3296 - val_loss: 1.2682\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3266 - val_loss: 0.5861\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3209 - val_loss: 0.3662\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3122 - val_loss: 0.3143\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3068 - val_loss: 0.3197\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3018 - val_loss: 0.3098\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2982 - val_loss: 0.3320\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2951 - val_loss: 0.3146\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2924 - val_loss: 0.3206\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2877 - val_loss: 0.2968\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2877 - val_loss: 0.3110\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2866 - val_loss: 0.3039\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2904 - val_loss: 0.3045\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2818 - val_loss: 0.2873\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2804 - val_loss: 0.3016\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2775 - val_loss: 0.3032\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2773 - val_loss: 0.2917\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2730 - val_loss: 0.3293\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2707 - val_loss: 0.2797\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2726 - val_loss: 0.2930\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2723 - val_loss: 0.3119\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2714 - val_loss: 0.2714\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2694 - val_loss: 0.2810\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2667 - val_loss: 0.3083\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2691 - val_loss: 0.2857\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2662 - val_loss: 0.2696\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2666 - val_loss: 0.3003\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2661 - val_loss: 0.2726\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2628 - val_loss: 0.2943\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2612 - val_loss: 0.3810\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2624 - val_loss: 0.4900\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2637 - val_loss: 0.2927\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2608 - val_loss: 0.3197\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2595 - val_loss: 0.2666\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2583 - val_loss: 0.3930\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2585 - val_loss: 0.2970\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2563 - val_loss: 0.2907\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2593 - val_loss: 0.2919\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.2552 - val_loss: 0.2843\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2544 - val_loss: 0.3440\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2555 - val_loss: 0.2671\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.2560 - val_loss: 0.4602\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2557 - val_loss: 0.3465\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2536 - val_loss: 0.6698\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3044\n",
      "[CV 5/5] END learning_rate=0.029237254345484322, n_hidden=2, n_neurons=50;, score=-0.304 total time=  32.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.7898 - val_loss: 2.0743\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.7602 - val_loss: 1.5860\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6540 - val_loss: 1.0796\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6170 - val_loss: 1.3576\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5820 - val_loss: 1.2559\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5980 - val_loss: 0.7081\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5483 - val_loss: 2.5420\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6171 - val_loss: 7.3149\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5683 - val_loss: 18.3094\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8525 - val_loss: 37.9292\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8836 - val_loss: 72.6890\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.5806 - val_loss: 94.5723\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 1.6701 - val_loss: 177.8111\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 2.2739 - val_loss: 262.0549\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 2.6657 - val_loss: 371.5284\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 3.8810 - val_loss: 585.3937\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 2.8012\n",
      "[CV 1/5] END learning_rate=0.005193166156712816, n_hidden=0, n_neurons=41;, score=-2.801 total time=   9.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.7387 - val_loss: 36.1839\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.9210 - val_loss: 87.6626\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 1.3403 - val_loss: 142.1988\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 2.6061 - val_loss: 182.6487\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.6192 - val_loss: 195.7573\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 1.8377 - val_loss: 388.0244\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 2.2867 - val_loss: 415.8077\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 3.7319 - val_loss: 626.2431\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 3.8934 - val_loss: 763.9325\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 4.5202 - val_loss: 1345.9522\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 17.4824 - val_loss: 2061.6862\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 6.3381\n",
      "[CV 2/5] END learning_rate=0.005193166156712816, n_hidden=0, n_neurons=41;, score=-6.338 total time=   6.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.3915 - val_loss: 25.1471\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5526 - val_loss: 19.0304\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5341 - val_loss: 17.8392\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 0.5227 - val_loss: 15.8205\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5175 - val_loss: 14.7178\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5146 - val_loss: 15.2758\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5178 - val_loss: 14.4523\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5129 - val_loss: 15.0154\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5074 - val_loss: 15.3506\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5084 - val_loss: 13.8437\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5074 - val_loss: 13.7185\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5034 - val_loss: 14.3204\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5043 - val_loss: 13.3096\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5034 - val_loss: 13.9163\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5001 - val_loss: 13.5195\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5063 - val_loss: 14.2906\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5019 - val_loss: 14.4324\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5041 - val_loss: 14.0532\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5043 - val_loss: 14.9112\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5020 - val_loss: 13.6773\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4998 - val_loss: 13.9655\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5035 - val_loss: 13.9549\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5004 - val_loss: 13.4563\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 1.0338\n",
      "[CV 3/5] END learning_rate=0.005193166156712816, n_hidden=0, n_neurons=41;, score=-1.034 total time=  13.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.8664 - val_loss: 53.7597\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.7870 - val_loss: 14.7930\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7748 - val_loss: 22.3640\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6350 - val_loss: 2.9035\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6436 - val_loss: 5.7760\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5576 - val_loss: 10.5038\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6686 - val_loss: 39.6496\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6799 - val_loss: 20.1709\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.9085 - val_loss: 10.7560\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5776 - val_loss: 5.0021\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6508 - val_loss: 0.9534\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5636 - val_loss: 4.2900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5505 - val_loss: 4.9770\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6244 - val_loss: 10.2419\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5716 - val_loss: 0.5179\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5711 - val_loss: 0.6448\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5558 - val_loss: 0.4954\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5592 - val_loss: 0.5630\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5556 - val_loss: 0.8130\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5781 - val_loss: 3.4995\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5523 - val_loss: 12.3176\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7311 - val_loss: 8.3253\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5639 - val_loss: 3.6345\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6324 - val_loss: 1.7675\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5416 - val_loss: 2.6230\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6003 - val_loss: 3.6178\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5558 - val_loss: 0.6658\n",
      "2322/2322 [==============================] - 0s 41us/sample - loss: 0.4779\n",
      "[CV 4/5] END learning_rate=0.005193166156712816, n_hidden=0, n_neurons=41;, score=-0.478 total time=  16.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.5555 - val_loss: 2.0144\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6613 - val_loss: 0.7412\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5973 - val_loss: 1.0092\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6058 - val_loss: 1.8625\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5633 - val_loss: 1.2347\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5549 - val_loss: 2.1733\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5375 - val_loss: 1.3236\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5697 - val_loss: 0.5321\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5472 - val_loss: 0.6791\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5366 - val_loss: 0.5290\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5311 - val_loss: 1.0952\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5599 - val_loss: 0.6673\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5197 - val_loss: 2.2049\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5882 - val_loss: 8.5643\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5504 - val_loss: 0.5848\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5404 - val_loss: 7.6669\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5439 - val_loss: 2.9129\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5813 - val_loss: 0.8833\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5230 - val_loss: 1.4086\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5649 - val_loss: 10.0037\n",
      "2322/2322 [==============================] - 0s 51us/sample - loss: 0.6353\n",
      "[CV 5/5] END learning_rate=0.005193166156712816, n_hidden=0, n_neurons=41;, score=-0.635 total time=  12.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 127us/sample - loss: 1.9797 - val_loss: 2.8035\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7506 - val_loss: 0.6677\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6487 - val_loss: 0.5800\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5978 - val_loss: 0.5614\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5596 - val_loss: 0.5187\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5285 - val_loss: 0.4910\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5024 - val_loss: 0.4542\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4816 - val_loss: 0.4374\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4648 - val_loss: 0.4203\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4513 - val_loss: 0.4097\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4404 - val_loss: 0.4090\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4316 - val_loss: 0.4066\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4240 - val_loss: 0.3973\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4181 - val_loss: 0.4154\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4129 - val_loss: 0.4214\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4085 - val_loss: 0.4013\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.4044 - val_loss: 0.4006\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4017 - val_loss: 0.3832\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3983 - val_loss: 0.3996\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3957 - val_loss: 0.3918\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3933 - val_loss: 0.4008\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3906 - val_loss: 0.3874\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.3887 - val_loss: 0.3810\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3866 - val_loss: 0.3813\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3849 - val_loss: 0.3867\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3833 - val_loss: 0.3844\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3808 - val_loss: 0.3890\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3796 - val_loss: 0.3993\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3781 - val_loss: 0.3647\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3770 - val_loss: 0.3590\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3755 - val_loss: 0.3830\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3740 - val_loss: 0.4030\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3730 - val_loss: 0.3744\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3719 - val_loss: 0.3958\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3705 - val_loss: 0.3547\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3692 - val_loss: 0.3953\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3685 - val_loss: 0.3688\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3674 - val_loss: 0.3613\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3659 - val_loss: 0.3659\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3650 - val_loss: 0.3807\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3646 - val_loss: 0.3551\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3631 - val_loss: 0.3578\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3626 - val_loss: 0.3516\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3616 - val_loss: 0.3551\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3607 - val_loss: 0.3849\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3601 - val_loss: 0.3694\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3590 - val_loss: 0.3911\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3583 - val_loss: 0.3699\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3577 - val_loss: 0.3463\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3569 - val_loss: 0.3451\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3558 - val_loss: 0.3619\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3553 - val_loss: 0.3426\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3543 - val_loss: 0.3779\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3536 - val_loss: 0.3414\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3531 - val_loss: 0.3367\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3524 - val_loss: 0.3464\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3513 - val_loss: 0.3471\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3510 - val_loss: 0.3380\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3502 - val_loss: 0.3749\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3497 - val_loss: 0.3395\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3489 - val_loss: 0.3558\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3484 - val_loss: 0.3524\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3478 - val_loss: 0.3646\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3471 - val_loss: 0.3443\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3463 - val_loss: 0.3627\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3309\n",
      "[CV 1/5] END learning_rate=0.0012079427918657015, n_hidden=2, n_neurons=65;, score=-0.331 total time=  41.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 1.8588 - val_loss: 9.7787\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.7095 - val_loss: 1.0496\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.5919 - val_loss: 0.6276\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5490 - val_loss: 0.5288\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5214 - val_loss: 0.5555\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4953 - val_loss: 0.4723\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4753 - val_loss: 0.5233\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4579 - val_loss: 0.4664\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4437 - val_loss: 0.4838\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.4318 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4221 - val_loss: 0.4706\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4137 - val_loss: 0.3970\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4070 - val_loss: 0.4213\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4011 - val_loss: 0.4022\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3964 - val_loss: 0.3985\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3918 - val_loss: 0.4040\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3880 - val_loss: 0.3973\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3848 - val_loss: 0.3944\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3814 - val_loss: 0.4015\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3791 - val_loss: 0.3832\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.3764 - val_loss: 0.3754\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3741 - val_loss: 0.4057\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3720 - val_loss: 0.4031\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3700 - val_loss: 0.3832\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3681 - val_loss: 0.3783\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3661 - val_loss: 0.3779\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3645 - val_loss: 0.3743\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3630 - val_loss: 0.3705\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3613 - val_loss: 0.3908\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3599 - val_loss: 0.3786\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3586 - val_loss: 0.3683\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3573 - val_loss: 0.3856\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3559 - val_loss: 0.3649\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3549 - val_loss: 0.3811\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3535 - val_loss: 0.3726\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3526 - val_loss: 0.3638\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3515 - val_loss: 0.3914\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3505 - val_loss: 0.3664\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3495 - val_loss: 0.3719\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3489 - val_loss: 0.3796\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3479 - val_loss: 0.3671\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3469 - val_loss: 0.3579\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3461 - val_loss: 0.3807\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3453 - val_loss: 0.3790\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3443 - val_loss: 0.3722\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3436 - val_loss: 0.3657\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3429 - val_loss: 0.3582\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3422 - val_loss: 0.3657\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3415 - val_loss: 0.3612\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3407 - val_loss: 0.3504\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3399 - val_loss: 0.3642\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3391 - val_loss: 0.3696\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3386 - val_loss: 0.3690\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3380 - val_loss: 0.3470\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3372 - val_loss: 0.3572\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3367 - val_loss: 0.3521\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3360 - val_loss: 0.3545\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3354 - val_loss: 0.3598\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3347 - val_loss: 0.3485\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3342 - val_loss: 0.3478\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3335 - val_loss: 0.3534\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3330 - val_loss: 0.3381\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3325 - val_loss: 0.3926\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3321 - val_loss: 0.3447\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3313 - val_loss: 0.3683\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3310 - val_loss: 0.3325\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3302 - val_loss: 0.3581\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3301 - val_loss: 0.3535\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3293 - val_loss: 0.3572\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3291 - val_loss: 0.3420\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3284 - val_loss: 0.3481\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3281 - val_loss: 0.3777\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3276 - val_loss: 0.3395\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3272 - val_loss: 0.3331\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3266 - val_loss: 0.3837\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3264 - val_loss: 0.3404\n",
      "2322/2322 [==============================] - 0s 46us/sample - loss: 0.3876\n",
      "[CV 2/5] END learning_rate=0.0012079427918657015, n_hidden=2, n_neurons=65;, score=-0.388 total time=  47.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.6440 - val_loss: 12.6496\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7012 - val_loss: 4.8323\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6114 - val_loss: 2.4937\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5655 - val_loss: 1.4168\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5307 - val_loss: 0.8940\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5041 - val_loss: 0.6392\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4829 - val_loss: 0.5173\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4660 - val_loss: 0.4626\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4531 - val_loss: 0.4396\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4424 - val_loss: 0.4299\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4331 - val_loss: 0.4426\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4257 - val_loss: 0.4406\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4188 - val_loss: 0.4626\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4129 - val_loss: 0.5373\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4083 - val_loss: 0.5912\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4037 - val_loss: 0.6446\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3997 - val_loss: 0.7263\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3958 - val_loss: 0.8064\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3924 - val_loss: 0.9215\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3893 - val_loss: 1.0227\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.4269\n",
      "[CV 3/5] END learning_rate=0.0012079427918657015, n_hidden=2, n_neurons=65;, score=-0.427 total time=  12.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 1.8394 - val_loss: 1.6308\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7978 - val_loss: 0.9514\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7132 - val_loss: 0.7746\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6598 - val_loss: 0.6780\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6213 - val_loss: 0.5631\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5849 - val_loss: 0.6144\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5498 - val_loss: 0.4899\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5233 - val_loss: 0.5008\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4990 - val_loss: 0.4509\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4816 - val_loss: 0.6283\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4648 - val_loss: 0.4426\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4526 - val_loss: 0.4470\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4407 - val_loss: 0.4001\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4323 - val_loss: 0.4244\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4247 - val_loss: 0.3859\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4190 - val_loss: 0.5001\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4136 - val_loss: 0.3790\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4087 - val_loss: 0.4580\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4047 - val_loss: 0.3704\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4003 - val_loss: 0.4488\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3978 - val_loss: 0.3898\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3940 - val_loss: 0.4595\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3916 - val_loss: 0.3807\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3895 - val_loss: 0.4174\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3871 - val_loss: 0.3557\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3843 - val_loss: 0.5165\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3843 - val_loss: 0.3579\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3802 - val_loss: 0.4075\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3782 - val_loss: 0.4711\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3769 - val_loss: 0.4097\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3750 - val_loss: 0.3468\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3731 - val_loss: 0.4730\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3725 - val_loss: 0.3495\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3703 - val_loss: 0.3927\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3691 - val_loss: 0.3435\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3676 - val_loss: 0.3825\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3666 - val_loss: 0.3607\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3650 - val_loss: 0.3669\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3636 - val_loss: 0.4160\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3633 - val_loss: 0.3685\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3615 - val_loss: 0.4235\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3610 - val_loss: 0.3361\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3599 - val_loss: 0.3558\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3588 - val_loss: 0.3368\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3577 - val_loss: 0.5592\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3577 - val_loss: 0.3357\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3564 - val_loss: 0.4081\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3551 - val_loss: 0.4302\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3548 - val_loss: 0.3352\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3534 - val_loss: 0.5465\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3538 - val_loss: 0.3329\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3515 - val_loss: 0.5711\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3529 - val_loss: 0.3414\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3513 - val_loss: 0.4523\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3517 - val_loss: 0.3369\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3493 - val_loss: 0.4936\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3498 - val_loss: 0.3459\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3482 - val_loss: 0.4408\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3476 - val_loss: 0.4003\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3468 - val_loss: 0.3560\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3461 - val_loss: 0.4743\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.3101\n",
      "[CV 4/5] END learning_rate=0.0012079427918657015, n_hidden=2, n_neurons=65;, score=-0.310 total time=  39.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 1.6031 - val_loss: 0.9501\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7232 - val_loss: 0.6585\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6462 - val_loss: 0.5957\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5928 - val_loss: 0.5530\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5497 - val_loss: 0.5109\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5141 - val_loss: 0.4801\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4858 - val_loss: 0.4579\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4632 - val_loss: 0.4405\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4451 - val_loss: 0.4211\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4310 - val_loss: 0.4135\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4200 - val_loss: 0.4236\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4114 - val_loss: 0.3962\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4042 - val_loss: 0.3887\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3985 - val_loss: 0.3892\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3935 - val_loss: 0.3853\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3890 - val_loss: 0.3787\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3853 - val_loss: 0.3752\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3820 - val_loss: 0.3827\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3789 - val_loss: 0.3838\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3767 - val_loss: 0.3765\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3743 - val_loss: 0.3680\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3722 - val_loss: 0.3669\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3702 - val_loss: 0.3798\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3689 - val_loss: 0.3634\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3671 - val_loss: 0.3722\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3656 - val_loss: 0.3993\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3643 - val_loss: 0.3905\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3623 - val_loss: 0.3795\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3620 - val_loss: 0.3547\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3608 - val_loss: 0.3595\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3589 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3585 - val_loss: 0.3645\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3571 - val_loss: 0.3616\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3558 - val_loss: 0.3870\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3552 - val_loss: 0.3508\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3541 - val_loss: 0.3859\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3532 - val_loss: 0.3625\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3519 - val_loss: 0.3818\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3512 - val_loss: 0.3735\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3507 - val_loss: 0.3409\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3495 - val_loss: 0.3414\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3481 - val_loss: 0.3756\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3480 - val_loss: 0.3444\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3467 - val_loss: 0.3720\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3464 - val_loss: 0.3611\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3456 - val_loss: 0.3375\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3448 - val_loss: 0.3437\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3439 - val_loss: 0.3518\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3430 - val_loss: 0.3358\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3424 - val_loss: 0.3385\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3413 - val_loss: 0.3673\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3414 - val_loss: 0.3359\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3400 - val_loss: 0.3783\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3396 - val_loss: 0.3438\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3388 - val_loss: 0.3866\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3383 - val_loss: 0.3383\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3370 - val_loss: 0.3702\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3371 - val_loss: 0.3306\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3361 - val_loss: 0.3477\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3353 - val_loss: 0.3810\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3350 - val_loss: 0.3683\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3345 - val_loss: 0.3611\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3339 - val_loss: 0.3291\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3331 - val_loss: 0.3289\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3326 - val_loss: 0.3357\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3315 - val_loss: 0.4097\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3317 - val_loss: 0.3291\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3309 - val_loss: 0.4027\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3306 - val_loss: 0.3554\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3294 - val_loss: 0.4187\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3301 - val_loss: 0.3244\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3291 - val_loss: 0.3496\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3285 - val_loss: 0.3732\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3280 - val_loss: 0.3711\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3273 - val_loss: 0.3433\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3272 - val_loss: 0.3270\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3267 - val_loss: 0.3497\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3256 - val_loss: 0.4042\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3264 - val_loss: 0.3524\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3257 - val_loss: 0.3688\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3252 - val_loss: 0.3270\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.3610\n",
      "[CV 5/5] END learning_rate=0.0012079427918657015, n_hidden=2, n_neurons=65;, score=-0.361 total time=  52.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 3.7360 - val_loss: 2.4621\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 2.0478 - val_loss: 1.6255\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.4309 - val_loss: 1.7674\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.1107 - val_loss: 1.8147\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.9258 - val_loss: 1.5912\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8164 - val_loss: 1.3846\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7511 - val_loss: 1.0961\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7115 - val_loss: 0.8780\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6861 - val_loss: 0.7668\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6683 - val_loss: 0.6963\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6543 - val_loss: 0.6547\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6425 - val_loss: 0.6304\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6320 - val_loss: 0.6146\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6221 - val_loss: 0.6011\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6130 - val_loss: 0.5867\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6046 - val_loss: 0.5760\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5965 - val_loss: 0.5685\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5889 - val_loss: 0.5580\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5818 - val_loss: 0.5503\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5748 - val_loss: 0.5426\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5682 - val_loss: 0.5365\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5618 - val_loss: 0.5311\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5556 - val_loss: 0.5261\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5498 - val_loss: 0.5206\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5442 - val_loss: 0.5148\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5388 - val_loss: 0.5086\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5336 - val_loss: 0.5038\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5286 - val_loss: 0.4991\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5237 - val_loss: 0.4936\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5189 - val_loss: 0.4896\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5144 - val_loss: 0.4845\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5100 - val_loss: 0.4802\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5056 - val_loss: 0.4760\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5015 - val_loss: 0.4716\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4975 - val_loss: 0.4676\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4935 - val_loss: 0.4629\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4896 - val_loss: 0.4590\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4858 - val_loss: 0.4552\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4820 - val_loss: 0.4517\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4786 - val_loss: 0.4477\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4752 - val_loss: 0.4441\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4718 - val_loss: 0.4409\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4687 - val_loss: 0.4380\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4655 - val_loss: 0.4352\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4625 - val_loss: 0.4319\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4595 - val_loss: 0.4292\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4568 - val_loss: 0.4261\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4541 - val_loss: 0.4235\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4515 - val_loss: 0.4210\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4489 - val_loss: 0.4186\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4465 - val_loss: 0.4162\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4441 - val_loss: 0.4143\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4419 - val_loss: 0.4121\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4397 - val_loss: 0.4100\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4376 - val_loss: 0.4081\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4356 - val_loss: 0.4063\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4336 - val_loss: 0.4044\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4317 - val_loss: 0.4029\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4299 - val_loss: 0.4011\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4280 - val_loss: 0.3996\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4263 - val_loss: 0.3977\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4246 - val_loss: 0.3963\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4230 - val_loss: 0.3950\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4215 - val_loss: 0.3936\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4199 - val_loss: 0.3924\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4185 - val_loss: 0.3913\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4171 - val_loss: 0.3902\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4157 - val_loss: 0.3886\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4145 - val_loss: 0.3877\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4132 - val_loss: 0.3863\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4120 - val_loss: 0.3852\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4108 - val_loss: 0.3846\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4097 - val_loss: 0.3834\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4085 - val_loss: 0.3823\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4075 - val_loss: 0.3816\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4065 - val_loss: 0.3814\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4055 - val_loss: 0.3804\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4045 - val_loss: 0.3794\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4036 - val_loss: 0.3788\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4027 - val_loss: 0.3784\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4018 - val_loss: 0.3773\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4010 - val_loss: 0.3770\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4001 - val_loss: 0.3764\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3994 - val_loss: 0.3758\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3986 - val_loss: 0.3753\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3978 - val_loss: 0.3743\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3970 - val_loss: 0.3750\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3963 - val_loss: 0.3728\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3956 - val_loss: 0.3724\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3949 - val_loss: 0.3720\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3943 - val_loss: 0.3717\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3936 - val_loss: 0.3715\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3929 - val_loss: 0.3706\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3923 - val_loss: 0.3709\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3917 - val_loss: 0.3698\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3910 - val_loss: 0.3700\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3905 - val_loss: 0.3698\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3900 - val_loss: 0.3684\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3894 - val_loss: 0.3683\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3889 - val_loss: 0.3749\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3726\n",
      "[CV 1/5] END learning_rate=0.00036839425321686054, n_hidden=2, n_neurons=13;, score=-0.373 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 105us/sample - loss: 3.1239 - val_loss: 2.8391\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.1103 - val_loss: 4.1941\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8585 - val_loss: 2.3813\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7698 - val_loss: 1.4162\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.7201 - val_loss: 0.9591\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6882 - val_loss: 0.7308\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6653 - val_loss: 0.6484\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6481 - val_loss: 0.6277\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6336 - val_loss: 0.6187\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6207 - val_loss: 0.6201\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6089 - val_loss: 0.6245\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5978 - val_loss: 0.6321\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5875 - val_loss: 0.6396\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5780 - val_loss: 0.6363\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5689 - val_loss: 0.6055\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5602 - val_loss: 0.5978\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5519 - val_loss: 0.6038\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5442 - val_loss: 0.5876\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5365 - val_loss: 0.5832\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5295 - val_loss: 0.5667\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5227 - val_loss: 0.5479\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5160 - val_loss: 0.5535\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5105 - val_loss: 0.5152\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5048 - val_loss: 0.5048\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4998 - val_loss: 0.4975\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4949 - val_loss: 0.4877\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4904 - val_loss: 0.4813\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4860 - val_loss: 0.4782\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4819 - val_loss: 0.4771\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4780 - val_loss: 0.4772\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4744 - val_loss: 0.4751\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4712 - val_loss: 0.4629\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4676 - val_loss: 0.4669\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4650 - val_loss: 0.4524\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4622 - val_loss: 0.4480\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4594 - val_loss: 0.4425\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4569 - val_loss: 0.4386\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4544 - val_loss: 0.4366\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4521 - val_loss: 0.4332\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4499 - val_loss: 0.4287\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4479 - val_loss: 0.4262\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4459 - val_loss: 0.4241\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4440 - val_loss: 0.4221\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4423 - val_loss: 0.4214\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4405 - val_loss: 0.4187\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4389 - val_loss: 0.4191\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4373 - val_loss: 0.4167\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4358 - val_loss: 0.4178\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4344 - val_loss: 0.4169\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4329 - val_loss: 0.4149\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4316 - val_loss: 0.4148\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4303 - val_loss: 0.4135\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4289 - val_loss: 0.4147\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4277 - val_loss: 0.4112\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4265 - val_loss: 0.4113\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4253 - val_loss: 0.4089\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4242 - val_loss: 0.4068\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4230 - val_loss: 0.4045\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4220 - val_loss: 0.4057\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4210 - val_loss: 0.4039\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4200 - val_loss: 0.4038\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4189 - val_loss: 0.4033\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4180 - val_loss: 0.4040\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4171 - val_loss: 0.4028\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4160 - val_loss: 0.3997\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4152 - val_loss: 0.4045\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4144 - val_loss: 0.4021\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4135 - val_loss: 0.4005\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4127 - val_loss: 0.4035\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4118 - val_loss: 0.4027\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4111 - val_loss: 0.4052\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4103 - val_loss: 0.4045\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4095 - val_loss: 0.4034\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4088 - val_loss: 0.3975\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4082 - val_loss: 0.3994\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4075 - val_loss: 0.4022\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4068 - val_loss: 0.4000\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4061 - val_loss: 0.3959\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4055 - val_loss: 0.3984\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4048 - val_loss: 0.4023\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4043 - val_loss: 0.4015\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4037 - val_loss: 0.4008\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4031 - val_loss: 0.4008\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4024 - val_loss: 0.3952\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4019 - val_loss: 0.3983\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4014 - val_loss: 0.4005\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4008 - val_loss: 0.3997\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4003 - val_loss: 0.3979\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3998 - val_loss: 0.3978\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3992 - val_loss: 0.3961\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3988 - val_loss: 0.4002\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3982 - val_loss: 0.3952\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3978 - val_loss: 0.3968\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3972 - val_loss: 0.4011\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4393\n",
      "[CV 2/5] END learning_rate=0.00036839425321686054, n_hidden=2, n_neurons=13;, score=-0.439 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 3.5990 - val_loss: 9.1160\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.8773 - val_loss: 11.5479\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3130 - val_loss: 12.0225\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.0669 - val_loss: 11.6759\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.9427 - val_loss: 10.9090\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8733 - val_loss: 10.2218\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8289 - val_loss: 9.4642\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7957 - val_loss: 8.7757\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7685 - val_loss: 8.2545\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7457 - val_loss: 7.7086\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7253 - val_loss: 7.2853\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7068 - val_loss: 6.8555\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6895 - val_loss: 6.4070\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6735 - val_loss: 6.1271\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6583 - val_loss: 5.7607\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6435 - val_loss: 5.4760\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6294 - val_loss: 5.2143\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6158 - val_loss: 4.9600\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6029 - val_loss: 4.7359\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5909 - val_loss: 4.4834\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5796 - val_loss: 4.2650\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5687 - val_loss: 4.0628\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5583 - val_loss: 3.8951\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5483 - val_loss: 3.7385\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5389 - val_loss: 3.5608\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5303 - val_loss: 3.4047\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5222 - val_loss: 3.2609\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5148 - val_loss: 3.1324\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5078 - val_loss: 2.9864\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5016 - val_loss: 2.8853\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4955 - val_loss: 2.7869\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4898 - val_loss: 2.6983\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4846 - val_loss: 2.6165\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4796 - val_loss: 2.5493\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4750 - val_loss: 2.4810\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4707 - val_loss: 2.4230\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4668 - val_loss: 2.3686\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4628 - val_loss: 2.3254\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4594 - val_loss: 2.2801\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4560 - val_loss: 2.2468\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4529 - val_loss: 2.2176\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4499 - val_loss: 2.1884\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4471 - val_loss: 2.1511\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4446 - val_loss: 2.1380\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4424 - val_loss: 2.1177\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4402 - val_loss: 2.0926\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4383 - val_loss: 2.0676\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4363 - val_loss: 2.0671\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4347 - val_loss: 2.0521\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4323 - val_loss: 2.0191\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4318 - val_loss: 2.0330\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4301 - val_loss: 2.0283\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4285 - val_loss: 2.0103\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4275 - val_loss: 2.0216\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4259 - val_loss: 2.0141\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4250 - val_loss: 1.9915\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4242 - val_loss: 1.9992\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4228 - val_loss: 2.0023\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4217 - val_loss: 1.9945\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4207 - val_loss: 2.0052\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4194 - val_loss: 1.9868\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4189 - val_loss: 1.9768\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4185 - val_loss: 1.9946\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4176 - val_loss: 2.0064\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4170 - val_loss: 1.9915\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4162 - val_loss: 1.9831\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4155 - val_loss: 1.9927\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4146 - val_loss: 1.9939\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4143 - val_loss: 1.9905\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4135 - val_loss: 1.9905\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4125 - val_loss: 1.9904\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4122 - val_loss: 1.9879\n",
      "2322/2322 [==============================] - 0s 40us/sample - loss: 0.4814\n",
      "[CV 3/5] END learning_rate=0.00036839425321686054, n_hidden=2, n_neurons=13;, score=-0.481 total time=  46.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 3.8201 - val_loss: 8.0664\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 2.3332 - val_loss: 5.7659\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.7014 - val_loss: 3.0922\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.3272 - val_loss: 2.0253\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.0728 - val_loss: 1.3906\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.9102 - val_loss: 1.0133\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.8144 - val_loss: 0.8236\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7597 - val_loss: 0.7292\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.7267 - val_loss: 0.6847\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7043 - val_loss: 0.6587\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6867 - val_loss: 0.6409\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6713 - val_loss: 0.6265\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6575 - val_loss: 0.6142\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6449 - val_loss: 0.6030\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6330 - val_loss: 0.5929\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6220 - val_loss: 0.5818\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6114 - val_loss: 0.5719\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6015 - val_loss: 0.5614\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5921 - val_loss: 0.5508\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5830 - val_loss: 0.5414\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5742 - val_loss: 0.5327\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5660 - val_loss: 0.5246\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5582 - val_loss: 0.5171\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5509 - val_loss: 0.5099\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5441 - val_loss: 0.5029\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5375 - val_loss: 0.4961\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5312 - val_loss: 0.4901\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5252 - val_loss: 0.4845\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5196 - val_loss: 0.4795\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5143 - val_loss: 0.4750\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5094 - val_loss: 0.4705\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5047 - val_loss: 0.4669\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5002 - val_loss: 0.4636\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4962 - val_loss: 0.4612\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4924 - val_loss: 0.4590\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4887 - val_loss: 0.4580\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4854 - val_loss: 0.4574\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4821 - val_loss: 0.4552\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4791 - val_loss: 0.4544\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4762 - val_loss: 0.4539\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4735 - val_loss: 0.4529\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4708 - val_loss: 0.4530\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4685 - val_loss: 0.4551\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4661 - val_loss: 0.4563\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4639 - val_loss: 0.4538\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4618 - val_loss: 0.4522\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4598 - val_loss: 0.4544\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4579 - val_loss: 0.4540\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4560 - val_loss: 0.4528\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4543 - val_loss: 0.4497\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4525 - val_loss: 0.4498\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4509 - val_loss: 0.4491\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4493 - val_loss: 0.4502\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4477 - val_loss: 0.4484\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4462 - val_loss: 0.4470\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4447 - val_loss: 0.4486\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4433 - val_loss: 0.4495\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4419 - val_loss: 0.4484\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4406 - val_loss: 0.4475\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4393 - val_loss: 0.4491\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4381 - val_loss: 0.4489\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4369 - val_loss: 0.4499\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4357 - val_loss: 0.4480\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4345 - val_loss: 0.4440\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4335 - val_loss: 0.4469\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4324 - val_loss: 0.4472\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4314 - val_loss: 0.4477\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4304 - val_loss: 0.4486\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4295 - val_loss: 0.4494\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4286 - val_loss: 0.4500\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4276 - val_loss: 0.4486\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4267 - val_loss: 0.4476\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4259 - val_loss: 0.4465\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4250 - val_loss: 0.4465\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3728\n",
      "[CV 4/5] END learning_rate=0.00036839425321686054, n_hidden=2, n_neurons=13;, score=-0.373 total time=  48.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 3.6510 - val_loss: 2.1180\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.4248 - val_loss: 1.3991\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.9471 - val_loss: 1.0241\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.8052 - val_loss: 0.8175\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7383 - val_loss: 0.7217\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6973 - val_loss: 0.6743\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6682 - val_loss: 0.6435\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6452 - val_loss: 0.6211\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6263 - val_loss: 0.6014\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6100 - val_loss: 0.5863\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5958 - val_loss: 0.5719\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5830 - val_loss: 0.5579\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5712 - val_loss: 0.5449\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5601 - val_loss: 0.5314\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5502 - val_loss: 0.5213\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5405 - val_loss: 0.5115\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5316 - val_loss: 0.5021\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5232 - val_loss: 0.4933\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5153 - val_loss: 0.4848\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5076 - val_loss: 0.4776\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5005 - val_loss: 0.4698\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4940 - val_loss: 0.4629\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4879 - val_loss: 0.4567\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4819 - val_loss: 0.4513\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4766 - val_loss: 0.4460\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4717 - val_loss: 0.4412\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4669 - val_loss: 0.4371\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4627 - val_loss: 0.4329\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4585 - val_loss: 0.4294\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4547 - val_loss: 0.4266\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4511 - val_loss: 0.4237\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4476 - val_loss: 0.4204\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4446 - val_loss: 0.4185\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4417 - val_loss: 0.4157\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4390 - val_loss: 0.4143\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4365 - val_loss: 0.4135\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4340 - val_loss: 0.4119\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4317 - val_loss: 0.4102\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4296 - val_loss: 0.4114\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4276 - val_loss: 0.4094\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4257 - val_loss: 0.4063\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4240 - val_loss: 0.4073\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4223 - val_loss: 0.4080\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4208 - val_loss: 0.4052\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4194 - val_loss: 0.4051\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4180 - val_loss: 0.4041\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4167 - val_loss: 0.4053\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4155 - val_loss: 0.4039\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4144 - val_loss: 0.4041\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4133 - val_loss: 0.4034\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4122 - val_loss: 0.4027\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4112 - val_loss: 0.4014\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4101 - val_loss: 0.4045\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4094 - val_loss: 0.4020\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4085 - val_loss: 0.4026\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4074 - val_loss: 0.3991\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4068 - val_loss: 0.4014\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4059 - val_loss: 0.4043\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4052 - val_loss: 0.4019\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4044 - val_loss: 0.4008\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4037 - val_loss: 0.4010\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4031 - val_loss: 0.4000\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4024 - val_loss: 0.3969\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4018 - val_loss: 0.3967\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4012 - val_loss: 0.3990\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4005 - val_loss: 0.3984\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3999 - val_loss: 0.4008\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3994 - val_loss: 0.3981\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3988 - val_loss: 0.3984\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3983 - val_loss: 0.3999\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3978 - val_loss: 0.3972\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3972 - val_loss: 0.3977\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3967 - val_loss: 0.3951\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3962 - val_loss: 0.3957\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3957 - val_loss: 0.3956\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3952 - val_loss: 0.3972\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3948 - val_loss: 0.3931\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3943 - val_loss: 0.3962\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3940 - val_loss: 0.3947\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3936 - val_loss: 0.3946\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3932 - val_loss: 0.3919\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3927 - val_loss: 0.3923\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3923 - val_loss: 0.3938\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3920 - val_loss: 0.3943\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3916 - val_loss: 0.3911\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3912 - val_loss: 0.3891\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3908 - val_loss: 0.3904\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3904 - val_loss: 0.3928\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3901 - val_loss: 0.3918\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3898 - val_loss: 0.3919\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3894 - val_loss: 0.3907\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3890 - val_loss: 0.3898\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3887 - val_loss: 0.3876\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3884 - val_loss: 0.3904\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3881 - val_loss: 0.3891\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3878 - val_loss: 0.3897\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3875 - val_loss: 0.3897\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3871 - val_loss: 0.3877\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3868 - val_loss: 0.3883\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3865 - val_loss: 0.3887\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.4087\n",
      "[CV 5/5] END learning_rate=0.00036839425321686054, n_hidden=2, n_neurons=13;, score=-0.409 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 2.8518 - val_loss: 4.7234\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.9527 - val_loss: 0.9584\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7200 - val_loss: 0.6590\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6685 - val_loss: 0.6341\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6390 - val_loss: 0.6410\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6156 - val_loss: 0.5784\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5938 - val_loss: 0.5695\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5751 - val_loss: 0.5652\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5590 - val_loss: 0.5305\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5445 - val_loss: 0.5008\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5311 - val_loss: 0.4939\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5197 - val_loss: 0.4763\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5091 - val_loss: 0.4703\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4996 - val_loss: 0.4654\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4913 - val_loss: 0.4620\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4840 - val_loss: 0.4429\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4769 - val_loss: 0.6345\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4723 - val_loss: 0.4397\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4650 - val_loss: 0.4235\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4599 - val_loss: 0.4188\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4555 - val_loss: 0.4144\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4513 - val_loss: 0.4103\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4477 - val_loss: 0.4069\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4443 - val_loss: 0.4037\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4412 - val_loss: 0.4009\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4383 - val_loss: 0.3983\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4357 - val_loss: 0.3961\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4331 - val_loss: 0.3941\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4310 - val_loss: 0.3936\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4286 - val_loss: 0.3901\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4266 - val_loss: 0.3914\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4245 - val_loss: 0.3870\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4227 - val_loss: 0.3867\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4208 - val_loss: 0.3849\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4190 - val_loss: 0.3831\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4173 - val_loss: 0.3821\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4157 - val_loss: 0.3844\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4142 - val_loss: 0.3795\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4126 - val_loss: 0.3781\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4111 - val_loss: 0.3753\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4098 - val_loss: 0.3746\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.4083 - val_loss: 0.3755\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.4070 - val_loss: 0.3720\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4058 - val_loss: 0.3728\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.4045 - val_loss: 0.3756\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4035 - val_loss: 0.3699\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4023 - val_loss: 0.3692\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4012 - val_loss: 0.3724\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4001 - val_loss: 0.3711\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3992 - val_loss: 0.3678\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3981 - val_loss: 0.3649\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3972 - val_loss: 0.3663\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3963 - val_loss: 0.3662\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3953 - val_loss: 0.3630\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3945 - val_loss: 0.3654\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3936 - val_loss: 0.3666\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3929 - val_loss: 0.3658\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3922 - val_loss: 0.3613\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3913 - val_loss: 0.3645\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3904 - val_loss: 0.3657\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3898 - val_loss: 0.3627\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3890 - val_loss: 0.3573\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3883 - val_loss: 0.3612\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3875 - val_loss: 0.3572\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3868 - val_loss: 0.3563\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3861 - val_loss: 0.3550\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3854 - val_loss: 0.3561\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3847 - val_loss: 0.3542\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3841 - val_loss: 0.3625\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3835 - val_loss: 0.3586\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3829 - val_loss: 0.3556\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3823 - val_loss: 0.3580\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3816 - val_loss: 0.3553\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3812 - val_loss: 0.3528\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3805 - val_loss: 0.3557\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3800 - val_loss: 0.3552\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3794 - val_loss: 0.3577\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3789 - val_loss: 0.3501\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3784 - val_loss: 0.3566\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3779 - val_loss: 0.3542\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3772 - val_loss: 0.3594\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3766 - val_loss: 0.3495\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3763 - val_loss: 0.3479\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3758 - val_loss: 0.3521\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3751 - val_loss: 0.3515\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3749 - val_loss: 0.3568\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3744 - val_loss: 0.3457\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3740 - val_loss: 0.3527\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3736 - val_loss: 0.3469\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3730 - val_loss: 0.3458\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3725 - val_loss: 0.3531\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3722 - val_loss: 0.3467\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3717 - val_loss: 0.3564\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3713 - val_loss: 0.3509\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3709 - val_loss: 0.3495\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3705 - val_loss: 0.3488\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3701 - val_loss: 0.3474\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3517\n",
      "[CV 1/5] END learning_rate=0.0008315883696811512, n_hidden=1, n_neurons=88;, score=-0.352 total time=  57.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 2.2107 - val_loss: 1.7774\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7592 - val_loss: 0.7479\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6411 - val_loss: 0.6289\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6057 - val_loss: 0.5990\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5798 - val_loss: 0.5633\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5583 - val_loss: 0.5421\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5390 - val_loss: 0.5257\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5231 - val_loss: 0.5076\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5087 - val_loss: 0.4980\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4964 - val_loss: 0.4903\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4854 - val_loss: 0.4719\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4761 - val_loss: 0.4668\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4676 - val_loss: 0.4687\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4603 - val_loss: 0.4499\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4539 - val_loss: 0.4493\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4481 - val_loss: 0.4415\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4429 - val_loss: 0.4407\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4382 - val_loss: 0.4462\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4341 - val_loss: 0.4399\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4303 - val_loss: 0.4300\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4270 - val_loss: 0.4296\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4236 - val_loss: 0.4345\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4208 - val_loss: 0.4238\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4181 - val_loss: 0.4238\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4154 - val_loss: 0.4282\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4131 - val_loss: 0.4247\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4110 - val_loss: 0.4213\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4088 - val_loss: 0.4166\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4070 - val_loss: 0.4191\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4051 - val_loss: 0.4178\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4033 - val_loss: 0.4157\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4015 - val_loss: 0.4230\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4001 - val_loss: 0.4096\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3985 - val_loss: 0.4163\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3970 - val_loss: 0.4105\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3956 - val_loss: 0.4220\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3944 - val_loss: 0.4166\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3929 - val_loss: 0.4207\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3918 - val_loss: 0.4211\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3906 - val_loss: 0.4154\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3894 - val_loss: 0.4108\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3883 - val_loss: 0.4229\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3873 - val_loss: 0.4142\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4283\n",
      "[CV 2/5] END learning_rate=0.0008315883696811512, n_hidden=1, n_neurons=88;, score=-0.428 total time=  26.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 2.0160 - val_loss: 23.5181\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8952 - val_loss: 17.2282\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7615 - val_loss: 10.4139\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7012 - val_loss: 5.9179\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6583 - val_loss: 3.3337\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6243 - val_loss: 1.8619\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5958 - val_loss: 1.0366\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5720 - val_loss: 0.6282\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5518 - val_loss: 0.5317\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5351 - val_loss: 0.5710\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5204 - val_loss: 0.7029\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5076 - val_loss: 0.8120\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4966 - val_loss: 0.9237\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4868 - val_loss: 1.0055\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4782 - val_loss: 1.0651\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4709 - val_loss: 1.1047\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4641 - val_loss: 1.1204\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4583 - val_loss: 1.1021\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4528 - val_loss: 1.0712\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.5102\n",
      "[CV 3/5] END learning_rate=0.0008315883696811512, n_hidden=1, n_neurons=88;, score=-0.510 total time=  11.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 2.7345 - val_loss: 5.6714\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.9106 - val_loss: 1.1660\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7142 - val_loss: 0.6505\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6716 - val_loss: 0.6160\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6443 - val_loss: 0.5949\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6211 - val_loss: 0.5708\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6003 - val_loss: 0.5501\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5815 - val_loss: 0.5330\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5647 - val_loss: 0.5160\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5496 - val_loss: 0.5012\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5359 - val_loss: 0.4899\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5237 - val_loss: 0.4766\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5125 - val_loss: 0.4660\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5023 - val_loss: 0.4589\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4934 - val_loss: 0.4485\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4854 - val_loss: 0.4464\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4782 - val_loss: 0.4357\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4719 - val_loss: 0.4324\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4661 - val_loss: 0.4266\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4611 - val_loss: 0.4243\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4565 - val_loss: 0.4283\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4524 - val_loss: 0.4181\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4487 - val_loss: 0.4138\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4452 - val_loss: 0.4101\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4422 - val_loss: 0.4126\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4394 - val_loss: 0.4125\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4367 - val_loss: 0.4118\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4342 - val_loss: 0.4086\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4318 - val_loss: 0.4119\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4295 - val_loss: 0.4067\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4275 - val_loss: 0.4004\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4256 - val_loss: 0.4072\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4238 - val_loss: 0.4130\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4220 - val_loss: 0.3958\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4205 - val_loss: 0.4016\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4190 - val_loss: 0.4087\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4174 - val_loss: 0.4146\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4162 - val_loss: 0.3965\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4148 - val_loss: 0.4018\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4135 - val_loss: 0.4044\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4122 - val_loss: 0.4068\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4112 - val_loss: 0.3965\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4099 - val_loss: 0.4027\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4089 - val_loss: 0.4004\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3577\n",
      "[CV 4/5] END learning_rate=0.0008315883696811512, n_hidden=1, n_neurons=88;, score=-0.358 total time=  27.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 2.1077 - val_loss: 0.9766\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7855 - val_loss: 0.7832\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6930 - val_loss: 0.6840\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6598 - val_loss: 0.6526\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6295 - val_loss: 0.6513\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6036 - val_loss: 0.5904\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5793 - val_loss: 0.5559\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5591 - val_loss: 0.5305\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5395 - val_loss: 0.5129\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5222 - val_loss: 0.4970\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5077 - val_loss: 0.4969\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4938 - val_loss: 0.4722\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4824 - val_loss: 0.4640\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4724 - val_loss: 0.4711\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4631 - val_loss: 0.4436\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4556 - val_loss: 0.4491\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4485 - val_loss: 0.4727\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4424 - val_loss: 0.4344\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4372 - val_loss: 0.4274\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4324 - val_loss: 0.4294\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4281 - val_loss: 0.4230\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4242 - val_loss: 0.4322\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4210 - val_loss: 0.4417\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4178 - val_loss: 0.4153\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4149 - val_loss: 0.4183\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4123 - val_loss: 0.4233\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4097 - val_loss: 0.4063\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4071 - val_loss: 0.4493\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4055 - val_loss: 0.4195\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4033 - val_loss: 0.4076\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4014 - val_loss: 0.4026\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3995 - val_loss: 0.4173\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3979 - val_loss: 0.3961\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3962 - val_loss: 0.4372\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3948 - val_loss: 0.3928\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3933 - val_loss: 0.4141\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3918 - val_loss: 0.4213\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3906 - val_loss: 0.4321\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3894 - val_loss: 0.4233\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3880 - val_loss: 0.4265\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3869 - val_loss: 0.4012\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3856 - val_loss: 0.4282\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3848 - val_loss: 0.4030\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3835 - val_loss: 0.4108\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3827 - val_loss: 0.4048\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4213\n",
      "[CV 5/5] END learning_rate=0.0008315883696811512, n_hidden=1, n_neurons=88;, score=-0.421 total time=  27.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 1.2596 - val_loss: 7.2825\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6855 - val_loss: 7.3085\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5819 - val_loss: 1.5260\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5177 - val_loss: 0.9903\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4737 - val_loss: 0.4444\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4545 - val_loss: 0.4525\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4436 - val_loss: 0.5036\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4354 - val_loss: 0.4999\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4302 - val_loss: 0.4043\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4248 - val_loss: 0.4803\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4210 - val_loss: 0.3964\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4175 - val_loss: 0.4529\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4143 - val_loss: 0.4075\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4116 - val_loss: 0.4074\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4087 - val_loss: 0.4078\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4062 - val_loss: 0.4117\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4040 - val_loss: 0.4250\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4016 - val_loss: 0.3814\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3990 - val_loss: 0.4679\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3977 - val_loss: 0.3663\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3961 - val_loss: 0.5246\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3937 - val_loss: 0.3660\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3921 - val_loss: 0.5915\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3907 - val_loss: 0.3999\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3889 - val_loss: 0.3698\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3863 - val_loss: 0.6470\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3868 - val_loss: 0.3985\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3851 - val_loss: 0.7491\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3852 - val_loss: 0.3683\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3821 - val_loss: 0.4825\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3807 - val_loss: 0.3522\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3795 - val_loss: 0.4405\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3776 - val_loss: 0.3562\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3780 - val_loss: 0.5091\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3760 - val_loss: 0.4055\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3751 - val_loss: 0.4011\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3742 - val_loss: 0.3679\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3724 - val_loss: 0.5219\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3720 - val_loss: 0.3815\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3718 - val_loss: 0.5937\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3706 - val_loss: 0.3544\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3485\n",
      "[CV 1/5] END learning_rate=0.002540632586606343, n_hidden=1, n_neurons=53;, score=-0.349 total time=  25.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 1.2458 - val_loss: 5.1591\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7141 - val_loss: 4.5825\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6227 - val_loss: 1.2256\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5560 - val_loss: 0.5688\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5055 - val_loss: 0.4742\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4775 - val_loss: 0.4820\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4565 - val_loss: 0.4281\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4419 - val_loss: 0.4228\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4317 - val_loss: 0.4075\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4229 - val_loss: 0.3995\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4167 - val_loss: 0.3996\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4107 - val_loss: 0.3897\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4058 - val_loss: 0.3892\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4015 - val_loss: 0.3845\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3978 - val_loss: 0.3988\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3943 - val_loss: 0.3815\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3909 - val_loss: 0.3864\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3886 - val_loss: 0.3684\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3854 - val_loss: 0.3889\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3835 - val_loss: 0.3763\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3811 - val_loss: 0.3706\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3790 - val_loss: 0.4014\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3775 - val_loss: 0.3624\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3753 - val_loss: 0.3633\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3733 - val_loss: 0.3640\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3717 - val_loss: 0.3614\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3698 - val_loss: 0.3963\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3686 - val_loss: 0.3614\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3673 - val_loss: 0.3594\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3653 - val_loss: 0.3843\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3643 - val_loss: 0.3645\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3630 - val_loss: 0.3642\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3617 - val_loss: 0.3458\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3604 - val_loss: 0.3667\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3594 - val_loss: 0.3805\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3582 - val_loss: 0.3492\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3569 - val_loss: 0.3932\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3565 - val_loss: 0.3444\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3550 - val_loss: 0.3480\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3541 - val_loss: 0.3767\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3534 - val_loss: 0.3524\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3527 - val_loss: 0.4946\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3522 - val_loss: 0.3537\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3511 - val_loss: 0.3954\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3504 - val_loss: 0.3402\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3494 - val_loss: 0.4021\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3489 - val_loss: 0.3380\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3482 - val_loss: 0.3654\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3472 - val_loss: 0.3412\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3468 - val_loss: 0.3353\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3458 - val_loss: 0.3569\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3454 - val_loss: 0.3410\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3444 - val_loss: 0.3550\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3441 - val_loss: 0.3587\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3436 - val_loss: 0.3379\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3425 - val_loss: 0.3375\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3422 - val_loss: 0.3407\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3419 - val_loss: 0.3419\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3408 - val_loss: 0.3635\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3402 - val_loss: 0.3397\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3915\n",
      "[CV 2/5] END learning_rate=0.002540632586606343, n_hidden=1, n_neurons=53;, score=-0.391 total time=  37.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 1.4855 - val_loss: 10.3490\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6788 - val_loss: 2.6769\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5955 - val_loss: 0.8098\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5397 - val_loss: 0.5025\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5022 - val_loss: 0.5441\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4753 - val_loss: 0.6478\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4575 - val_loss: 0.6403\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4426 - val_loss: 0.5659\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4316 - val_loss: 0.5185\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4234 - val_loss: 0.4743\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4171 - val_loss: 0.4118\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4108 - val_loss: 0.3925\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4067 - val_loss: 0.3829\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4024 - val_loss: 0.3881\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3984 - val_loss: 0.4028\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3955 - val_loss: 0.4574\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3924 - val_loss: 0.4676\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3894 - val_loss: 0.4914\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3871 - val_loss: 0.5389\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3848 - val_loss: 0.5555\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3827 - val_loss: 0.5577\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3807 - val_loss: 0.7239\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3797 - val_loss: 0.6977\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.4154\n",
      "[CV 3/5] END learning_rate=0.002540632586606343, n_hidden=1, n_neurons=53;, score=-0.415 total time=  14.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 104us/sample - loss: 1.1676 - val_loss: 8.7887\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7196 - val_loss: 3.2611\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5628 - val_loss: 0.7098\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5037 - val_loss: 0.5350\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4744 - val_loss: 0.4382\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4574 - val_loss: 0.4095\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4468 - val_loss: 0.4158\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4379 - val_loss: 0.3952\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4323 - val_loss: 0.4208\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4271 - val_loss: 0.3896\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4225 - val_loss: 0.3966\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4190 - val_loss: 0.3812\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4153 - val_loss: 0.4131\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4127 - val_loss: 0.3887\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4100 - val_loss: 0.3684\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4071 - val_loss: 0.3810\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4046 - val_loss: 0.3989\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4025 - val_loss: 0.4385\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4004 - val_loss: 0.4312\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3987 - val_loss: 0.4211\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3971 - val_loss: 0.3623\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3953 - val_loss: 0.3593\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3937 - val_loss: 0.3564\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3913 - val_loss: 0.5075\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3911 - val_loss: 0.3823\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3889 - val_loss: 0.4057\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3881 - val_loss: 0.3544\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3864 - val_loss: 0.4182\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3851 - val_loss: 0.3545\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3827 - val_loss: 0.3593\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3820 - val_loss: 0.3507\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3806 - val_loss: 0.3871\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3798 - val_loss: 0.3837\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3793 - val_loss: 0.3475\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3778 - val_loss: 0.3565\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3763 - val_loss: 0.4168\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3760 - val_loss: 0.3431\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3743 - val_loss: 0.3601\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3737 - val_loss: 0.3477\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3722 - val_loss: 0.4229\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3714 - val_loss: 0.4088\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3710 - val_loss: 0.3425\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3691 - val_loss: 0.5138\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3702 - val_loss: 0.3443\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3677 - val_loss: 0.3874\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3673 - val_loss: 0.3421\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3666 - val_loss: 0.3503\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3661 - val_loss: 0.3398\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3646 - val_loss: 0.4563\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3648 - val_loss: 0.3374\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3630 - val_loss: 0.4750\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3635 - val_loss: 0.3448\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3620 - val_loss: 0.3393\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3611 - val_loss: 0.3913\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3606 - val_loss: 0.3527\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3598 - val_loss: 0.4063\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3593 - val_loss: 0.4064\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3590 - val_loss: 0.3419\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3578 - val_loss: 0.3314\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3564 - val_loss: 0.4927\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3573 - val_loss: 0.3648\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3553 - val_loss: 0.3949\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3560 - val_loss: 0.3680\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3553 - val_loss: 0.5683\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3556 - val_loss: 0.3707\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3540 - val_loss: 0.3785\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3540 - val_loss: 0.3329\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3524 - val_loss: 0.3418\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3523 - val_loss: 0.3278\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3515 - val_loss: 0.3445\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3515 - val_loss: 0.3282\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3507 - val_loss: 0.4033\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3519 - val_loss: 0.3283\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3496 - val_loss: 0.3710\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3494 - val_loss: 0.3633\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3495 - val_loss: 0.3653\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3482 - val_loss: 0.4245\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3487 - val_loss: 0.3246\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3478 - val_loss: 0.3976\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3477 - val_loss: 0.3337\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3477 - val_loss: 0.3737\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3466 - val_loss: 0.3303\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3458 - val_loss: 0.4681\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3461 - val_loss: 0.3442\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3454 - val_loss: 0.3710\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3454 - val_loss: 0.3264\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3440 - val_loss: 0.4454\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3438 - val_loss: 0.3328\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3000\n",
      "[CV 4/5] END learning_rate=0.002540632586606343, n_hidden=1, n_neurons=53;, score=-0.300 total time=  53.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 1.4102 - val_loss: 4.8898\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6648 - val_loss: 2.2608\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5627 - val_loss: 0.9782\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5125 - val_loss: 0.7218\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4809 - val_loss: 0.4904\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4601 - val_loss: 0.4752\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4468 - val_loss: 0.4268\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4361 - val_loss: 0.4654\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4274 - val_loss: 0.4107\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4206 - val_loss: 0.4275\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4148 - val_loss: 0.3864\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4088 - val_loss: 0.5500\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4073 - val_loss: 0.3803\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4021 - val_loss: 0.3820\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4002 - val_loss: 0.4035\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3955 - val_loss: 0.4253\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3940 - val_loss: 0.3811\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3910 - val_loss: 0.5007\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3905 - val_loss: 0.4500\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3866 - val_loss: 0.4035\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3862 - val_loss: 0.4229\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3817 - val_loss: 0.3633\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3796 - val_loss: 0.4574\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3802 - val_loss: 0.4987\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3771 - val_loss: 0.8111\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3792 - val_loss: 0.8031\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3769 - val_loss: 0.6001\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3766 - val_loss: 0.5615\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3725 - val_loss: 0.8101\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3754 - val_loss: 0.5930\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3698 - val_loss: 0.6167\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3730 - val_loss: 0.7287\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3998\n",
      "[CV 5/5] END learning_rate=0.002540632586606343, n_hidden=1, n_neurons=53;, score=-0.400 total time=  19.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 88us/sample - loss: 2.5475 - val_loss: 1.6189\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.1927 - val_loss: 1.2218\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.9404 - val_loss: 0.8896\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.8528 - val_loss: 0.7783\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8042 - val_loss: 0.7318\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7702 - val_loss: 0.7103\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7436 - val_loss: 0.6866\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7211 - val_loss: 0.6715\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7015 - val_loss: 0.6694\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6842 - val_loss: 0.6542\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6683 - val_loss: 0.6386\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6534 - val_loss: 0.6228\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6395 - val_loss: 0.6109\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6263 - val_loss: 0.6032\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6138 - val_loss: 0.6041\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6021 - val_loss: 0.5915\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5908 - val_loss: 0.5747\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5802 - val_loss: 0.5682\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5704 - val_loss: 0.5438\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5607 - val_loss: 0.5395\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5518 - val_loss: 0.5226\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5432 - val_loss: 0.5072\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5353 - val_loss: 0.4964\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5277 - val_loss: 0.4876\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5204 - val_loss: 0.4846\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5139 - val_loss: 0.4766\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5078 - val_loss: 0.4683\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5020 - val_loss: 0.4640\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4967 - val_loss: 0.4565\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4916 - val_loss: 0.4512\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4870 - val_loss: 0.4458\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4828 - val_loss: 0.4422\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4789 - val_loss: 0.4377\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4751 - val_loss: 0.4339\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4716 - val_loss: 0.4308\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4684 - val_loss: 0.4276\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4654 - val_loss: 0.4251\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4625 - val_loss: 0.4237\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4599 - val_loss: 0.4207\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4574 - val_loss: 0.4199\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4550 - val_loss: 0.4175\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4527 - val_loss: 0.4159\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4506 - val_loss: 0.4137\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4486 - val_loss: 0.4119\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4467 - val_loss: 0.4120\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4449 - val_loss: 0.4099\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4432 - val_loss: 0.4105\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4415 - val_loss: 0.4090\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4399 - val_loss: 0.4065\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4383 - val_loss: 0.4073\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4369 - val_loss: 0.4080\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4355 - val_loss: 0.4068\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4341 - val_loss: 0.4064\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4329 - val_loss: 0.4057\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4316 - val_loss: 0.4062\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4304 - val_loss: 0.4051\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4292 - val_loss: 0.4034\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4282 - val_loss: 0.4038\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4270 - val_loss: 0.4067\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4260 - val_loss: 0.4026\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4250 - val_loss: 0.4046\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4240 - val_loss: 0.4027\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4230 - val_loss: 0.4061\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4221 - val_loss: 0.4025\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4212 - val_loss: 0.4020\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4204 - val_loss: 0.4028\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4196 - val_loss: 0.4055\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4188 - val_loss: 0.4077\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4180 - val_loss: 0.4048\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4172 - val_loss: 0.4063\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4164 - val_loss: 0.4096\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4157 - val_loss: 0.4132\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4150 - val_loss: 0.4080\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4143 - val_loss: 0.4051\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4136 - val_loss: 0.4103\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3747\n",
      "[CV 1/5] END learning_rate=0.0004865926131115961, n_hidden=1, n_neurons=37;, score=-0.375 total time=  46.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 4.6999 - val_loss: 3.1795\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.8695 - val_loss: 1.3626\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.1428 - val_loss: 0.9420\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8733 - val_loss: 0.7983\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7616 - val_loss: 0.7372\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7091 - val_loss: 0.7009\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6796 - val_loss: 0.6752\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6594 - val_loss: 0.6508\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6433 - val_loss: 0.6410\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6295 - val_loss: 0.6230\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6169 - val_loss: 0.6076\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6052 - val_loss: 0.5970\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5944 - val_loss: 0.5815\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5842 - val_loss: 0.5691\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5745 - val_loss: 0.5604\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5653 - val_loss: 0.5511\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5566 - val_loss: 0.5414\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5483 - val_loss: 0.5322\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5404 - val_loss: 0.5239\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5328 - val_loss: 0.5141\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5256 - val_loss: 0.5079\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5188 - val_loss: 0.4995\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5122 - val_loss: 0.4921\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5060 - val_loss: 0.4854\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5001 - val_loss: 0.4786\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4945 - val_loss: 0.4732\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4892 - val_loss: 0.4677\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4842 - val_loss: 0.4626\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4795 - val_loss: 0.4584\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4750 - val_loss: 0.4537\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4708 - val_loss: 0.4491\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4668 - val_loss: 0.4452\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4630 - val_loss: 0.4421\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4594 - val_loss: 0.4403\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4560 - val_loss: 0.4373\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4528 - val_loss: 0.4346\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4498 - val_loss: 0.4326\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4470 - val_loss: 0.4305\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4443 - val_loss: 0.4288\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4418 - val_loss: 0.4274\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4394 - val_loss: 0.4269\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4372 - val_loss: 0.4254\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4350 - val_loss: 0.4250\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4330 - val_loss: 0.4224\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4311 - val_loss: 0.4203\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4293 - val_loss: 0.4217\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4275 - val_loss: 0.4197\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4259 - val_loss: 0.4192\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4243 - val_loss: 0.4206\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4228 - val_loss: 0.4201\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4214 - val_loss: 0.4209\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4201 - val_loss: 0.4181\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4188 - val_loss: 0.4183\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4175 - val_loss: 0.4173\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4163 - val_loss: 0.4161\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4152 - val_loss: 0.4164\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4140 - val_loss: 0.4197\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4130 - val_loss: 0.4194\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4119 - val_loss: 0.4164\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4109 - val_loss: 0.4191\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4100 - val_loss: 0.4178\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4090 - val_loss: 0.4199\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4080 - val_loss: 0.4172\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4071 - val_loss: 0.4179\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4063 - val_loss: 0.4144\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4054 - val_loss: 0.4174\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4045 - val_loss: 0.4161\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4037 - val_loss: 0.4162\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4029 - val_loss: 0.4152\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4021 - val_loss: 0.4148\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4014 - val_loss: 0.4152\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4006 - val_loss: 0.4117\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3999 - val_loss: 0.4137\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3992 - val_loss: 0.4141\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3985 - val_loss: 0.4175\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3978 - val_loss: 0.4222\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3972 - val_loss: 0.4215\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3966 - val_loss: 0.4159\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3959 - val_loss: 0.4147\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3953 - val_loss: 0.4145\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3947 - val_loss: 0.4111\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.3941 - val_loss: 0.4126\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.3935 - val_loss: 0.4195\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.3929 - val_loss: 0.4184\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3923 - val_loss: 0.4129\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.3917 - val_loss: 0.4223\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3913 - val_loss: 0.4172\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.3907 - val_loss: 0.4222\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3902 - val_loss: 0.4149\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3896 - val_loss: 0.4151\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3891 - val_loss: 0.4169\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4376\n",
      "[CV 2/5] END learning_rate=0.0004865926131115961, n_hidden=1, n_neurons=37;, score=-0.438 total time=  54.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 2.7856 - val_loss: 2.3949\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 1.1789 - val_loss: 2.3256\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.8073 - val_loss: 1.9793\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.7034 - val_loss: 1.5382\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6603 - val_loss: 1.1710\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6345 - val_loss: 0.9115\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6151 - val_loss: 0.7256\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5988 - val_loss: 0.6129\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5845 - val_loss: 0.5545\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5715 - val_loss: 0.5372\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5598 - val_loss: 0.5512\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5491 - val_loss: 0.5862\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5394 - val_loss: 0.6431\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5305 - val_loss: 0.7099\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5223 - val_loss: 0.7809\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5147 - val_loss: 0.8501\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5078 - val_loss: 0.9145\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5013 - val_loss: 0.9752\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4953 - val_loss: 1.0436\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4899 - val_loss: 1.0949\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.5436\n",
      "[CV 3/5] END learning_rate=0.0004865926131115961, n_hidden=1, n_neurons=37;, score=-0.544 total time=  12.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 4.1278 - val_loss: 1.9991\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3696 - val_loss: 1.3495\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.9109 - val_loss: 1.0265\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.8023 - val_loss: 0.8156\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7622 - val_loss: 0.7261\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7397 - val_loss: 0.6883\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7225 - val_loss: 0.6695\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7074 - val_loss: 0.6562\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6934 - val_loss: 0.6432\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6803 - val_loss: 0.6307\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6679 - val_loss: 0.6221\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6563 - val_loss: 0.6093\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6448 - val_loss: 0.6022\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6345 - val_loss: 0.5910\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6244 - val_loss: 0.5861\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6150 - val_loss: 0.5701\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6059 - val_loss: 0.5628\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5973 - val_loss: 0.5522\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5891 - val_loss: 0.5461\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5812 - val_loss: 0.5366\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5737 - val_loss: 0.5272\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5664 - val_loss: 0.5209\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5595 - val_loss: 0.5123\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5528 - val_loss: 0.5062\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5465 - val_loss: 0.5002\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5403 - val_loss: 0.4948\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5345 - val_loss: 0.4885\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5289 - val_loss: 0.4831\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5235 - val_loss: 0.4779\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5184 - val_loss: 0.4738\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5135 - val_loss: 0.4703\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5087 - val_loss: 0.4662\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5043 - val_loss: 0.4622\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5000 - val_loss: 0.4586\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4960 - val_loss: 0.4550\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4922 - val_loss: 0.4525\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4886 - val_loss: 0.4495\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4853 - val_loss: 0.4489\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4821 - val_loss: 0.4445\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4792 - val_loss: 0.4449\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4762 - val_loss: 0.4429\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4736 - val_loss: 0.4415\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4711 - val_loss: 0.4407\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4686 - val_loss: 0.4415\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4663 - val_loss: 0.4398\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4641 - val_loss: 0.4376\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4619 - val_loss: 0.4359\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4600 - val_loss: 0.4373\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4581 - val_loss: 0.4354\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4563 - val_loss: 0.4326\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4546 - val_loss: 0.4358\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4529 - val_loss: 0.4347\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4513 - val_loss: 0.4348\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4497 - val_loss: 0.4298\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4483 - val_loss: 0.4350\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4469 - val_loss: 0.4349\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4455 - val_loss: 0.4342\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4442 - val_loss: 0.4369\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4430 - val_loss: 0.4342\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4419 - val_loss: 0.4356\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4407 - val_loss: 0.4342\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4396 - val_loss: 0.4303\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4385 - val_loss: 0.4316\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4374 - val_loss: 0.4353\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3866\n",
      "[CV 4/5] END learning_rate=0.0004865926131115961, n_hidden=1, n_neurons=37;, score=-0.387 total time=  39.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 103us/sample - loss: 3.1424 - val_loss: 3.5134\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.4212 - val_loss: 2.1393\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.0036 - val_loss: 1.2890\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8485 - val_loss: 0.8754\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7818 - val_loss: 0.7529\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7438 - val_loss: 0.7179\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7177 - val_loss: 0.7174\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6970 - val_loss: 0.6944\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6791 - val_loss: 0.6866\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6630 - val_loss: 0.6664\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6482 - val_loss: 0.6608\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6347 - val_loss: 0.6356\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6219 - val_loss: 0.6337\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6103 - val_loss: 0.6091\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5992 - val_loss: 0.6036\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5888 - val_loss: 0.5996\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5790 - val_loss: 0.5877\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5696 - val_loss: 0.5729\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5607 - val_loss: 0.5589\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5522 - val_loss: 0.5529\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5443 - val_loss: 0.5425\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5368 - val_loss: 0.5299\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5297 - val_loss: 0.5222\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5229 - val_loss: 0.5169\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5165 - val_loss: 0.5064\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5106 - val_loss: 0.5021\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5048 - val_loss: 0.4926\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4994 - val_loss: 0.4875\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4942 - val_loss: 0.4797\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4894 - val_loss: 0.4749\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4847 - val_loss: 0.4670\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4805 - val_loss: 0.4632\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4762 - val_loss: 0.4606\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4724 - val_loss: 0.4549\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4687 - val_loss: 0.4500\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4653 - val_loss: 0.4468\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4621 - val_loss: 0.4431\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4591 - val_loss: 0.4399\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4560 - val_loss: 0.4368\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4535 - val_loss: 0.4341\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4508 - val_loss: 0.4320\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4484 - val_loss: 0.4298\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4462 - val_loss: 0.4272\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4439 - val_loss: 0.4253\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4419 - val_loss: 0.4235\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4399 - val_loss: 0.4233\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4382 - val_loss: 0.4209\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4364 - val_loss: 0.4188\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4347 - val_loss: 0.4177\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4331 - val_loss: 0.4166\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4316 - val_loss: 0.4185\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4301 - val_loss: 0.4139\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4289 - val_loss: 0.4169\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4275 - val_loss: 0.4167\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4265 - val_loss: 0.4175\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4253 - val_loss: 0.4120\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4242 - val_loss: 0.4118\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4230 - val_loss: 0.4109\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4221 - val_loss: 0.4107\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4211 - val_loss: 0.4132\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4201 - val_loss: 0.4086\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4192 - val_loss: 0.4095\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4183 - val_loss: 0.4076\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4174 - val_loss: 0.4111\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4166 - val_loss: 0.4094\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4158 - val_loss: 0.4097\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4150 - val_loss: 0.4104\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4142 - val_loss: 0.4091\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4135 - val_loss: 0.4113\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4127 - val_loss: 0.4055\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4119 - val_loss: 0.4101\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4113 - val_loss: 0.4074\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4105 - val_loss: 0.4111\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4098 - val_loss: 0.4082\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4092 - val_loss: 0.4081\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4085 - val_loss: 0.4070\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4078 - val_loss: 0.4051\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4071 - val_loss: 0.4025\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4066 - val_loss: 0.4070\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4059 - val_loss: 0.4097\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4053 - val_loss: 0.4126\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4047 - val_loss: 0.4053\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4041 - val_loss: 0.4064\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4035 - val_loss: 0.4026\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4029 - val_loss: 0.4063\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4024 - val_loss: 0.4000\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4018 - val_loss: 0.3987\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4012 - val_loss: 0.3996\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4006 - val_loss: 0.3979\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4001 - val_loss: 0.3947\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3995 - val_loss: 0.3972\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3990 - val_loss: 0.4004\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3985 - val_loss: 0.3951\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3979 - val_loss: 0.3950\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3974 - val_loss: 0.3989\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3968 - val_loss: 0.3973\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3964 - val_loss: 0.4011\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3959 - val_loss: 0.3995\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3954 - val_loss: 0.4018\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3950 - val_loss: 0.3945\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END learning_rate=0.0004865926131115961, n_hidden=1, n_neurons=37;, score=-0.439 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 1.0592 - val_loss: 14.3954\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8144 - val_loss: 6.7159\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5902 - val_loss: 0.8603\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5045 - val_loss: 0.4395\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4664 - val_loss: 0.4351\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4502 - val_loss: 0.4615\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4405 - val_loss: 0.5003\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4360 - val_loss: 0.5201\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4316 - val_loss: 0.5282\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4297 - val_loss: 0.4961\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4262 - val_loss: 0.4879\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4251 - val_loss: 0.5049\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4226 - val_loss: 0.4836\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4209 - val_loss: 0.4805\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4186 - val_loss: 0.4835\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3998\n",
      "[CV 1/5] END learning_rate=0.008867452498635279, n_hidden=1, n_neurons=10;, score=-0.400 total time=   9.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 1.0293 - val_loss: 6.9248\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5324 - val_loss: 0.6891\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4501 - val_loss: 0.4082\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4231 - val_loss: 0.4060\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4122 - val_loss: 0.3899\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4063 - val_loss: 0.3837\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4012 - val_loss: 0.3815\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3979 - val_loss: 0.3832\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3936 - val_loss: 0.3840\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3933 - val_loss: 0.3850\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3915 - val_loss: 0.3835\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3890 - val_loss: 0.3823\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3879 - val_loss: 0.3815\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3861 - val_loss: 0.3863\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3852 - val_loss: 0.3905\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3844 - val_loss: 0.3866\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3835 - val_loss: 0.3800\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3816 - val_loss: 0.3783\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3800 - val_loss: 0.3767\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3798 - val_loss: 0.3747\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3790 - val_loss: 0.3691\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3773 - val_loss: 0.3655\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3755 - val_loss: 0.3742\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3756 - val_loss: 0.3581\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3742 - val_loss: 0.3606\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3737 - val_loss: 0.3586\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3724 - val_loss: 0.3563\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3711 - val_loss: 0.3618\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3715 - val_loss: 0.3555\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3703 - val_loss: 0.3592\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3688 - val_loss: 0.3540\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3687 - val_loss: 0.3542\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3675 - val_loss: 0.3557\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3663 - val_loss: 0.3527\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3654 - val_loss: 0.3531\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3652 - val_loss: 0.3525\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3648 - val_loss: 0.3518\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3646 - val_loss: 0.3507\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3638 - val_loss: 0.3509\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3632 - val_loss: 0.3504\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3621 - val_loss: 0.3491\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3623 - val_loss: 0.3466\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3611 - val_loss: 0.3474\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3611 - val_loss: 0.3464\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3600 - val_loss: 0.3453\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3597 - val_loss: 0.3496\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3584 - val_loss: 0.3471\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3582 - val_loss: 0.3434\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3580 - val_loss: 0.3533\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3568 - val_loss: 0.3486\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3572 - val_loss: 0.3463\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3576 - val_loss: 0.3530\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3563 - val_loss: 0.3428\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3558 - val_loss: 0.3459\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3561 - val_loss: 0.3419\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3550 - val_loss: 0.3464\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3548 - val_loss: 0.3420\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3544 - val_loss: 0.3399\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3543 - val_loss: 0.3426\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3545 - val_loss: 0.3438\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3532 - val_loss: 0.3441\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3534 - val_loss: 0.3409\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3533 - val_loss: 0.3391\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3526 - val_loss: 0.3436\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3529 - val_loss: 0.3383\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3526 - val_loss: 0.3408\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3530 - val_loss: 0.3405\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3516 - val_loss: 0.3400\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3526 - val_loss: 0.3376\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3513 - val_loss: 0.3391\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3501 - val_loss: 0.3381\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3508 - val_loss: 0.3418\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3508 - val_loss: 0.3369\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3508 - val_loss: 0.3381\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3506 - val_loss: 0.3363\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3500 - val_loss: 0.3437\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3498 - val_loss: 0.3390\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3497 - val_loss: 0.3359\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3490 - val_loss: 0.3354\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3492 - val_loss: 0.3352\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3483 - val_loss: 0.3354\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3482 - val_loss: 0.3349\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3480 - val_loss: 0.3337\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3474 - val_loss: 0.3409\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3474 - val_loss: 0.3414\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3469 - val_loss: 0.3343\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3463 - val_loss: 0.3316\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3466 - val_loss: 0.3308\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3456 - val_loss: 0.3305\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3456 - val_loss: 0.3302\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3455 - val_loss: 0.3326\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3441 - val_loss: 0.3330\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3438 - val_loss: 0.3411\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3438 - val_loss: 0.3307\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3432 - val_loss: 0.3299\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3433 - val_loss: 0.3283\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3429 - val_loss: 0.3288\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3424 - val_loss: 0.3279\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3416 - val_loss: 0.3283\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3431 - val_loss: 0.3291\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.4051\n",
      "[CV 2/5] END learning_rate=0.008867452498635279, n_hidden=1, n_neurons=10;, score=-0.405 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.8740 - val_loss: 0.6963\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5436 - val_loss: 0.8850\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4708 - val_loss: 0.6459\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4414 - val_loss: 0.4694\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4276 - val_loss: 0.3975\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4186 - val_loss: 0.3855\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4104 - val_loss: 0.3790\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4074 - val_loss: 0.3758\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4021 - val_loss: 0.3703\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4019 - val_loss: 0.3758\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3954 - val_loss: 0.3656\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3937 - val_loss: 0.3615\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3893 - val_loss: 0.3581\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3881 - val_loss: 0.3623\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3849 - val_loss: 0.3613\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3829 - val_loss: 0.3556\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3819 - val_loss: 0.3533\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3779 - val_loss: 0.3508\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3758 - val_loss: 0.3522\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3767 - val_loss: 0.3492\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3733 - val_loss: 0.3477\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3699 - val_loss: 0.3462\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3681 - val_loss: 0.3497\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3669 - val_loss: 0.4032\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3657 - val_loss: 0.4793\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3636 - val_loss: 0.5007\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3650 - val_loss: 0.6350\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3641 - val_loss: 0.7512\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3683 - val_loss: 0.7940\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3595 - val_loss: 0.7906\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3592 - val_loss: 0.7887\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3606 - val_loss: 0.8528\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3951\n",
      "[CV 3/5] END learning_rate=0.008867452498635279, n_hidden=1, n_neurons=10;, score=-0.395 total time=  20.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.0451 - val_loss: 12.9165\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5743 - val_loss: 10.8095\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5101 - val_loss: 0.5364\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4494 - val_loss: 0.4082\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4377 - val_loss: 0.4053\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4315 - val_loss: 0.4190\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4270 - val_loss: 0.4271\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4237 - val_loss: 0.4268\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4215 - val_loss: 0.4407\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4205 - val_loss: 0.4388\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4171 - val_loss: 0.4389\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4151 - val_loss: 0.4840\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4149 - val_loss: 0.4607\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4118 - val_loss: 0.4541\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4100 - val_loss: 0.4497\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3622\n",
      "[CV 4/5] END learning_rate=0.008867452498635279, n_hidden=1, n_neurons=10;, score=-0.362 total time=   9.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 1.1556 - val_loss: 0.5598\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5314 - val_loss: 0.4553\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4650 - val_loss: 0.4236\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4395 - val_loss: 0.4017\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4246 - val_loss: 0.3916\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4139 - val_loss: 0.3848\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4074 - val_loss: 0.3795\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4027 - val_loss: 0.3779\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3978 - val_loss: 0.3794\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3940 - val_loss: 0.3712\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3923 - val_loss: 0.3695\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3890 - val_loss: 0.3680\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3856 - val_loss: 0.3692\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3846 - val_loss: 0.3719\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3843 - val_loss: 0.3666\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3881 - val_loss: 0.3634\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3834 - val_loss: 0.3607\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3847 - val_loss: 0.3599\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3780 - val_loss: 0.3583\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3840 - val_loss: 0.3625\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3877 - val_loss: 0.3587\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3767 - val_loss: 0.3671\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3855 - val_loss: 0.3586\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3729 - val_loss: 0.3571\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3726 - val_loss: 0.3601\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3739 - val_loss: 0.3596\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3717 - val_loss: 0.3555\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3739 - val_loss: 0.3562\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3691 - val_loss: 0.3540\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3702 - val_loss: 0.3688\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3778 - val_loss: 0.3532\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3692 - val_loss: 0.3539\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3715 - val_loss: 0.3626\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3714 - val_loss: 0.3527\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3664 - val_loss: 0.3495\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3676 - val_loss: 0.3509\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3660 - val_loss: 0.3500\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3696 - val_loss: 0.3506\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3744 - val_loss: 0.3508\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3746 - val_loss: 0.3959\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3682 - val_loss: 0.3484\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3686 - val_loss: 0.3508\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3623 - val_loss: 0.3503\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3638 - val_loss: 0.3494\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3609 - val_loss: 0.3544\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3607 - val_loss: 0.3463\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3856 - val_loss: 0.3659\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3664 - val_loss: 0.3459\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3593 - val_loss: 0.3449\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3600 - val_loss: 0.3459\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3587 - val_loss: 0.3471\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3583 - val_loss: 0.3434\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3569 - val_loss: 0.3440\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3600 - val_loss: 0.3440\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3577 - val_loss: 0.3419\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3565 - val_loss: 0.3425\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3592 - val_loss: 0.3403\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3584 - val_loss: 0.3392\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3578 - val_loss: 0.3396\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3549 - val_loss: 0.3573\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3634 - val_loss: 0.3449\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3547 - val_loss: 0.3387\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3530 - val_loss: 0.3385\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3607 - val_loss: 0.3713\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3545 - val_loss: 0.3386\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3551 - val_loss: 0.3391\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3541 - val_loss: 0.3376\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3657 - val_loss: 0.3680\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4761 - val_loss: 0.3472\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3691 - val_loss: 0.3445\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3574 - val_loss: 0.3441\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3587 - val_loss: 0.3523\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3587 - val_loss: 0.3398\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3554 - val_loss: 0.3412\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3562 - val_loss: 0.3360\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3529 - val_loss: 0.3378\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3548 - val_loss: 0.3357\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3537 - val_loss: 0.3352\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3499 - val_loss: 0.3427\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3507 - val_loss: 0.3393\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3521 - val_loss: 0.3409\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3517 - val_loss: 0.3349\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3491 - val_loss: 0.3330\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3487 - val_loss: 0.3354\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3467 - val_loss: 0.3331\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3522 - val_loss: 0.3345\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3529 - val_loss: 0.3347\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3477 - val_loss: 0.3333\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3463 - val_loss: 0.3322\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3538 - val_loss: 0.3362\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3470 - val_loss: 0.3335\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3446 - val_loss: 0.3337\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3517 - val_loss: 0.3408\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3469 - val_loss: 0.3352\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3462 - val_loss: 0.3366\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3516 - val_loss: 0.3335\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3475 - val_loss: 0.3313\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3433 - val_loss: 0.3329\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3431 - val_loss: 0.3303\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3426 - val_loss: 0.3310\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3756\n",
      "[CV 5/5] END learning_rate=0.008867452498635279, n_hidden=1, n_neurons=10;, score=-0.376 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.5995 - val_loss: 4.0915\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4793 - val_loss: 2.9111\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4824 - val_loss: 56.5470\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4038 - val_loss: 0.4202\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3741 - val_loss: 10.6461\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3715 - val_loss: 9.1485\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4304 - val_loss: 3.6344\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3714 - val_loss: 0.4034\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3450 - val_loss: 0.3528\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3362 - val_loss: 0.3679\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3305 - val_loss: 0.3320\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3267 - val_loss: 0.3435\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3211 - val_loss: 0.3322\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3194 - val_loss: 0.3034\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3147 - val_loss: 0.3264\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3139 - val_loss: 0.3419\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3115 - val_loss: 0.2921\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3088 - val_loss: 0.2955\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3077 - val_loss: 0.3665\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3080 - val_loss: 0.3523\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3022 - val_loss: 0.2966\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3009 - val_loss: 0.3051\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3007 - val_loss: 0.2833\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2976 - val_loss: 0.3224\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2969 - val_loss: 0.3084\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2956 - val_loss: 0.2965\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2936 - val_loss: 0.3065\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2920 - val_loss: 0.3316\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2914 - val_loss: 0.2806\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2904 - val_loss: 0.3534\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2914 - val_loss: 0.3250\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2885 - val_loss: 0.4319\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2891 - val_loss: 0.3366\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2884 - val_loss: 0.3166\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2852 - val_loss: 0.3029\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2838 - val_loss: 0.2772\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2829 - val_loss: 0.2811\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2824 - val_loss: 0.3420\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2809 - val_loss: 0.3104\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2807 - val_loss: 0.2757\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2796 - val_loss: 0.2911\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2789 - val_loss: 0.3538\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2840 - val_loss: 0.3659\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2794 - val_loss: 0.3067\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2790 - val_loss: 0.2791\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2766 - val_loss: 0.3126\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2746 - val_loss: 0.3954\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2756 - val_loss: 0.3195\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2739 - val_loss: 0.2762\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2728 - val_loss: 0.3377\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.3071\n",
      "[CV 1/5] END learning_rate=0.02405007663294175, n_hidden=2, n_neurons=39;, score=-0.307 total time=  32.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 0.6062 - val_loss: 2.8552\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5051 - val_loss: 10.9631\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5570 - val_loss: 195.7381\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 3.3213 - val_loss: 1.5620\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4281 - val_loss: 0.3832\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3762 - val_loss: 0.3847\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3702 - val_loss: 0.3532\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3544 - val_loss: 0.3820\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3504 - val_loss: 0.3476\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3460 - val_loss: 0.3328\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3390 - val_loss: 0.3551\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3369 - val_loss: 0.3584\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3351 - val_loss: 0.4114\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3305 - val_loss: 0.4191\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3278 - val_loss: 0.3463\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3253 - val_loss: 0.3395\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3232 - val_loss: 0.3271\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3193 - val_loss: 0.3662\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3155 - val_loss: 0.3161\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3089 - val_loss: 0.3250\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3109 - val_loss: 0.3133\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3076 - val_loss: 0.3214\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3046 - val_loss: 0.3565\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3027 - val_loss: 0.3223\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3010 - val_loss: 0.3213\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2965 - val_loss: 0.3454\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2956 - val_loss: 0.3239\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2924 - val_loss: 0.2924\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2930 - val_loss: 0.2949\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2910 - val_loss: 0.3102\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2909 - val_loss: 0.2944\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3009 - val_loss: 0.2977\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2883 - val_loss: 0.2959\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2864 - val_loss: 0.3127\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2850 - val_loss: 0.3707\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2852 - val_loss: 0.2780\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2819 - val_loss: 0.3141\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2802 - val_loss: 0.3002\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2774 - val_loss: 0.3266\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2795 - val_loss: 0.2848\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2789 - val_loss: 0.3917\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2769 - val_loss: 0.3314\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2776 - val_loss: 0.3024\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2754 - val_loss: 0.2844\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2727 - val_loss: 0.2825\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2737 - val_loss: 0.2799\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3413\n",
      "[CV 2/5] END learning_rate=0.02405007663294175, n_hidden=2, n_neurons=39;, score=-0.341 total time=  30.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 0.6356 - val_loss: 1.7548\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3997 - val_loss: 1.4458\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3754 - val_loss: 3.0698\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3600 - val_loss: 0.9564\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3499 - val_loss: 2.9539\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3401 - val_loss: 2.3539\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3341 - val_loss: 0.5209\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3342 - val_loss: 0.3442\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3240 - val_loss: 0.5574\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3220 - val_loss: 0.4372\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3151 - val_loss: 2.0332\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3160 - val_loss: 1.4789\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3101 - val_loss: 2.0371\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3077 - val_loss: 2.3249\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3022 - val_loss: 1.3436\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3010 - val_loss: 0.4594\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2984 - val_loss: 0.3426\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2981 - val_loss: 0.2904\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2958 - val_loss: 1.1858\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2965 - val_loss: 0.3360\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2934 - val_loss: 0.3533\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2912 - val_loss: 0.6994\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2908 - val_loss: 0.8708\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2886 - val_loss: 0.5019\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2891 - val_loss: 1.1040\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2863 - val_loss: 1.2685\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2855 - val_loss: 2.1002\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2849 - val_loss: 1.0740\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3457\n",
      "[CV 3/5] END learning_rate=0.02405007663294175, n_hidden=2, n_neurons=39;, score=-0.346 total time=  18.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 0.7267 - val_loss: 5.8928\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4424 - val_loss: 3.0190\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4777 - val_loss: 0.3578\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3760 - val_loss: 0.4811\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3716 - val_loss: 0.5231\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3645 - val_loss: 0.5476\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3513 - val_loss: 0.3478\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3432 - val_loss: 0.3204\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3387 - val_loss: 0.3174\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3331 - val_loss: 0.3504\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3290 - val_loss: 0.3161\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3285 - val_loss: 0.3249\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3233 - val_loss: 0.3363\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3211 - val_loss: 0.3147\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3368 - val_loss: 0.4212\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3269 - val_loss: 0.2960\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3180 - val_loss: 0.4037\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3181 - val_loss: 0.5619\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3175 - val_loss: 0.5441\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3151 - val_loss: 0.3351\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3128 - val_loss: 0.3530\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3093 - val_loss: 0.2876\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3070 - val_loss: 0.3516\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3075 - val_loss: 0.2905\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3028 - val_loss: 0.3690\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3041 - val_loss: 0.2857\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3024 - val_loss: 0.3526\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3008 - val_loss: 0.3243\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2999 - val_loss: 0.9534\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2989 - val_loss: 0.6419\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3008 - val_loss: 0.5225\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2978 - val_loss: 0.2930\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2971 - val_loss: 0.3034\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2948 - val_loss: 0.2978\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2994 - val_loss: 0.2935\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2981 - val_loss: 0.3774\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.2812\n",
      "[CV 4/5] END learning_rate=0.02405007663294175, n_hidden=2, n_neurons=39;, score=-0.281 total time=  23.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 0.6887 - val_loss: 2.4378\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4507 - val_loss: 0.5966\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4100 - val_loss: 12.0714\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4553 - val_loss: 0.3485\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3579 - val_loss: 0.3304\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3481 - val_loss: 0.3262\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3411 - val_loss: 0.3184\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3385 - val_loss: 0.3473\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3306 - val_loss: 0.3322\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3253 - val_loss: 0.3356\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3205 - val_loss: 0.3356\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3167 - val_loss: 0.3182\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3146 - val_loss: 0.4375\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3134 - val_loss: 0.3068\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3099 - val_loss: 0.3446\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3087 - val_loss: 0.2909\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3040 - val_loss: 0.3168\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3023 - val_loss: 0.3257\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3021 - val_loss: 0.3658\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2985 - val_loss: 0.2993\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2981 - val_loss: 0.3285\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2962 - val_loss: 0.3631\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2949 - val_loss: 0.2980\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2951 - val_loss: 0.3036\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2950 - val_loss: 0.4755\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2925 - val_loss: 0.2947\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3268\n",
      "[CV 5/5] END learning_rate=0.02405007663294175, n_hidden=2, n_neurons=39;, score=-0.327 total time=  16.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 97us/sample - loss: 2.4360 - val_loss: 3.6136\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.9328 - val_loss: 1.6369\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7308 - val_loss: 0.8627\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6650 - val_loss: 0.6646\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6285 - val_loss: 0.5960\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6013 - val_loss: 0.5596\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5778 - val_loss: 0.5365\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5569 - val_loss: 0.5151\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5382 - val_loss: 0.4982\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5220 - val_loss: 0.4837\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5068 - val_loss: 0.4692\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4935 - val_loss: 0.4601\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4815 - val_loss: 0.4553\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4708 - val_loss: 0.4488\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4613 - val_loss: 0.4368\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4530 - val_loss: 0.4318\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4453 - val_loss: 0.4253\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4386 - val_loss: 0.4218\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4330 - val_loss: 0.4174\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4275 - val_loss: 0.4181\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4229 - val_loss: 0.4120\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4186 - val_loss: 0.4006\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4147 - val_loss: 0.4047\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4111 - val_loss: 0.4031\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4078 - val_loss: 0.4121\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4044 - val_loss: 0.3920\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4017 - val_loss: 0.4008\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3993 - val_loss: 0.4130\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3969 - val_loss: 0.4141\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3944 - val_loss: 0.4129\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3925 - val_loss: 0.4099\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3906 - val_loss: 0.4131\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3885 - val_loss: 0.3989\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3868 - val_loss: 0.4024\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3852 - val_loss: 0.4072\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3836 - val_loss: 0.4146\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3606\n",
      "[CV 1/5] END learning_rate=0.0005605872335636286, n_hidden=3, n_neurons=93;, score=-0.361 total time=  23.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 115us/sample - loss: 2.8495 - val_loss: 7.3053\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.1197 - val_loss: 3.8290\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.8489 - val_loss: 1.7033\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7306 - val_loss: 1.0427\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6720 - val_loss: 0.7342\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6350 - val_loss: 0.6240\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6066 - val_loss: 0.5764\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5828 - val_loss: 0.5543\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5612 - val_loss: 0.5387\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5417 - val_loss: 0.5230\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5236 - val_loss: 0.5071\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5069 - val_loss: 0.4914\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4919 - val_loss: 0.4775\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4780 - val_loss: 0.4618\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4660 - val_loss: 0.4483\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4549 - val_loss: 0.4381\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4447 - val_loss: 0.4279\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4359 - val_loss: 0.4193\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4278 - val_loss: 0.4115\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4206 - val_loss: 0.4067\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4144 - val_loss: 0.4006\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4085 - val_loss: 0.3973\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4033 - val_loss: 0.3926\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3988 - val_loss: 0.3877\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3945 - val_loss: 0.3873\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3909 - val_loss: 0.3843\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3874 - val_loss: 0.3810\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3844 - val_loss: 0.3826\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3815 - val_loss: 0.3823\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3789 - val_loss: 0.3782\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3765 - val_loss: 0.3787\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3742 - val_loss: 0.3768\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3721 - val_loss: 0.3805\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3701 - val_loss: 0.3747\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3683 - val_loss: 0.3763\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3665 - val_loss: 0.3783\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3649 - val_loss: 0.3737\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3633 - val_loss: 0.3756\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3619 - val_loss: 0.3745\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3606 - val_loss: 0.3727\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3590 - val_loss: 0.3762\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3580 - val_loss: 0.3745\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3567 - val_loss: 0.3710\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3556 - val_loss: 0.3697\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3545 - val_loss: 0.3726\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3536 - val_loss: 0.3707\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3523 - val_loss: 0.3706\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3513 - val_loss: 0.3736\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3504 - val_loss: 0.3647\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3497 - val_loss: 0.3644\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3488 - val_loss: 0.3695\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3479 - val_loss: 0.3709\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3469 - val_loss: 0.3668\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3461 - val_loss: 0.3741\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3452 - val_loss: 0.3670\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3447 - val_loss: 0.3656\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3437 - val_loss: 0.3637\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3431 - val_loss: 0.3637\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3423 - val_loss: 0.3692\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3416 - val_loss: 0.3617\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3408 - val_loss: 0.3624\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3404 - val_loss: 0.3589\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3395 - val_loss: 0.3557\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3390 - val_loss: 0.3556\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3383 - val_loss: 0.3619\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3380 - val_loss: 0.3576\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3372 - val_loss: 0.3599\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3365 - val_loss: 0.3601\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3361 - val_loss: 0.3565\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3355 - val_loss: 0.3594\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3351 - val_loss: 0.3582\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3345 - val_loss: 0.3558\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3340 - val_loss: 0.3579\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3336 - val_loss: 0.3536\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3331 - val_loss: 0.3553\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3325 - val_loss: 0.3540\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3321 - val_loss: 0.3544\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3317 - val_loss: 0.3518\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3310 - val_loss: 0.3598\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3306 - val_loss: 0.3559\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3302 - val_loss: 0.3512\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3299 - val_loss: 0.3541\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3294 - val_loss: 0.3502\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3288 - val_loss: 0.3540\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3285 - val_loss: 0.3482\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3281 - val_loss: 0.3539\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3277 - val_loss: 0.3497\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3273 - val_loss: 0.3491\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3269 - val_loss: 0.3536\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3266 - val_loss: 0.3488\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3260 - val_loss: 0.3474\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3257 - val_loss: 0.3476\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3252 - val_loss: 0.3484\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3251 - val_loss: 0.3450\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3246 - val_loss: 0.3470\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3242 - val_loss: 0.3445\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3240 - val_loss: 0.3477\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3237 - val_loss: 0.3446\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3232 - val_loss: 0.3453\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3229 - val_loss: 0.3475\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3830\n",
      "[CV 2/5] END learning_rate=0.0005605872335636286, n_hidden=3, n_neurons=93;, score=-0.383 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 94us/sample - loss: 2.0218 - val_loss: 13.2482\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.8900 - val_loss: 9.8320\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7665 - val_loss: 7.3136\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7118 - val_loss: 5.3675\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6723 - val_loss: 4.0980\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.6395 - val_loss: 3.0967\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6110 - val_loss: 2.4447\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5856 - val_loss: 1.9503\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5632 - val_loss: 1.5681\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5431 - val_loss: 1.2476\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5250 - val_loss: 1.0877\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5088 - val_loss: 0.9607\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4944 - val_loss: 0.8395\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4817 - val_loss: 0.7426\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4702 - val_loss: 0.6775\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4600 - val_loss: 0.6287\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4509 - val_loss: 0.6028\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4428 - val_loss: 0.5840\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4354 - val_loss: 0.5617\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4289 - val_loss: 0.5587\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4228 - val_loss: 0.5509\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4173 - val_loss: 0.5694\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4124 - val_loss: 0.5838\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4075 - val_loss: 0.5947\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4036 - val_loss: 0.6362\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3998 - val_loss: 0.6710\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3959 - val_loss: 0.6908\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3928 - val_loss: 0.7022\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3897 - val_loss: 0.7651\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3869 - val_loss: 0.8102\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3842 - val_loss: 0.8303\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.4182\n",
      "[CV 3/5] END learning_rate=0.0005605872335636286, n_hidden=3, n_neurons=93;, score=-0.418 total time=  20.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 2.7663 - val_loss: 10.3041\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.9962 - val_loss: 0.9796\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7399 - val_loss: 0.7226\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6715 - val_loss: 0.6095\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6321 - val_loss: 0.5727\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6022 - val_loss: 0.5544\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5774 - val_loss: 0.5435\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5562 - val_loss: 0.5399\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5380 - val_loss: 0.5030\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5212 - val_loss: 0.4965\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.5067 - val_loss: 0.4773\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4937 - val_loss: 0.4576\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4823 - val_loss: 0.4385\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4719 - val_loss: 0.4300\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4626 - val_loss: 0.4263\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4546 - val_loss: 0.4231\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4476 - val_loss: 0.4091\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4409 - val_loss: 0.4013\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4353 - val_loss: 0.3969\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4298 - val_loss: 0.3915\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4251 - val_loss: 0.3873\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4209 - val_loss: 0.3842\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4168 - val_loss: 0.3801\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4131 - val_loss: 0.3770\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4097 - val_loss: 0.3769\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4063 - val_loss: 0.3806\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4034 - val_loss: 0.3711\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4001 - val_loss: 0.3802\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3977 - val_loss: 0.3681\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3951 - val_loss: 0.3718\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3926 - val_loss: 0.3754\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3905 - val_loss: 0.3733\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3883 - val_loss: 0.3710\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3860 - val_loss: 0.3742\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3841 - val_loss: 0.3766\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3824 - val_loss: 0.3662\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3807 - val_loss: 0.3747\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3790 - val_loss: 0.3693\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3774 - val_loss: 0.3720\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3758 - val_loss: 0.3818\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3743 - val_loss: 0.3708\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3732 - val_loss: 0.3650\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3717 - val_loss: 0.3631\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3703 - val_loss: 0.3704\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3691 - val_loss: 0.3692\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3681 - val_loss: 0.3632\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3670 - val_loss: 0.3592\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3656 - val_loss: 0.3527\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3647 - val_loss: 0.3612\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3634 - val_loss: 0.3666\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3626 - val_loss: 0.3482\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3614 - val_loss: 0.3599\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3608 - val_loss: 0.3516\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3595 - val_loss: 0.3421\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3588 - val_loss: 0.3431\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3577 - val_loss: 0.3759\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3572 - val_loss: 0.3614\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3562 - val_loss: 0.3440\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3557 - val_loss: 0.3570\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3548 - val_loss: 0.3471\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3540 - val_loss: 0.3675\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3535 - val_loss: 0.3456\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3526 - val_loss: 0.3458\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3519 - val_loss: 0.3613\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3055\n",
      "[CV 4/5] END learning_rate=0.0005605872335636286, n_hidden=3, n_neurons=93;, score=-0.305 total time=  42.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 2.6133 - val_loss: 9.8477\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.9262 - val_loss: 0.9914\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6713 - val_loss: 0.6311\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6161 - val_loss: 0.5867\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5866 - val_loss: 0.5780\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5636 - val_loss: 0.5451\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5426 - val_loss: 0.5392\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5244 - val_loss: 0.5070\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5075 - val_loss: 0.4815\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4923 - val_loss: 0.4689\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4784 - val_loss: 0.4520\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4659 - val_loss: 0.4407\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4545 - val_loss: 0.4306\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4443 - val_loss: 0.4206\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4351 - val_loss: 0.4134\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4271 - val_loss: 0.4104\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4200 - val_loss: 0.4038\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4135 - val_loss: 0.3927\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4083 - val_loss: 0.3943\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4031 - val_loss: 0.3861\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3985 - val_loss: 0.3796\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3945 - val_loss: 0.3844\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3909 - val_loss: 0.3844\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3874 - val_loss: 0.3783\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3844 - val_loss: 0.3714\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3813 - val_loss: 0.3657\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3789 - val_loss: 0.3676\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3766 - val_loss: 0.3892\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3743 - val_loss: 0.3719\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3721 - val_loss: 0.3610\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3702 - val_loss: 0.3777\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3684 - val_loss: 0.3773\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3667 - val_loss: 0.3764\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3649 - val_loss: 0.3716\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3636 - val_loss: 0.3574\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3619 - val_loss: 0.3536\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3603 - val_loss: 0.3718\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3590 - val_loss: 0.3600\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3580 - val_loss: 0.3824\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3568 - val_loss: 0.3616\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3556 - val_loss: 0.3527\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3544 - val_loss: 0.3720\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3532 - val_loss: 0.3708\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3523 - val_loss: 0.3650\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3510 - val_loss: 0.3536\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3500 - val_loss: 0.3560\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3488 - val_loss: 0.3790\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3480 - val_loss: 0.3949\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3474 - val_loss: 0.3743\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3463 - val_loss: 0.3466\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3455 - val_loss: 0.3507\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3445 - val_loss: 0.3450\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3438 - val_loss: 0.3462\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3431 - val_loss: 0.3392\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3423 - val_loss: 0.3459\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3413 - val_loss: 0.3385\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3407 - val_loss: 0.3702\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3401 - val_loss: 0.3350\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3393 - val_loss: 0.3500\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3390 - val_loss: 0.3341\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3378 - val_loss: 0.3506\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3375 - val_loss: 0.3359\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3368 - val_loss: 0.3624\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3360 - val_loss: 0.3365\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3355 - val_loss: 0.3394\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3350 - val_loss: 0.3478\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3340 - val_loss: 0.3757\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3339 - val_loss: 0.3403\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3331 - val_loss: 0.3391\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3325 - val_loss: 0.3369\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3655\n",
      "[CV 5/5] END learning_rate=0.0005605872335636286, n_hidden=3, n_neurons=93;, score=-0.365 total time=  46.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 3.9720 - val_loss: 6.5811\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 2.1274 - val_loss: 5.5669\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 1.5810 - val_loss: 4.1280\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.2785 - val_loss: 2.7642\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.0765 - val_loss: 2.1322\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9459 - val_loss: 1.9365\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8641 - val_loss: 1.7565\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8114 - val_loss: 1.5721\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7760 - val_loss: 1.4064\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7507 - val_loss: 1.2529\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7314 - val_loss: 1.1194\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7159 - val_loss: 1.0143\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7030 - val_loss: 0.9246\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6916 - val_loss: 0.8499\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6812 - val_loss: 0.7915\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6717 - val_loss: 0.7404\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6628 - val_loss: 0.7017\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6544 - val_loss: 0.6686\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6464 - val_loss: 0.6418\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6389 - val_loss: 0.6196\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6317 - val_loss: 0.6015\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6250 - val_loss: 0.5866\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6185 - val_loss: 0.5746\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6123 - val_loss: 0.5658\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6061 - val_loss: 0.5566\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6006 - val_loss: 0.5505\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5950 - val_loss: 0.5441\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5897 - val_loss: 0.5385\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5846 - val_loss: 0.5341\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5796 - val_loss: 0.5297\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5747 - val_loss: 0.5255\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5701 - val_loss: 0.5218\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5657 - val_loss: 0.5181\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5613 - val_loss: 0.5149\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5571 - val_loss: 0.5116\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5530 - val_loss: 0.5081\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5490 - val_loss: 0.5050\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5451 - val_loss: 0.5015\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5415 - val_loss: 0.4982\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5379 - val_loss: 0.4950\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5344 - val_loss: 0.4919\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5310 - val_loss: 0.4885\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5279 - val_loss: 0.4855\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5247 - val_loss: 0.4826\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5216 - val_loss: 0.4793\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5188 - val_loss: 0.4762\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5159 - val_loss: 0.4733\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5132 - val_loss: 0.4704\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5105 - val_loss: 0.4681\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5080 - val_loss: 0.4650\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5055 - val_loss: 0.4624\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5032 - val_loss: 0.4601\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5010 - val_loss: 0.4579\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4986 - val_loss: 0.4555\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4966 - val_loss: 0.4534\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4945 - val_loss: 0.4514\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4925 - val_loss: 0.4497\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4906 - val_loss: 0.4485\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4888 - val_loss: 0.4466\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4870 - val_loss: 0.4450\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4853 - val_loss: 0.4436\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4837 - val_loss: 0.4423\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4819 - val_loss: 0.4412\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4804 - val_loss: 0.4398\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4788 - val_loss: 0.4393\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4772 - val_loss: 0.4376\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4759 - val_loss: 0.4370\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4744 - val_loss: 0.4361\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4729 - val_loss: 0.4352\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4716 - val_loss: 0.4345\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4703 - val_loss: 0.4340\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4691 - val_loss: 0.4336\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4679 - val_loss: 0.4332\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4664 - val_loss: 0.4322\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4654 - val_loss: 0.4323\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4641 - val_loss: 0.4315\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4628 - val_loss: 0.4310\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4618 - val_loss: 0.4304\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4606 - val_loss: 0.4298\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4595 - val_loss: 0.4294\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4583 - val_loss: 0.4289\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4572 - val_loss: 0.4288\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4561 - val_loss: 0.4280\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4551 - val_loss: 0.4277\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4541 - val_loss: 0.4274\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4531 - val_loss: 0.4267\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4518 - val_loss: 0.4251\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4511 - val_loss: 0.4240\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4500 - val_loss: 0.4228\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4491 - val_loss: 0.4218\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4481 - val_loss: 0.4202\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4470 - val_loss: 0.4190\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4461 - val_loss: 0.4175\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4452 - val_loss: 0.4162\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4443 - val_loss: 0.4147\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4432 - val_loss: 0.4141\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4423 - val_loss: 0.4123\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4414 - val_loss: 0.4109\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4405 - val_loss: 0.4098\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4396 - val_loss: 0.4086\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4179\n",
      "[CV 1/5] END learning_rate=0.0004969432001318289, n_hidden=1, n_neurons=11;, score=-0.418 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 5.1330 - val_loss: 18.7378\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 2.9382 - val_loss: 12.4107\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.9819 - val_loss: 7.9088\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.4013 - val_loss: 4.5968\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.0109 - val_loss: 2.6311\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7792 - val_loss: 1.4936\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6586 - val_loss: 0.9345\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6002 - val_loss: 0.6817\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5701 - val_loss: 0.5787\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5518 - val_loss: 0.5360\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5389 - val_loss: 0.5135\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5286 - val_loss: 0.5033\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5195 - val_loss: 0.4970\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5115 - val_loss: 0.4929\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5043 - val_loss: 0.4887\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4975 - val_loss: 0.4870\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4912 - val_loss: 0.4850\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4855 - val_loss: 0.4823\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4803 - val_loss: 0.4774\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4755 - val_loss: 0.4747\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4712 - val_loss: 0.4685\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4671 - val_loss: 0.4638\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4634 - val_loss: 0.4608\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4599 - val_loss: 0.4527\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4568 - val_loss: 0.4479\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4538 - val_loss: 0.4438\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4511 - val_loss: 0.4389\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4486 - val_loss: 0.4347\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4463 - val_loss: 0.4300\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4441 - val_loss: 0.4269\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4422 - val_loss: 0.4229\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4403 - val_loss: 0.4199\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4387 - val_loss: 0.4167\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4370 - val_loss: 0.4138\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4355 - val_loss: 0.4123\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4341 - val_loss: 0.4102\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4326 - val_loss: 0.4083\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4314 - val_loss: 0.4069\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4301 - val_loss: 0.4057\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4290 - val_loss: 0.4045\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4280 - val_loss: 0.4037\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4269 - val_loss: 0.4028\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4259 - val_loss: 0.4024\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4249 - val_loss: 0.4021\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4239 - val_loss: 0.4022\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4232 - val_loss: 0.4012\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4223 - val_loss: 0.4006\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4214 - val_loss: 0.4003\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4206 - val_loss: 0.3999\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4198 - val_loss: 0.4004\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4189 - val_loss: 0.4000\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4179 - val_loss: 0.3999\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4170 - val_loss: 0.3996\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4161 - val_loss: 0.3989\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4152 - val_loss: 0.3983\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4142 - val_loss: 0.3987\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4134 - val_loss: 0.3992\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4125 - val_loss: 0.3983\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4115 - val_loss: 0.3981\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4108 - val_loss: 0.3973\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4098 - val_loss: 0.3979\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4090 - val_loss: 0.3971\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4081 - val_loss: 0.3967\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4073 - val_loss: 0.3968\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4065 - val_loss: 0.3944\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4059 - val_loss: 0.3949\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4053 - val_loss: 0.3941\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4046 - val_loss: 0.3946\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4041 - val_loss: 0.3942\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4035 - val_loss: 0.3940\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4030 - val_loss: 0.3937\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4025 - val_loss: 0.3941\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4021 - val_loss: 0.3931\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4016 - val_loss: 0.3928\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4013 - val_loss: 0.3931\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4009 - val_loss: 0.3931\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4006 - val_loss: 0.3930\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4002 - val_loss: 0.3919\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3999 - val_loss: 0.3911\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3996 - val_loss: 0.3924\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3993 - val_loss: 0.3922\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3990 - val_loss: 0.3932\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3988 - val_loss: 0.3923\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3985 - val_loss: 0.3910\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3982 - val_loss: 0.3915\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3980 - val_loss: 0.3906\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3977 - val_loss: 0.3901\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3975 - val_loss: 0.3901\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3972 - val_loss: 0.3906\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3970 - val_loss: 0.3915\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3968 - val_loss: 0.3899\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3965 - val_loss: 0.3899\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3963 - val_loss: 0.3897\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3961 - val_loss: 0.3899\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3959 - val_loss: 0.3907\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3957 - val_loss: 0.3906\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3955 - val_loss: 0.3894\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3953 - val_loss: 0.3901\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3951 - val_loss: 0.3899\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3949 - val_loss: 0.3894\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.4330\n",
      "[CV 2/5] END learning_rate=0.0004969432001318289, n_hidden=1, n_neurons=11;, score=-0.433 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 4.1141 - val_loss: 6.9272\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.4620 - val_loss: 3.6622\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9784 - val_loss: 2.7302\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8575 - val_loss: 2.3385\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8072 - val_loss: 2.1462\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7727 - val_loss: 2.0193\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7443 - val_loss: 1.9137\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7197 - val_loss: 1.8405\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6979 - val_loss: 1.7596\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6780 - val_loss: 1.7100\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6601 - val_loss: 1.6231\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6436 - val_loss: 1.5805\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6284 - val_loss: 1.5296\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6144 - val_loss: 1.4947\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6010 - val_loss: 1.4538\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5888 - val_loss: 1.4220\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5774 - val_loss: 1.3849\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5669 - val_loss: 1.3530\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5570 - val_loss: 1.3053\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5477 - val_loss: 1.2762\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5394 - val_loss: 1.2335\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5316 - val_loss: 1.1943\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5243 - val_loss: 1.1567\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5176 - val_loss: 1.1219\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5114 - val_loss: 1.0787\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5057 - val_loss: 1.0359\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5005 - val_loss: 0.9967\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4957 - val_loss: 0.9575\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4912 - val_loss: 0.9153\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4872 - val_loss: 0.8768\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4834 - val_loss: 0.8393\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4800 - val_loss: 0.7975\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4767 - val_loss: 0.7584\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4737 - val_loss: 0.7243\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4709 - val_loss: 0.6890\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4683 - val_loss: 0.6571\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4658 - val_loss: 0.6268\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4634 - val_loss: 0.5980\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4611 - val_loss: 0.5715\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4590 - val_loss: 0.5474\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4571 - val_loss: 0.5260\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4553 - val_loss: 0.5061\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4535 - val_loss: 0.4872\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4518 - val_loss: 0.4720\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4502 - val_loss: 0.4573\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4487 - val_loss: 0.4455\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4473 - val_loss: 0.4359\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4458 - val_loss: 0.4286\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4445 - val_loss: 0.4228\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4432 - val_loss: 0.4188\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4420 - val_loss: 0.4169\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4409 - val_loss: 0.4164\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4397 - val_loss: 0.4174\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4386 - val_loss: 0.4199\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4376 - val_loss: 0.4241\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4366 - val_loss: 0.4300\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4355 - val_loss: 0.4366\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4346 - val_loss: 0.4453\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4336 - val_loss: 0.4550\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4327 - val_loss: 0.4659\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4318 - val_loss: 0.4790\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4310 - val_loss: 0.4909\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.4572\n",
      "[CV 3/5] END learning_rate=0.0004969432001318289, n_hidden=1, n_neurons=11;, score=-0.457 total time=  37.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 4.4073 - val_loss: 2.8598\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 2.2201 - val_loss: 2.0919\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.6146 - val_loss: 1.4463\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.2946 - val_loss: 1.1208\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.0844 - val_loss: 0.9712\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9388 - val_loss: 0.8619\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.8394 - val_loss: 0.7491\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7711 - val_loss: 0.6982\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7265 - val_loss: 0.6581\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6958 - val_loss: 0.6330\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6742 - val_loss: 0.6090\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6573 - val_loss: 0.5939\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6438 - val_loss: 0.5814\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6322 - val_loss: 0.5774\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6219 - val_loss: 0.5735\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6126 - val_loss: 0.5606\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6039 - val_loss: 0.5484\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5960 - val_loss: 0.5433\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5886 - val_loss: 0.5358\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5817 - val_loss: 0.5281\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5754 - val_loss: 0.5333\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5691 - val_loss: 0.5252\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5633 - val_loss: 0.5185\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5577 - val_loss: 0.5125\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5524 - val_loss: 0.5070\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5473 - val_loss: 0.5023\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5427 - val_loss: 0.5050\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5380 - val_loss: 0.4960\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5341 - val_loss: 0.4980\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5299 - val_loss: 0.5039\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5262 - val_loss: 0.4937\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5228 - val_loss: 0.4892\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5193 - val_loss: 0.4920\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5163 - val_loss: 0.4855\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5132 - val_loss: 0.4966\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5102 - val_loss: 0.4951\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5072 - val_loss: 0.4898\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5043 - val_loss: 0.4924\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5014 - val_loss: 0.4841\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4988 - val_loss: 0.4889\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4960 - val_loss: 0.4905\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4934 - val_loss: 0.4799\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4909 - val_loss: 0.4831\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4882 - val_loss: 0.4885\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4855 - val_loss: 0.4897\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4830 - val_loss: 0.4716\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4807 - val_loss: 0.4686\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4781 - val_loss: 0.4708\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4758 - val_loss: 0.4752\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4734 - val_loss: 0.4761\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4711 - val_loss: 0.4767\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4686 - val_loss: 0.4808\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4665 - val_loss: 0.4802\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4642 - val_loss: 0.4700\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4624 - val_loss: 0.4764\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4605 - val_loss: 0.4670\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4586 - val_loss: 0.4771\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4567 - val_loss: 0.4791\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4551 - val_loss: 0.4691\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4533 - val_loss: 0.4687\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4516 - val_loss: 0.4801\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4501 - val_loss: 0.4732\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4486 - val_loss: 0.4804\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4472 - val_loss: 0.4812\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4459 - val_loss: 0.4809\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4445 - val_loss: 0.4614\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4432 - val_loss: 0.4740\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4420 - val_loss: 0.4767\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4407 - val_loss: 0.4796\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4397 - val_loss: 0.4958\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4387 - val_loss: 0.4911\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4377 - val_loss: 0.4786\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4367 - val_loss: 0.4778\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.433 - 1s 66us/sample - loss: 0.4359 - val_loss: 0.4825\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4352 - val_loss: 0.4730\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4344 - val_loss: 0.4651\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3783\n",
      "[CV 4/5] END learning_rate=0.0004969432001318289, n_hidden=1, n_neurons=11;, score=-0.378 total time=  46.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 3.8806 - val_loss: 9.8420\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.5204 - val_loss: 3.0077\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9392 - val_loss: 1.4687\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.8030 - val_loss: 0.9326\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7480 - val_loss: 0.7916\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7140 - val_loss: 0.7227\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6879 - val_loss: 0.6888\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6653 - val_loss: 0.6616\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6448 - val_loss: 0.6418\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6264 - val_loss: 0.6247\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6093 - val_loss: 0.6082\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5931 - val_loss: 0.5944\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5781 - val_loss: 0.5773\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5641 - val_loss: 0.5645\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5514 - val_loss: 0.5511\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5398 - val_loss: 0.5392\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5292 - val_loss: 0.5253\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5195 - val_loss: 0.5127\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5105 - val_loss: 0.5039\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5023 - val_loss: 0.4945\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4946 - val_loss: 0.4838\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4880 - val_loss: 0.4754\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4818 - val_loss: 0.4680\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4765 - val_loss: 0.4608\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4716 - val_loss: 0.4546\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4671 - val_loss: 0.4488\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4629 - val_loss: 0.4439\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4593 - val_loss: 0.4394\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4561 - val_loss: 0.4363\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4531 - val_loss: 0.4333\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4504 - val_loss: 0.4307\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4481 - val_loss: 0.4295\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4458 - val_loss: 0.4304\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4437 - val_loss: 0.4290\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4418 - val_loss: 0.4290\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4401 - val_loss: 0.4279\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4384 - val_loss: 0.4265\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4369 - val_loss: 0.4267\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4356 - val_loss: 0.4293\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4342 - val_loss: 0.4312\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4330 - val_loss: 0.4317\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4319 - val_loss: 0.4314\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4308 - val_loss: 0.4378\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4298 - val_loss: 0.4352\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4288 - val_loss: 0.4389\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4280 - val_loss: 0.4395\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4271 - val_loss: 0.4410\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.4591\n",
      "[CV 5/5] END learning_rate=0.0004969432001318289, n_hidden=1, n_neurons=11;, score=-0.459 total time=  28.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 97us/sample - loss: 2.4036 - val_loss: 1.1877\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.0503 - val_loss: 0.9358\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.8790 - val_loss: 0.7800\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7736 - val_loss: 0.6813\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6978 - val_loss: 0.6141\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6430 - val_loss: 0.5711\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6002 - val_loss: 0.5376\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5665 - val_loss: 0.5173\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5378 - val_loss: 0.4951\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5130 - val_loss: 0.4726\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4924 - val_loss: 0.4513\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4744 - val_loss: 0.4352\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4589 - val_loss: 0.4190\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4464 - val_loss: 0.4059\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4359 - val_loss: 0.3976\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4272 - val_loss: 0.3940\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4201 - val_loss: 0.3902\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4146 - val_loss: 0.3878\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4096 - val_loss: 0.3852\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4060 - val_loss: 0.3874\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4018 - val_loss: 0.3970\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3985 - val_loss: 0.3954\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3959 - val_loss: 0.3907\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3933 - val_loss: 0.3911\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3908 - val_loss: 0.3965\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3887 - val_loss: 0.3966\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3865 - val_loss: 0.3959\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3849 - val_loss: 0.3969\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3829 - val_loss: 0.4037\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.3655\n",
      "[CV 1/5] END learning_rate=0.0009092976729274063, n_hidden=3, n_neurons=39;, score=-0.366 total time=  19.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 2.3053 - val_loss: 1.0448\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.8190 - val_loss: 0.8087\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.7037 - val_loss: 0.7105\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6541 - val_loss: 0.6115\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6183 - val_loss: 0.5941\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5874 - val_loss: 0.5554\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5595 - val_loss: 0.5526\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5351 - val_loss: 0.5203\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5118 - val_loss: 0.5057\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4919 - val_loss: 0.4757\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4737 - val_loss: 0.4486\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4579 - val_loss: 0.4344\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4447 - val_loss: 0.4244\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4336 - val_loss: 0.4160\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4240 - val_loss: 0.4072\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4163 - val_loss: 0.3985\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4095 - val_loss: 0.3914\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4032 - val_loss: 0.3888\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3979 - val_loss: 0.3939\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3933 - val_loss: 0.3772\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3890 - val_loss: 0.3741\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3852 - val_loss: 0.3716\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3816 - val_loss: 0.3706\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3782 - val_loss: 0.3656\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3752 - val_loss: 0.3649\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3723 - val_loss: 0.3658\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3696 - val_loss: 0.3578\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3669 - val_loss: 0.3573\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3648 - val_loss: 0.3530\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3625 - val_loss: 0.3533\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3605 - val_loss: 0.3479\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3583 - val_loss: 0.3479\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3564 - val_loss: 0.3618\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3546 - val_loss: 0.3435\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3531 - val_loss: 0.3459\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3514 - val_loss: 0.3421\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3495 - val_loss: 0.3406\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3484 - val_loss: 0.3451\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3469 - val_loss: 0.3460\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3453 - val_loss: 0.3417\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3440 - val_loss: 0.3400\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3428 - val_loss: 0.3351\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3417 - val_loss: 0.3383\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3405 - val_loss: 0.3326\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3392 - val_loss: 0.3323\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3381 - val_loss: 0.3313\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3372 - val_loss: 0.3300\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3361 - val_loss: 0.3295\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3352 - val_loss: 0.3402\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3344 - val_loss: 0.3272\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3333 - val_loss: 0.3461\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3331 - val_loss: 0.3272\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3320 - val_loss: 0.3293\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3312 - val_loss: 0.3289\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3303 - val_loss: 0.3278\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3301 - val_loss: 0.3249\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3294 - val_loss: 0.3279\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3284 - val_loss: 0.3312\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3283 - val_loss: 0.3232\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3274 - val_loss: 0.3295\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3271 - val_loss: 0.3262\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3263 - val_loss: 0.3337\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3257 - val_loss: 0.3210\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3252 - val_loss: 0.3199\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3246 - val_loss: 0.3224\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3244 - val_loss: 0.3228\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3234 - val_loss: 0.3199\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3228 - val_loss: 0.3207\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3225 - val_loss: 0.3292\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3224 - val_loss: 0.3189\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3216 - val_loss: 0.3206\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3210 - val_loss: 0.3218\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3206 - val_loss: 0.3182\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3201 - val_loss: 0.3190\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3198 - val_loss: 0.3181\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3192 - val_loss: 0.3171\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3191 - val_loss: 0.3154\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3182 - val_loss: 0.3175\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3179 - val_loss: 0.3154\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 0.3177 - val_loss: 0.3142\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3167 - val_loss: 0.3335\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3168 - val_loss: 0.3242\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3164 - val_loss: 0.3329\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3159 - val_loss: 0.3144\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3156 - val_loss: 0.3155\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3151 - val_loss: 0.3120\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3144 - val_loss: 0.3191\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3141 - val_loss: 0.3111\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3138 - val_loss: 0.3318\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3134 - val_loss: 0.3126\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3131 - val_loss: 0.3324\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3128 - val_loss: 0.3335\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3124 - val_loss: 0.3121\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3120 - val_loss: 0.3310\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3114 - val_loss: 0.3493\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3117 - val_loss: 0.3194\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3106 - val_loss: 0.3111\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3105 - val_loss: 0.3071\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3102 - val_loss: 0.3177\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3096 - val_loss: 0.3091\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3657\n",
      "[CV 2/5] END learning_rate=0.0009092976729274063, n_hidden=3, n_neurons=39;, score=-0.366 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 1.9221 - val_loss: 15.3713\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.8450 - val_loss: 6.1563\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6861 - val_loss: 3.3641\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6316 - val_loss: 1.7010\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5945 - val_loss: 1.1710\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5643 - val_loss: 0.7405\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5393 - val_loss: 0.5864\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5177 - val_loss: 0.5447\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4991 - val_loss: 0.4776\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4828 - val_loss: 0.4523\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4692 - val_loss: 0.4389\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4570 - val_loss: 0.4268\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4458 - val_loss: 0.4187\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4363 - val_loss: 0.4098\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4278 - val_loss: 0.4010\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4199 - val_loss: 0.3945\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4130 - val_loss: 0.3886\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4068 - val_loss: 0.4154\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4011 - val_loss: 0.4091\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3963 - val_loss: 0.4282\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3922 - val_loss: 0.4688\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3880 - val_loss: 0.5599\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3846 - val_loss: 0.5294\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3814 - val_loss: 0.5911\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3786 - val_loss: 0.5822\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3758 - val_loss: 0.6276\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3734 - val_loss: 0.6374\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.4071\n",
      "[CV 3/5] END learning_rate=0.0009092976729274063, n_hidden=3, n_neurons=39;, score=-0.407 total time=  17.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 110us/sample - loss: 1.8304 - val_loss: 22.8607\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9489 - val_loss: 1.3491\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6491 - val_loss: 0.8069\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6139 - val_loss: 0.6216\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5862 - val_loss: 0.5578\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5628 - val_loss: 0.5345\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5415 - val_loss: 0.5071\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5235 - val_loss: 0.4985\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5065 - val_loss: 0.4886\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4916 - val_loss: 0.4599\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4787 - val_loss: 0.4809\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4675 - val_loss: 0.4653\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4578 - val_loss: 0.4406\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4495 - val_loss: 0.4455\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4419 - val_loss: 0.4538\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4355 - val_loss: 0.4415\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4299 - val_loss: 0.4350\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4246 - val_loss: 0.4290\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4201 - val_loss: 0.4288\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4160 - val_loss: 0.4485\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4122 - val_loss: 0.4347\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4088 - val_loss: 0.4520\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4057 - val_loss: 0.4206\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4029 - val_loss: 0.4126\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4002 - val_loss: 0.4190\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3972 - val_loss: 0.4200\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3954 - val_loss: 0.4360\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3930 - val_loss: 0.4253\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3910 - val_loss: 0.4290\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3884 - val_loss: 0.4329\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3871 - val_loss: 0.4057\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3850 - val_loss: 0.4265\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3830 - val_loss: 0.4134\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3819 - val_loss: 0.4255\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3802 - val_loss: 0.3989\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3787 - val_loss: 0.4096\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3772 - val_loss: 0.4314\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3762 - val_loss: 0.3999\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3742 - val_loss: 0.4009\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3730 - val_loss: 0.4138\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3716 - val_loss: 0.4129\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3710 - val_loss: 0.3998\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3693 - val_loss: 0.3947\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3684 - val_loss: 0.4103\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3672 - val_loss: 0.3832\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3661 - val_loss: 0.4064\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3653 - val_loss: 0.3986\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3642 - val_loss: 0.3905\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3633 - val_loss: 0.3822\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3623 - val_loss: 0.4021\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3612 - val_loss: 0.3908\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3606 - val_loss: 0.3995\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3599 - val_loss: 0.3777\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3586 - val_loss: 0.4034\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3582 - val_loss: 0.3761\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3572 - val_loss: 0.3792\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3567 - val_loss: 0.3645\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3557 - val_loss: 0.3644\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3549 - val_loss: 0.3767\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3540 - val_loss: 0.4106\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3533 - val_loss: 0.3635\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3526 - val_loss: 0.3878\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3522 - val_loss: 0.3561\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3512 - val_loss: 0.3853\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3508 - val_loss: 0.3965\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3500 - val_loss: 0.3765\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3500 - val_loss: 0.3783\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3492 - val_loss: 0.3534\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3490 - val_loss: 0.3647\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3482 - val_loss: 0.3879\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3474 - val_loss: 0.3639\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3470 - val_loss: 0.3803\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3464 - val_loss: 0.3695\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3459 - val_loss: 0.3703\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3453 - val_loss: 0.3615\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3448 - val_loss: 0.3552\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3442 - val_loss: 0.3571\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3437 - val_loss: 0.3850\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3038\n",
      "[CV 4/5] END learning_rate=0.0009092976729274063, n_hidden=3, n_neurons=39;, score=-0.304 total time=  49.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 2.3863 - val_loss: 1.6032\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.9223 - val_loss: 1.2218\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7305 - val_loss: 0.7476\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6721 - val_loss: 0.6446\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6358 - val_loss: 0.5995\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6073 - val_loss: 0.5718\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5817 - val_loss: 0.5501\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5601 - val_loss: 0.5294\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5398 - val_loss: 0.5127\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5221 - val_loss: 0.4957\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5059 - val_loss: 0.4804\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4917 - val_loss: 0.4670\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4789 - val_loss: 0.4554\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4673 - val_loss: 0.4477\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4579 - val_loss: 0.4365\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4489 - val_loss: 0.4255\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4412 - val_loss: 0.4161\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4334 - val_loss: 0.4099\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4274 - val_loss: 0.4031\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4215 - val_loss: 0.3991\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4166 - val_loss: 0.3940\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4121 - val_loss: 0.3910\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4079 - val_loss: 0.3928\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4042 - val_loss: 0.3864\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4005 - val_loss: 0.3836\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3967 - val_loss: 0.3833\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3942 - val_loss: 0.3823\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3906 - val_loss: 0.3833\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3884 - val_loss: 0.3844\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3863 - val_loss: 0.3863\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3835 - val_loss: 0.3870\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3812 - val_loss: 0.3857\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3788 - val_loss: 0.3901\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3770 - val_loss: 0.3919\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3749 - val_loss: 0.3884\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3731 - val_loss: 0.3893\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3711 - val_loss: 0.3915\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4004\n",
      "[CV 5/5] END learning_rate=0.0009092976729274063, n_hidden=3, n_neurons=39;, score=-0.400 total time=  23.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 0.6558 - val_loss: 6.0783\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4968 - val_loss: 5.5438\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8160 - val_loss: 2.3312\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7961 - val_loss: 0.3572\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3803 - val_loss: 2.2871\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4044 - val_loss: 1.5542\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3962 - val_loss: 2.7968\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3799 - val_loss: 1.4150\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3810 - val_loss: 0.4392\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4242 - val_loss: 0.4282\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3303 - val_loss: 0.3104\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3267 - val_loss: 3.8153\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3648 - val_loss: 0.7944\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3222 - val_loss: 4.9898\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3445 - val_loss: 0.3285\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3070 - val_loss: 0.3010\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3017 - val_loss: 0.2860\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2995 - val_loss: 0.2895\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2999 - val_loss: 0.2795\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2947 - val_loss: 0.2942\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2915 - val_loss: 0.3002\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2889 - val_loss: 0.2771\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2894 - val_loss: 0.2804\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2869 - val_loss: 0.3090\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2850 - val_loss: 0.2801\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2829 - val_loss: 0.2746\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2800 - val_loss: 0.3111\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2769 - val_loss: 0.2974\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2785 - val_loss: 0.2751\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2757 - val_loss: 0.2772\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2733 - val_loss: 0.2740\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2741 - val_loss: 0.3062\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2707 - val_loss: 0.2843\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2722 - val_loss: 0.2738\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2694 - val_loss: 0.2718\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2681 - val_loss: 0.2741\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2673 - val_loss: 0.2851\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2679 - val_loss: 0.3048\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2663 - val_loss: 0.2617\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2642 - val_loss: 0.2691\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2646 - val_loss: 0.2857\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2629 - val_loss: 0.3081\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2618 - val_loss: 0.7415\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2680 - val_loss: 0.4434\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2641 - val_loss: 0.2820\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2611 - val_loss: 0.3243\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2615 - val_loss: 0.2834\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2590 - val_loss: 0.3197\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2602 - val_loss: 0.2757\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.2866\n",
      "[CV 1/5] END learning_rate=0.020355884634043687, n_hidden=2, n_neurons=92;, score=-0.287 total time=  29.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 0.5607 - val_loss: 39.0286\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 1.0415 - val_loss: 4.3235\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5073 - val_loss: 174.1706\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: nan - val_loss: nan\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: nan\n",
      "[CV 2/5] END learning_rate=0.020355884634043687, n_hidden=2, n_neurons=92;, score=nan total time=   7.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 0.6150 - val_loss: 0.7531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3990 - val_loss: 1.6889\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3664 - val_loss: 0.8069\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3530 - val_loss: 1.3511\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3500 - val_loss: 0.9406\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3389 - val_loss: 0.6250\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3316 - val_loss: 1.5890\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3266 - val_loss: 0.9306\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3237 - val_loss: 1.0034\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3182 - val_loss: 1.3758\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3143 - val_loss: 1.1742\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3119 - val_loss: 0.7601\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3087 - val_loss: 0.4630\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3137 - val_loss: 0.4681\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3076 - val_loss: 0.4247\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3018 - val_loss: 1.0144\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2998 - val_loss: 0.7597\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2979 - val_loss: 0.5399\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2938 - val_loss: 0.3396\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2934 - val_loss: 0.3003\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2919 - val_loss: 2.2994\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2867 - val_loss: 1.6541\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2915 - val_loss: 0.3846\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2866 - val_loss: 1.6363\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2864 - val_loss: 1.2334\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2823 - val_loss: 1.5336\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2808 - val_loss: 0.4931\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2808 - val_loss: 0.3470\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2769 - val_loss: 0.3124\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2799 - val_loss: 1.2352\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3514\n",
      "[CV 3/5] END learning_rate=0.020355884634043687, n_hidden=2, n_neurons=92;, score=-0.351 total time=  18.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 0.7355 - val_loss: 17.0891\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4424 - val_loss: 1.8349\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4632 - val_loss: 0.4382\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3733 - val_loss: 0.4846\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3608 - val_loss: 1.7675\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3678 - val_loss: 13.9619\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3771 - val_loss: 1.0472\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3605 - val_loss: 1.3190\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3527 - val_loss: 0.8093\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3400 - val_loss: 0.4260\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3336 - val_loss: 0.3885\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3308 - val_loss: 0.3101\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3284 - val_loss: 0.3415\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3184 - val_loss: 0.3920\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3167 - val_loss: 0.2974\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3130 - val_loss: 0.3530\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3116 - val_loss: 0.3351\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3053 - val_loss: 0.4246\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3050 - val_loss: 0.2979\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3024 - val_loss: 0.3520\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2979 - val_loss: 0.3718\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2990 - val_loss: 0.3664\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3008 - val_loss: 0.3050\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2938 - val_loss: 0.2883\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2961 - val_loss: 0.3008\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2919 - val_loss: 0.3211\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2904 - val_loss: 0.3425\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2880 - val_loss: 0.4935\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2878 - val_loss: 0.2916\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2869 - val_loss: 0.4648\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2911 - val_loss: 0.2812\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2859 - val_loss: 0.2721\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2822 - val_loss: 0.3898\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2836 - val_loss: 0.3063\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2777 - val_loss: 0.3213\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2787 - val_loss: 0.3005\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2768 - val_loss: 0.3904\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2802 - val_loss: 0.2895\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2873 - val_loss: 0.4280\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2789 - val_loss: 0.4944\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2776 - val_loss: 0.5342\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2735 - val_loss: 0.2850\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.2566\n",
      "[CV 4/5] END learning_rate=0.020355884634043687, n_hidden=2, n_neurons=92;, score=-0.257 total time=  25.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 0.6917 - val_loss: 20.9451\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4544 - val_loss: 23.7815\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4568 - val_loss: 46.5656\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.8461 - val_loss: 5.0091\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4835 - val_loss: 0.7816\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 9.7967 - val_loss: 0.9744\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4961 - val_loss: 0.7409\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4400 - val_loss: 0.5129\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4191 - val_loss: 0.4039\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4056 - val_loss: 0.4056\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3928 - val_loss: 0.3890\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3813 - val_loss: 0.3660\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3784 - val_loss: 0.8108\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3752 - val_loss: 0.6139\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3683 - val_loss: 0.6932\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3651 - val_loss: 0.4276\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3596 - val_loss: 4.7230\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4460 - val_loss: 1.8070\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3754 - val_loss: 0.3755\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3589 - val_loss: 0.3314\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3506 - val_loss: 0.3328\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3454 - val_loss: 0.3174\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3413 - val_loss: 0.4655\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3420 - val_loss: 0.4500\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3390 - val_loss: 0.6624\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3332 - val_loss: 0.4355\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3327 - val_loss: 0.3382\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3297 - val_loss: 0.3421\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3243 - val_loss: 0.4054\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3244 - val_loss: 0.2988\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3198 - val_loss: 0.3517\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3222 - val_loss: 0.3326\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3168 - val_loss: 0.3060\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3174 - val_loss: 0.3034\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3126 - val_loss: 0.3428\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3158 - val_loss: 0.3302\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3124 - val_loss: 0.2998\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3102 - val_loss: 0.3014\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3082 - val_loss: 0.3292\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3070 - val_loss: 0.2984\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3073 - val_loss: 0.3148\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3048 - val_loss: 0.3050\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3034 - val_loss: 0.3341\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3007 - val_loss: 0.3022\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2995 - val_loss: 0.3102\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2980 - val_loss: 0.3067\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2969 - val_loss: 0.3208\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2986 - val_loss: 0.3209\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2967 - val_loss: 0.3072\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2933 - val_loss: 0.3284\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3571\n",
      "[CV 5/5] END learning_rate=0.020355884634043687, n_hidden=2, n_neurons=92;, score=-0.357 total time=  30.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.8660 - val_loss: 14.0605\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7274 - val_loss: 10.0210\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5735 - val_loss: 0.5650\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4506 - val_loss: 0.4717\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4327 - val_loss: 0.4123\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4224 - val_loss: 0.3958\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4147 - val_loss: 0.3928\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4087 - val_loss: 0.3719\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4019 - val_loss: 0.3773\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3977 - val_loss: 0.4064\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3942 - val_loss: 0.4122\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3902 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3856 - val_loss: 0.4150\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3856 - val_loss: 0.4033\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3830 - val_loss: 0.4246\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3791 - val_loss: 0.3933\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3772 - val_loss: 0.3874\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3749 - val_loss: 0.4074\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.3578\n",
      "[CV 1/5] END learning_rate=0.00632509177371149, n_hidden=1, n_neurons=63;, score=-0.358 total time=   9.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.9030 - val_loss: 0.6068\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5445 - val_loss: 5.5939\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4901 - val_loss: 33.1797\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8697 - val_loss: 20.6041\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5517 - val_loss: 1.8537\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4260 - val_loss: 0.4123\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3961 - val_loss: 0.3757\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3876 - val_loss: 0.3738\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3805 - val_loss: 0.3955\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3765 - val_loss: 0.3965\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3716 - val_loss: 0.3910\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3682 - val_loss: 0.3920\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3648 - val_loss: 0.3996\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3629 - val_loss: 0.4002\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3601 - val_loss: 0.3890\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3577 - val_loss: 0.3772\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3559 - val_loss: 0.3773\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3539 - val_loss: 0.3734\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3522 - val_loss: 0.3907\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3502 - val_loss: 0.3689\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3499 - val_loss: 0.3757\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3482 - val_loss: 0.3799\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3459 - val_loss: 0.3606\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3451 - val_loss: 0.3725\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3443 - val_loss: 0.3624\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3429 - val_loss: 0.3900\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3420 - val_loss: 0.3947\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3408 - val_loss: 0.3629\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3399 - val_loss: 0.3913\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3390 - val_loss: 0.3555\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3378 - val_loss: 0.3790\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3372 - val_loss: 0.3688\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3354 - val_loss: 0.3832\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3345 - val_loss: 0.3781\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3343 - val_loss: 0.3776\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3331 - val_loss: 0.3655\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3319 - val_loss: 0.3677\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3310 - val_loss: 0.3481\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3309 - val_loss: 0.4136\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3294 - val_loss: 0.3331\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3285 - val_loss: 0.3844\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3280 - val_loss: 0.3577\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3276 - val_loss: 0.3963\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3268 - val_loss: 0.3539\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3258 - val_loss: 0.3378\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3250 - val_loss: 0.3452\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3248 - val_loss: 0.3571\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3241 - val_loss: 0.3300\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3236 - val_loss: 0.3548\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3226 - val_loss: 0.3177\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3225 - val_loss: 0.4862\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3229 - val_loss: 0.3520\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3224 - val_loss: 0.5488\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3214 - val_loss: 0.3366\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3216 - val_loss: 0.6497\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3213 - val_loss: 0.3736\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3203 - val_loss: 0.9595\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3246 - val_loss: 0.5758\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3216 - val_loss: 1.3351\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3239 - val_loss: 0.8435\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4173\n",
      "[CV 2/5] END learning_rate=0.00632509177371149, n_hidden=1, n_neurons=63;, score=-0.417 total time=  32.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.8804 - val_loss: 0.6620\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5149 - val_loss: 0.8081\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4574 - val_loss: 0.4279\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4294 - val_loss: 0.3974\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4132 - val_loss: 0.5676\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4037 - val_loss: 0.8387\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3948 - val_loss: 1.1205\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3888 - val_loss: 1.3104\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3830 - val_loss: 1.2108\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3797 - val_loss: 1.3251\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3762 - val_loss: 1.2760\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3735 - val_loss: 1.0429\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3701 - val_loss: 1.5785\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3678 - val_loss: 1.6276\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.4439\n",
      "[CV 3/5] END learning_rate=0.00632509177371149, n_hidden=1, n_neurons=63;, score=-0.444 total time=   7.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.9581 - val_loss: 6.2173\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6932 - val_loss: 17.6533\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7208 - val_loss: 6.0413\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5117 - val_loss: 0.5591\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4443 - val_loss: 0.4231\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4252 - val_loss: 0.4048\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4174 - val_loss: 0.4053\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4098 - val_loss: 0.4080\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4083 - val_loss: 0.3878\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4003 - val_loss: 0.3933\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3975 - val_loss: 0.4094\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3933 - val_loss: 0.3960\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3945 - val_loss: 0.3660\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3890 - val_loss: 0.4111\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3880 - val_loss: 0.4079\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3844 - val_loss: 0.3486\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3829 - val_loss: 0.3754\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3785 - val_loss: 0.3812\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3774 - val_loss: 0.3470\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3787 - val_loss: 0.3683\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3743 - val_loss: 0.3691\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3729 - val_loss: 0.3601\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3703 - val_loss: 0.4060\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3708 - val_loss: 0.3452\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3692 - val_loss: 0.3450\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3686 - val_loss: 0.3448\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3683 - val_loss: 0.3907\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3659 - val_loss: 0.3595\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3658 - val_loss: 0.3816\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3662 - val_loss: 0.3338\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3631 - val_loss: 0.3430\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3624 - val_loss: 0.3430\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3610 - val_loss: 0.3958\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3603 - val_loss: 0.3797\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3589 - val_loss: 0.4397\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3698 - val_loss: 0.3306\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3630 - val_loss: 0.3545\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3618 - val_loss: 0.3781\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3564 - val_loss: 0.3264\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3563 - val_loss: 0.4087\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3555 - val_loss: 0.3585\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3535 - val_loss: 0.3342\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3547 - val_loss: 0.3505\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3529 - val_loss: 0.3273\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3568 - val_loss: 0.3588\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3499 - val_loss: 0.3565\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3541 - val_loss: 0.4006\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3521 - val_loss: 0.3347\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3498 - val_loss: 0.4102\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.3025\n",
      "[CV 4/5] END learning_rate=0.00632509177371149, n_hidden=1, n_neurons=63;, score=-0.303 total time=  27.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.9858 - val_loss: 18.7971\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.7197 - val_loss: 1.0666\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4795 - val_loss: 0.5251\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4361 - val_loss: 0.4207\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4172 - val_loss: 0.4030\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4076 - val_loss: 0.3934\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3991 - val_loss: 0.3928\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3932 - val_loss: 0.3878\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3872 - val_loss: 0.7356\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4145 - val_loss: 0.3785\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3823 - val_loss: 0.3938\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3802 - val_loss: 0.3908\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3740 - val_loss: 0.3954\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3877 - val_loss: 0.3673\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3746 - val_loss: 0.3688\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3698 - val_loss: 0.3568\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3646 - val_loss: 0.3569\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3653 - val_loss: 0.3798\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3593 - val_loss: 0.3600\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3592 - val_loss: 0.3690\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3606 - val_loss: 0.3685\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3566 - val_loss: 0.3821\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3566 - val_loss: 0.3437\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3560 - val_loss: 0.3479\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3520 - val_loss: 0.3743\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3504 - val_loss: 0.3747\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3522 - val_loss: 0.3806\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3488 - val_loss: 0.3309\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3608 - val_loss: 0.3476\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3460 - val_loss: 0.3403\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3456 - val_loss: 0.3823\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3447 - val_loss: 0.3510\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3454 - val_loss: 0.3400\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3420 - val_loss: 0.3439\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3419 - val_loss: 0.3289\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3392 - val_loss: 0.3414\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3391 - val_loss: 0.3325\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3386 - val_loss: 0.3486\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3367 - val_loss: 0.3322\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3362 - val_loss: 0.3363\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3358 - val_loss: 0.3296\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3365 - val_loss: 0.3257\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3354 - val_loss: 0.3283\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3376 - val_loss: 0.3491\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3337 - val_loss: 0.3510\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3797 - val_loss: 0.3750\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3344 - val_loss: 0.3398\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3306 - val_loss: 0.3229\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3297 - val_loss: 0.3804\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3362 - val_loss: 0.3640\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3299 - val_loss: 0.3189\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3286 - val_loss: 0.3781\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3263 - val_loss: 0.3377\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3261 - val_loss: 0.3352\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3257 - val_loss: 0.3175\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3359 - val_loss: 0.3212\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3263 - val_loss: 0.3219\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3253 - val_loss: 0.3783\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3243 - val_loss: 0.3296\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3253 - val_loss: 0.3180\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3256 - val_loss: 0.3353\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3214 - val_loss: 0.3282\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3213 - val_loss: 0.3178\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3211 - val_loss: 0.3606\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3203 - val_loss: 0.3121\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3229 - val_loss: 0.3389\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3180 - val_loss: 0.3308\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3189 - val_loss: 0.3746\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3190 - val_loss: 0.3205\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3161 - val_loss: 0.3134\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3164 - val_loss: 0.3203\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3145 - val_loss: 0.3076\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3143 - val_loss: 0.3052\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3144 - val_loss: 0.3079\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3193 - val_loss: 0.3096\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3179 - val_loss: 0.3071\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3119 - val_loss: 0.3535\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3124 - val_loss: 0.3792\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3119 - val_loss: 0.3118\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3097 - val_loss: 0.3115\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3120 - val_loss: 0.3322\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3117 - val_loss: 0.3034\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3092 - val_loss: 0.3735\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3105 - val_loss: 0.3891\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3116 - val_loss: 0.3352\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3087 - val_loss: 0.3066\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3075 - val_loss: 0.3096\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3084 - val_loss: 0.3223\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3074 - val_loss: 0.3508\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3150 - val_loss: 0.3083\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3103 - val_loss: 0.3151\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3091 - val_loss: 0.3039\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3463\n",
      "[CV 5/5] END learning_rate=0.00632509177371149, n_hidden=1, n_neurons=63;, score=-0.346 total time=  50.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.6484 - val_loss: 0.6744\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6550 - val_loss: 0.5776\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5788 - val_loss: 0.5157\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5253 - val_loss: 0.7265\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4874 - val_loss: 0.4346\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4571 - val_loss: 0.4795\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4354 - val_loss: 0.4285\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4200 - val_loss: 0.4031\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4098 - val_loss: 0.3748\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4011 - val_loss: 0.3850\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3947 - val_loss: 0.3767\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3880 - val_loss: 0.4692\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3854 - val_loss: 0.3859\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3808 - val_loss: 0.3493\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3769 - val_loss: 0.4594\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3744 - val_loss: 0.3572\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3707 - val_loss: 0.4730\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3699 - val_loss: 0.3722\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3664 - val_loss: 0.6247\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3659 - val_loss: 0.3936\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3628 - val_loss: 0.6036\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3616 - val_loss: 0.3889\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3589 - val_loss: 0.6189\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3585 - val_loss: 0.3781\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3404\n",
      "[CV 1/5] END learning_rate=0.001860821195587783, n_hidden=2, n_neurons=94;, score=-0.340 total time=  14.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 107us/sample - loss: 1.7437 - val_loss: 7.7693\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7247 - val_loss: 0.5954\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5677 - val_loss: 0.5340\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5193 - val_loss: 0.5057\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4838 - val_loss: 0.4508\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4564 - val_loss: 0.4262\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4366 - val_loss: 0.4168\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4221 - val_loss: 0.4055\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4102 - val_loss: 0.3998\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4011 - val_loss: 0.3964\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3939 - val_loss: 0.3951\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3880 - val_loss: 0.3866\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3827 - val_loss: 0.3817\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3781 - val_loss: 0.3945\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3739 - val_loss: 0.3960\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3696 - val_loss: 0.3963\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3664 - val_loss: 0.3888\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3634 - val_loss: 0.3775\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3610 - val_loss: 0.3906\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3582 - val_loss: 0.3649\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3559 - val_loss: 0.3669\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3534 - val_loss: 0.3890\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3517 - val_loss: 0.3711\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3499 - val_loss: 0.3740\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3481 - val_loss: 0.3409\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3462 - val_loss: 0.3622\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3450 - val_loss: 0.3747\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3434 - val_loss: 0.3559\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3419 - val_loss: 0.3792\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3412 - val_loss: 0.3374\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3391 - val_loss: 0.3542\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3383 - val_loss: 0.3631\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3374 - val_loss: 0.3337\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3360 - val_loss: 0.3481\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3348 - val_loss: 0.3565\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3343 - val_loss: 0.3296\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3331 - val_loss: 0.3617\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3324 - val_loss: 0.3368\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3311 - val_loss: 0.3321\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3301 - val_loss: 0.3290\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3291 - val_loss: 0.3480\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3286 - val_loss: 0.3282\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3274 - val_loss: 0.3757\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3268 - val_loss: 0.3246\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3258 - val_loss: 0.3538\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3252 - val_loss: 0.3376\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3241 - val_loss: 0.3446\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3234 - val_loss: 0.3502\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3231 - val_loss: 0.3182\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3218 - val_loss: 0.3802\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3218 - val_loss: 0.3165\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3201 - val_loss: 0.3343\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3200 - val_loss: 0.3256\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3193 - val_loss: 0.3158\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3182 - val_loss: 0.3782\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3181 - val_loss: 0.3285\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3170 - val_loss: 0.3323\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3166 - val_loss: 0.3453\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3159 - val_loss: 0.3133\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3151 - val_loss: 0.3563\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3146 - val_loss: 0.3108\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3138 - val_loss: 0.3682\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3135 - val_loss: 0.3098\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3126 - val_loss: 0.3607\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3125 - val_loss: 0.3180\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3118 - val_loss: 0.4539\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3120 - val_loss: 0.3158\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3104 - val_loss: 0.3834\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3104 - val_loss: 0.3138\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3093 - val_loss: 0.3732\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3089 - val_loss: 0.3071\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3083 - val_loss: 0.3544\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3079 - val_loss: 0.3250\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3078 - val_loss: 0.3970\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3072 - val_loss: 0.3136\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3064 - val_loss: 0.4439\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3061 - val_loss: 0.3696\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3068 - val_loss: 0.6284\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3076 - val_loss: 0.4402\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3068 - val_loss: 0.5470\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3052 - val_loss: 0.3054\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3037 - val_loss: 0.3557\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3030 - val_loss: 0.3049\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3023 - val_loss: 0.3221\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3022 - val_loss: 0.3309\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3016 - val_loss: 0.3191\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3009 - val_loss: 0.3164\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3006 - val_loss: 0.3323\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3001 - val_loss: 0.3173\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2999 - val_loss: 0.3147\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2989 - val_loss: 0.3035\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2991 - val_loss: 0.3263\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2986 - val_loss: 0.3138\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2982 - val_loss: 0.3301\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2979 - val_loss: 0.2963\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2974 - val_loss: 0.3375\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2969 - val_loss: 0.2984\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2968 - val_loss: 0.3132\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2961 - val_loss: 0.2983\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2958 - val_loss: 0.3542\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3557\n",
      "[CV 2/5] END learning_rate=0.001860821195587783, n_hidden=2, n_neurons=94;, score=-0.356 total time= 1.0min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.5509 - val_loss: 12.9896\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7576 - val_loss: 3.5439\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6524 - val_loss: 0.7397\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5827 - val_loss: 0.6394\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5331 - val_loss: 1.3023\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4971 - val_loss: 1.7687\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4701 - val_loss: 1.5006\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4512 - val_loss: 1.5063\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4362 - val_loss: 1.3603\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4231 - val_loss: 0.9876\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4137 - val_loss: 0.8680\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4059 - val_loss: 0.6573\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3987 - val_loss: 0.5098\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3913 - val_loss: 0.3905\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3875 - val_loss: 0.3739\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3823 - val_loss: 0.3650\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3774 - val_loss: 0.3911\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3739 - val_loss: 0.4528\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3700 - val_loss: 0.5321\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3661 - val_loss: 0.5871\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3632 - val_loss: 0.6371\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3606 - val_loss: 0.7717\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3579 - val_loss: 0.7242\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3559 - val_loss: 0.8410\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3534 - val_loss: 0.8252\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3515 - val_loss: 0.9419\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3956\n",
      "[CV 3/5] END learning_rate=0.001860821195587783, n_hidden=2, n_neurons=94;, score=-0.396 total time=  15.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.3531 - val_loss: 1.3648\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6891 - val_loss: 0.6851\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6012 - val_loss: 0.5510\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5487 - val_loss: 0.4861\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5092 - val_loss: 0.5076\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4788 - val_loss: 0.4559\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4578 - val_loss: 0.4252\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4414 - val_loss: 0.4171\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4299 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4208 - val_loss: 0.4863\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4143 - val_loss: 0.4029\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4079 - val_loss: 0.4681\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4033 - val_loss: 0.3683\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3984 - val_loss: 0.4766\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3958 - val_loss: 0.3811\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3914 - val_loss: 0.4621\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3888 - val_loss: 0.3785\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3867 - val_loss: 0.5947\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3846 - val_loss: 0.3587\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3820 - val_loss: 0.3807\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3796 - val_loss: 0.4072\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3776 - val_loss: 0.3833\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3755 - val_loss: 0.3728\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3732 - val_loss: 0.5135\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3725 - val_loss: 0.3698\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3701 - val_loss: 0.5104\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3697 - val_loss: 0.4041\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3679 - val_loss: 0.3436\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3668 - val_loss: 0.6126\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3659 - val_loss: 0.3418\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3627 - val_loss: 0.4644\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3634 - val_loss: 0.3850\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3614 - val_loss: 0.3364\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3599 - val_loss: 0.4536\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3590 - val_loss: 0.3758\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3574 - val_loss: 0.3545\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3566 - val_loss: 0.4549\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3560 - val_loss: 0.3326\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3541 - val_loss: 0.6175\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3558 - val_loss: 0.3302\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3517 - val_loss: 0.4578\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3518 - val_loss: 0.3580\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3493 - val_loss: 0.4366\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3496 - val_loss: 0.3284\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3478 - val_loss: 0.4275\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3478 - val_loss: 0.3256\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3460 - val_loss: 0.3468\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3448 - val_loss: 0.3957\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3444 - val_loss: 0.3789\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3441 - val_loss: 0.5116\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3443 - val_loss: 0.3300\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3411 - val_loss: 0.3819\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3409 - val_loss: 0.3963\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3405 - val_loss: 0.5423\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3407 - val_loss: 0.3983\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3391 - val_loss: 0.4771\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.2999\n",
      "[CV 4/5] END learning_rate=0.001860821195587783, n_hidden=2, n_neurons=94;, score=-0.300 total time=  33.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.5019 - val_loss: 2.8399\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6565 - val_loss: 1.1644\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5765 - val_loss: 0.5631\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5301 - val_loss: 0.5009\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4956 - val_loss: 0.4624\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4672 - val_loss: 0.4832\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4460 - val_loss: 0.4212\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4303 - val_loss: 0.4043\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4169 - val_loss: 0.3899\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4061 - val_loss: 0.3852\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3973 - val_loss: 0.4805\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3918 - val_loss: 0.3703\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3850 - val_loss: 0.3649\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3793 - val_loss: 0.4252\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3752 - val_loss: 0.3859\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3711 - val_loss: 0.3540\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3674 - val_loss: 0.4380\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3654 - val_loss: 0.4273\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3614 - val_loss: 0.4037\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3601 - val_loss: 0.4984\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3565 - val_loss: 0.3495\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3526 - val_loss: 0.3969\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3519 - val_loss: 0.3417\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3494 - val_loss: 0.3801\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3471 - val_loss: 0.3980\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3463 - val_loss: 0.4421\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3441 - val_loss: 0.3999\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3423 - val_loss: 0.3357\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3407 - val_loss: 0.3733\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3395 - val_loss: 0.3375\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3379 - val_loss: 0.3269\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3360 - val_loss: 0.3489\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3349 - val_loss: 0.3252\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3333 - val_loss: 0.3963\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3329 - val_loss: 0.4883\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3322 - val_loss: 0.5306\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3333 - val_loss: 0.4753\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3315 - val_loss: 0.7598\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3326 - val_loss: 0.5586\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3288 - val_loss: 0.8662\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3307 - val_loss: 0.3991\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3251 - val_loss: 0.3198\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3243 - val_loss: 0.3474\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3227 - val_loss: 0.4343\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3249 - val_loss: 0.3259\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3220 - val_loss: 0.3793\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3211 - val_loss: 0.3123\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3204 - val_loss: 0.3143\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3196 - val_loss: 0.3273\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3175 - val_loss: 0.4801\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3198 - val_loss: 0.4503\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3179 - val_loss: 0.5352\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3185 - val_loss: 0.3230\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3158 - val_loss: 0.3729\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3152 - val_loss: 0.3870\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3147 - val_loss: 0.4067\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3152 - val_loss: 0.3393\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3485\n",
      "[CV 5/5] END learning_rate=0.001860821195587783, n_hidden=2, n_neurons=94;, score=-0.348 total time=  33.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.8141 - val_loss: 14.9545\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6489 - val_loss: 4.5959\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5097 - val_loss: 2.2502\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.9455 - val_loss: 1.0316\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4801 - val_loss: 8.2916\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.8766 - val_loss: 1.9774\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.3464 - val_loss: 2.3221\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 4.6529 - val_loss: 13.1950\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4535 - val_loss: 5.8552\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3773 - val_loss: 8.0867\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3599 - val_loss: 4.7684\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3629 - val_loss: 9.6499\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3523 - val_loss: 7.7180\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3453 - val_loss: 7.5702\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3370\n",
      "[CV 1/5] END learning_rate=0.024970097850850106, n_hidden=1, n_neurons=86;, score=-0.337 total time=   7.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.5747 - val_loss: 0.5233\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4008 - val_loss: 0.4885\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4085 - val_loss: 2.6112\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3928 - val_loss: 15.0071\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4375 - val_loss: 0.4077\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3586 - val_loss: 0.3439\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3449 - val_loss: 0.3229\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3395 - val_loss: 0.3327\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3390 - val_loss: 0.3200\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3333 - val_loss: 0.3284\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3301 - val_loss: 0.3223\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3271 - val_loss: 0.3167\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3230 - val_loss: 0.3387\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3228 - val_loss: 0.3164\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3179 - val_loss: 0.3438\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3163 - val_loss: 0.3307\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3202 - val_loss: 0.3436\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3410 - val_loss: 0.3406\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3125 - val_loss: 0.3254\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3121 - val_loss: 0.3109\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3100 - val_loss: 0.3382\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3079 - val_loss: 0.3225\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3075 - val_loss: 0.3150\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3072 - val_loss: 0.3145\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3045 - val_loss: 0.3170\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3064 - val_loss: 0.3571\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3042 - val_loss: 0.3234\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3025 - val_loss: 0.3136\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3021 - val_loss: 0.3097\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2996 - val_loss: 0.3251\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2969 - val_loss: 0.3127\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2997 - val_loss: 0.3117\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2990 - val_loss: 0.3296\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2960 - val_loss: 0.3187\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2944 - val_loss: 0.3343\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2966 - val_loss: 0.3253\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2970 - val_loss: 0.3264\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2976 - val_loss: 0.3679\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2962 - val_loss: 0.2924\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.2969 - val_loss: 0.3599\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2931 - val_loss: 0.3908\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2961 - val_loss: 0.2994\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.2928 - val_loss: 0.4263\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2903 - val_loss: 0.3262\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2896 - val_loss: 0.4350\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2970 - val_loss: 0.2947\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2897 - val_loss: 0.3520\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2899 - val_loss: 0.3490\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2874 - val_loss: 0.3496\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.3978\n",
      "[CV 2/5] END learning_rate=0.024970097850850106, n_hidden=1, n_neurons=86;, score=-0.398 total time=  26.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 1.9436 - val_loss: 17.9506\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4277 - val_loss: 0.7880\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3837 - val_loss: 0.7282\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3662 - val_loss: 2.5480\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3596 - val_loss: 3.3594\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3509 - val_loss: 1.8976\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3493 - val_loss: 1.9408\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3421 - val_loss: 1.0972\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3382 - val_loss: 1.4389\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3360 - val_loss: 2.0218\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3321 - val_loss: 3.4484\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3282 - val_loss: 0.3448\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3273 - val_loss: 2.1375\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3247 - val_loss: 2.3630\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3213 - val_loss: 0.3341\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3191 - val_loss: 1.6046\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3176 - val_loss: 0.4540\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3159 - val_loss: 0.3335\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3160 - val_loss: 1.7923\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3168 - val_loss: 1.5081\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3113 - val_loss: 0.4117\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3081 - val_loss: 1.5243\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3088 - val_loss: 2.8074\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3086 - val_loss: 0.3417\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3092 - val_loss: 0.3278\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3029 - val_loss: 1.5054\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3034 - val_loss: 0.4950\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3024 - val_loss: 0.5314\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2987 - val_loss: 0.6745\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2984 - val_loss: 0.6089\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2975 - val_loss: 3.2887\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.2991 - val_loss: 0.4777\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2973 - val_loss: 0.6156\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.2957 - val_loss: 2.5147\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3186 - val_loss: 0.6632\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.6954\n",
      "[CV 3/5] END learning_rate=0.024970097850850106, n_hidden=1, n_neurons=86;, score=-0.695 total time=  19.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 102us/sample - loss: 0.7163 - val_loss: 15.8504\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4365 - val_loss: 59.1223\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8347 - val_loss: 0.9357\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5420 - val_loss: 115.5962\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4009 - val_loss: 0.5252\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8632 - val_loss: 163.6995\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: nan\n",
      "[CV 4/5] END learning_rate=0.024970097850850106, n_hidden=1, n_neurons=86;, score=nan total time=   8.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.7100 - val_loss: 1.7974\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4817 - val_loss: 0.4053\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4754 - val_loss: 0.8088\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4303 - val_loss: 1.6126\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4017 - val_loss: 2.6140\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4179 - val_loss: 0.3839\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4303 - val_loss: 0.3602\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3745 - val_loss: 0.3536\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3638 - val_loss: 0.4186\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3623 - val_loss: 0.3577\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3483 - val_loss: 0.3750\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3458 - val_loss: 0.3372\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3443 - val_loss: 0.3790\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3378 - val_loss: 0.3301\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3358 - val_loss: 0.3311\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3341 - val_loss: 0.3460\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3321 - val_loss: 0.3202\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3330 - val_loss: 0.3254\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3272 - val_loss: 0.3202\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3262 - val_loss: 0.3490\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3427 - val_loss: 0.3580\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3277 - val_loss: 0.3756\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3236 - val_loss: 0.3360\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3195 - val_loss: 0.4040\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3188 - val_loss: 0.3314\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3141 - val_loss: 0.3337\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3139 - val_loss: 0.3056\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3118 - val_loss: 0.3086\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3097 - val_loss: 0.4096\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3102 - val_loss: 0.3177\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3082 - val_loss: 0.3382\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3058 - val_loss: 0.4577\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3086 - val_loss: 0.3449\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3068 - val_loss: 1.2531\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3199 - val_loss: 0.3545\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3061 - val_loss: 0.4167\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3078 - val_loss: 0.4974\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3361\n",
      "[CV 5/5] END learning_rate=0.024970097850850106, n_hidden=1, n_neurons=86;, score=-0.336 total time=  20.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 1.1188 - val_loss: 4.7465\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5741 - val_loss: 0.8393\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4603 - val_loss: 0.4346\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4292 - val_loss: 0.4295\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4104 - val_loss: 0.4192\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3976 - val_loss: 0.4458\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3903 - val_loss: 0.3859\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3826 - val_loss: 0.4370\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3763 - val_loss: 0.3811\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3705 - val_loss: 0.4468\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3688 - val_loss: 0.3766\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3645 - val_loss: 0.3890\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3615 - val_loss: 0.3570\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3595 - val_loss: 0.4053\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3565 - val_loss: 0.3697\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3542 - val_loss: 0.4014\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3536 - val_loss: 0.4194\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3505 - val_loss: 0.3689\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3488 - val_loss: 0.3440\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3466 - val_loss: 0.3895\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3440 - val_loss: 0.3920\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3430 - val_loss: 0.3244\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3412 - val_loss: 0.3239\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3393 - val_loss: 0.3918\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3392 - val_loss: 0.3179\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3365 - val_loss: 0.4538\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3368 - val_loss: 0.3234\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3338 - val_loss: 0.3401\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3330 - val_loss: 0.3502\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3316 - val_loss: 0.3589\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3301 - val_loss: 0.3496\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3292 - val_loss: 0.3381\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3278 - val_loss: 0.3118\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3262 - val_loss: 0.4529\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3268 - val_loss: 0.3305\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3249 - val_loss: 0.4247\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3242 - val_loss: 0.3506\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3229 - val_loss: 0.3099\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3216 - val_loss: 0.3605\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3211 - val_loss: 0.3309\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3193 - val_loss: 0.4226\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3201 - val_loss: 0.3037\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3176 - val_loss: 0.3471\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3168 - val_loss: 0.3869\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3171 - val_loss: 0.3393\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3163 - val_loss: 0.4978\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3166 - val_loss: 0.3120\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3139 - val_loss: 0.3080\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3133 - val_loss: 0.3172\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3120 - val_loss: 0.3059\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3114 - val_loss: 0.3166\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3104 - val_loss: 0.3963\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3043\n",
      "[CV 1/5] END learning_rate=0.004249039768528955, n_hidden=2, n_neurons=43;, score=-0.304 total time=  30.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 1.1221 - val_loss: 20.2062\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6594 - val_loss: 2.2797\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4882 - val_loss: 0.4665\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4313 - val_loss: 0.4152\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4073 - val_loss: 0.3985\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3915 - val_loss: 0.3865\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3790 - val_loss: 0.3771\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3699 - val_loss: 0.3739\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3632 - val_loss: 0.3640\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3567 - val_loss: 0.3546\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3521 - val_loss: 0.3507\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3482 - val_loss: 0.3527\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3441 - val_loss: 0.3552\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3411 - val_loss: 0.3459\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3387 - val_loss: 0.3517\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3363 - val_loss: 0.3329\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3343 - val_loss: 0.3457\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3321 - val_loss: 0.3510\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3297 - val_loss: 0.3375\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3282 - val_loss: 0.3458\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3258 - val_loss: 0.3410\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3245 - val_loss: 0.3271\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3233 - val_loss: 0.3310\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3219 - val_loss: 0.3262\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3205 - val_loss: 0.3368\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3193 - val_loss: 0.3226\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3174 - val_loss: 0.3397\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3164 - val_loss: 0.3162\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3150 - val_loss: 0.3272\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3141 - val_loss: 0.3207\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3128 - val_loss: 0.3298\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3120 - val_loss: 0.3127\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3107 - val_loss: 0.3269\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3105 - val_loss: 0.3110\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3096 - val_loss: 0.3418\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3077 - val_loss: 0.3151\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3072 - val_loss: 0.3187\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3063 - val_loss: 0.3119\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3059 - val_loss: 0.3262\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3051 - val_loss: 0.3175\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3041 - val_loss: 0.3425\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3037 - val_loss: 0.3499\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3030 - val_loss: 0.4232\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3025 - val_loss: 0.3920\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3685\n",
      "[CV 2/5] END learning_rate=0.004249039768528955, n_hidden=2, n_neurons=43;, score=-0.369 total time=  26.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.1605 - val_loss: 4.6555\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5904 - val_loss: 0.6668\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4844 - val_loss: 0.4581\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4347 - val_loss: 0.4767\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4099 - val_loss: 0.3847\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3969 - val_loss: 0.3736\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3876 - val_loss: 0.4032\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3765 - val_loss: 0.5000\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3729 - val_loss: 0.6229\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3654 - val_loss: 0.8370\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3651 - val_loss: 0.9032\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3573 - val_loss: 0.8707\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3567 - val_loss: 1.0002\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3547 - val_loss: 0.9937\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3518 - val_loss: 1.0380\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3482 - val_loss: 1.0168\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3978\n",
      "[CV 3/5] END learning_rate=0.004249039768528955, n_hidden=2, n_neurons=43;, score=-0.398 total time=   9.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 1.0488 - val_loss: 0.8633\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5833 - val_loss: 0.7934\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4916 - val_loss: 0.4349\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4454 - val_loss: 0.3933\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4209 - val_loss: 0.5722\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4099 - val_loss: 0.4281\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3956 - val_loss: 0.4518\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3876 - val_loss: 0.4147\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3816 - val_loss: 0.3694\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3752 - val_loss: 0.3551\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3708 - val_loss: 0.3814\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3669 - val_loss: 0.5005\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3636 - val_loss: 0.3479\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3589 - val_loss: 0.4893\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3595 - val_loss: 0.3433\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3543 - val_loss: 0.3312\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3515 - val_loss: 0.3969\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3508 - val_loss: 0.3431\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3473 - val_loss: 0.3255\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3454 - val_loss: 0.3315\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3440 - val_loss: 0.3730\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3420 - val_loss: 0.4280\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3435 - val_loss: 0.3944\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3411 - val_loss: 0.3577\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3384 - val_loss: 0.3401\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3369 - val_loss: 0.3189\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3347 - val_loss: 0.3287\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3335 - val_loss: 0.6828\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3423 - val_loss: 0.3908\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3334 - val_loss: 0.3441\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3326 - val_loss: 0.3329\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3322 - val_loss: 0.3224\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3296 - val_loss: 0.3692\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3293 - val_loss: 0.3141\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3308 - val_loss: 0.4596\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3285 - val_loss: 0.7503\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3274 - val_loss: 0.7816\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3265 - val_loss: 0.7671\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3293 - val_loss: 0.4212\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3225 - val_loss: 0.3075\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3219 - val_loss: 0.4449\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3224 - val_loss: 0.6550\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3222 - val_loss: 0.7521\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3219 - val_loss: 0.8526\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3234 - val_loss: 0.6848\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3222 - val_loss: 1.1419\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3311 - val_loss: 1.1245\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3244 - val_loss: 1.1228\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3248 - val_loss: 0.3984\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3166 - val_loss: 0.3665\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.2851\n",
      "[CV 4/5] END learning_rate=0.004249039768528955, n_hidden=2, n_neurons=43;, score=-0.285 total time=  29.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 107us/sample - loss: 1.1320 - val_loss: 3.5076\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6204 - val_loss: 11.8666\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6063 - val_loss: 5.6058\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5134 - val_loss: 0.4937\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4353 - val_loss: 0.4711\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4134 - val_loss: 0.4381\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3998 - val_loss: 0.4211\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3891 - val_loss: 0.4000\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3827 - val_loss: 0.4428\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3757 - val_loss: 0.4258\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3713 - val_loss: 0.3929\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3662 - val_loss: 0.3959\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3630 - val_loss: 0.4362\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3603 - val_loss: 0.4089\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3575 - val_loss: 0.3885\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3540 - val_loss: 0.3973\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3515 - val_loss: 0.4012\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3513 - val_loss: 0.3370\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3471 - val_loss: 0.3902\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3476 - val_loss: 0.3504\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3436 - val_loss: 0.3819\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3416 - val_loss: 0.3551\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3454 - val_loss: 0.3322\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3375 - val_loss: 0.3829\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3367 - val_loss: 0.3462\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3353 - val_loss: 0.3618\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3348 - val_loss: 0.3289\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3357 - val_loss: 0.3607\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3318 - val_loss: 0.3317\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3296 - val_loss: 0.3842\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3287 - val_loss: 0.3547\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3293 - val_loss: 0.3590\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3273 - val_loss: 0.3158\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3242 - val_loss: 0.3936\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3238 - val_loss: 0.3227\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3231 - val_loss: 0.3421\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3208 - val_loss: 0.3735\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3213 - val_loss: 0.3182\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3195 - val_loss: 0.3708\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3184 - val_loss: 0.3213\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3181 - val_loss: 0.3573\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3175 - val_loss: 0.3132\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3175 - val_loss: 0.3218\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3156 - val_loss: 0.3169\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3137 - val_loss: 0.3508\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3134 - val_loss: 0.3248\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3126 - val_loss: 0.3579\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3121 - val_loss: 0.3099\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3117 - val_loss: 0.3190\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3105 - val_loss: 0.3077\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3091 - val_loss: 0.3284\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3086 - val_loss: 0.3069\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3071 - val_loss: 0.3819\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3084 - val_loss: 0.3098\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3086 - val_loss: 0.3885\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3065 - val_loss: 0.3094\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3054 - val_loss: 0.3065\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3038 - val_loss: 0.3086\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3035 - val_loss: 0.3482\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3039 - val_loss: 0.3094\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3030 - val_loss: 0.3017\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3016 - val_loss: 0.3001\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3014 - val_loss: 0.4067\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3005 - val_loss: 0.3324\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3011 - val_loss: 0.3522\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2999 - val_loss: 0.2984\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2991 - val_loss: 0.2976\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2972 - val_loss: 0.3728\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2976 - val_loss: 0.2988\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2961 - val_loss: 0.2994\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2967 - val_loss: 0.3434\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2968 - val_loss: 0.3742\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2961 - val_loss: 0.6463\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2999 - val_loss: 0.3654\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2966 - val_loss: 0.4834\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2949 - val_loss: 0.3235\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2934 - val_loss: 0.3353\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3362\n",
      "[CV 5/5] END learning_rate=0.004249039768528955, n_hidden=2, n_neurons=43;, score=-0.336 total time=  45.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 1.1805 - val_loss: 6.2087\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6669 - val_loss: 1.8168\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5278 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4606 - val_loss: 0.4191\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4308 - val_loss: 0.4136\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4126 - val_loss: 0.4395\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4013 - val_loss: 0.4793\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3920 - val_loss: 0.3997\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3860 - val_loss: 0.3781\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3803 - val_loss: 0.3625\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3765 - val_loss: 0.4142\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3729 - val_loss: 0.3591\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3711 - val_loss: 0.3873\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3672 - val_loss: 0.3418\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3639 - val_loss: 0.3984\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3619 - val_loss: 0.4115\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3599 - val_loss: 0.3420\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3557 - val_loss: 0.3901\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3543 - val_loss: 0.3352\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3524 - val_loss: 0.3615\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3504 - val_loss: 0.4478\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3509 - val_loss: 0.3369\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3467 - val_loss: 0.3427\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3453 - val_loss: 0.3784\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3449 - val_loss: 0.3530\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3436 - val_loss: 0.3384\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3422 - val_loss: 0.4217\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3408 - val_loss: 0.4232\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3405 - val_loss: 0.3422\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3278\n",
      "[CV 1/5] END learning_rate=0.0031087150872009988, n_hidden=2, n_neurons=80;, score=-0.328 total time=  17.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.2117 - val_loss: 8.7752\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6535 - val_loss: 2.4205\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5067 - val_loss: 0.5047\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4493 - val_loss: 0.4350\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4243 - val_loss: 0.4174\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4088 - val_loss: 0.4019\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3973 - val_loss: 0.3891\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3901 - val_loss: 0.3924\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3832 - val_loss: 0.3828\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3778 - val_loss: 0.3832\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3732 - val_loss: 0.3884\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3693 - val_loss: 0.3852\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3649 - val_loss: 0.3652\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3611 - val_loss: 0.3795\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3588 - val_loss: 0.3696\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3555 - val_loss: 0.3726\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3533 - val_loss: 0.3751\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3508 - val_loss: 0.3798\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3490 - val_loss: 0.3579\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3470 - val_loss: 0.3468\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3453 - val_loss: 0.3711\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3430 - val_loss: 0.3763\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3416 - val_loss: 0.3720\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3400 - val_loss: 0.3411\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3385 - val_loss: 0.3469\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3373 - val_loss: 0.3534\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3359 - val_loss: 0.3434\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3345 - val_loss: 0.3399\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3337 - val_loss: 0.3538\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3322 - val_loss: 0.3398\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3312 - val_loss: 0.3429\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3298 - val_loss: 0.3545\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3291 - val_loss: 0.3462\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3278 - val_loss: 0.3358\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3272 - val_loss: 0.3466\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3261 - val_loss: 0.3365\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3253 - val_loss: 0.3279\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3238 - val_loss: 0.3430\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3233 - val_loss: 0.3415\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3221 - val_loss: 0.3282\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3214 - val_loss: 0.3337\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3205 - val_loss: 0.3269\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3197 - val_loss: 0.3334\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3192 - val_loss: 0.3195\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3183 - val_loss: 0.3253\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3168 - val_loss: 0.3268\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3165 - val_loss: 0.3199\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3154 - val_loss: 0.3322\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3144 - val_loss: 0.3175\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3144 - val_loss: 0.3181\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3137 - val_loss: 0.3150\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3124 - val_loss: 0.3294\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3120 - val_loss: 0.3224\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3113 - val_loss: 0.3250\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3101 - val_loss: 0.3127\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3101 - val_loss: 0.3190\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3095 - val_loss: 0.3208\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3088 - val_loss: 0.3149\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3075 - val_loss: 0.3176\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3070 - val_loss: 0.3164\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3061 - val_loss: 0.3135\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3056 - val_loss: 0.3175\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3052 - val_loss: 0.3096\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3042 - val_loss: 0.3088\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3040 - val_loss: 0.3104\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3033 - val_loss: 0.3071\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3022 - val_loss: 0.3110\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3013 - val_loss: 0.3033\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3014 - val_loss: 0.3063\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3002 - val_loss: 0.3036\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3001 - val_loss: 0.3056\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2991 - val_loss: 0.3038\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2989 - val_loss: 0.3062\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2980 - val_loss: 0.3269\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2979 - val_loss: 0.2998\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2972 - val_loss: 0.3099\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2960 - val_loss: 0.2999\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2959 - val_loss: 0.3146\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2955 - val_loss: 0.3024\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2953 - val_loss: 0.3258\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2935 - val_loss: 0.3152\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2937 - val_loss: 0.3451\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2931 - val_loss: 0.4065\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2940 - val_loss: 0.6579\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2947 - val_loss: 0.8697\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3568\n",
      "[CV 2/5] END learning_rate=0.0031087150872009988, n_hidden=2, n_neurons=80;, score=-0.357 total time=  50.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 1.2245 - val_loss: 1.0096\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5754 - val_loss: 0.6358\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4976 - val_loss: 0.9641\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4540 - val_loss: 0.9483\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4301 - val_loss: 0.6682\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4127 - val_loss: 0.5218\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4036 - val_loss: 0.3816\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3929 - val_loss: 0.3893\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3839 - val_loss: 0.4693\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3770 - val_loss: 0.5550\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3717 - val_loss: 0.7130\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3670 - val_loss: 0.8422\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3642 - val_loss: 0.9069\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3596 - val_loss: 1.0266\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3565 - val_loss: 1.1722\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3541 - val_loss: 1.3677\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3522 - val_loss: 1.2633\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.4028\n",
      "[CV 3/5] END learning_rate=0.0031087150872009988, n_hidden=2, n_neurons=80;, score=-0.403 total time=  10.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 1.2315 - val_loss: 32.9581\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 1.0378 - val_loss: 4.8847\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5822 - val_loss: 0.5279\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4872 - val_loss: 0.4647\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4536 - val_loss: 0.4211\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4331 - val_loss: 0.4203\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4188 - val_loss: 0.4057\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4087 - val_loss: 0.3970\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4011 - val_loss: 0.3990\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3951 - val_loss: 0.3975\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3907 - val_loss: 0.3791\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3856 - val_loss: 0.4001\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3818 - val_loss: 0.4007\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3792 - val_loss: 0.3620\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3760 - val_loss: 0.3731\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3731 - val_loss: 0.3782\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3708 - val_loss: 0.3560\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3689 - val_loss: 0.3462\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3668 - val_loss: 0.3631\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3647 - val_loss: 0.3401\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3625 - val_loss: 0.3595\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3610 - val_loss: 0.3452\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3594 - val_loss: 0.3517\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3580 - val_loss: 0.3340\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3560 - val_loss: 0.3373\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3553 - val_loss: 0.3308\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3535 - val_loss: 0.3315\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3523 - val_loss: 0.3663\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3514 - val_loss: 0.3652\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3493 - val_loss: 0.3260\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3479 - val_loss: 0.3430\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3474 - val_loss: 0.3409\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3455 - val_loss: 0.3222\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3448 - val_loss: 0.3506\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3437 - val_loss: 0.3231\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3432 - val_loss: 0.3306\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3411 - val_loss: 0.3345\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3402 - val_loss: 0.3522\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3398 - val_loss: 0.3196\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3384 - val_loss: 0.3550\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3377 - val_loss: 0.3202\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3367 - val_loss: 0.3625\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3359 - val_loss: 0.3490\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3346 - val_loss: 0.3150\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3336 - val_loss: 0.3174\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3334 - val_loss: 0.3157\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3320 - val_loss: 0.3414\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3310 - val_loss: 0.3508\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3308 - val_loss: 0.3142\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3288 - val_loss: 0.3204\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3290 - val_loss: 0.3126\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3285 - val_loss: 0.3226\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3273 - val_loss: 0.3433\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3265 - val_loss: 0.3206\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3244 - val_loss: 0.3082\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3252 - val_loss: 0.3387\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3240 - val_loss: 0.3346\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3235 - val_loss: 0.3067\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3221 - val_loss: 0.3439\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3219 - val_loss: 0.3308\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3214 - val_loss: 0.3067\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3200 - val_loss: 0.3083\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3200 - val_loss: 0.3143\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3191 - val_loss: 0.3048\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3178 - val_loss: 0.3384\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3179 - val_loss: 0.3645\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3171 - val_loss: 0.3049\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3164 - val_loss: 0.3095\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3162 - val_loss: 0.3172\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3143 - val_loss: 0.3171\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3144 - val_loss: 0.3206\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3130 - val_loss: 0.2987\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3121 - val_loss: 0.2999\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3116 - val_loss: 0.3060\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3111 - val_loss: 0.3207\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3099 - val_loss: 0.3070\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3088 - val_loss: 0.2980\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3089 - val_loss: 0.3010\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3076 - val_loss: 0.3137\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3075 - val_loss: 0.3049\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3065 - val_loss: 0.3022\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3062 - val_loss: 0.3031\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3053 - val_loss: 0.2953\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3053 - val_loss: 0.3072\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3035 - val_loss: 0.3108\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3036 - val_loss: 0.3067\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3029 - val_loss: 0.2967\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3029 - val_loss: 0.2944\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3021 - val_loss: 0.3117\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3009 - val_loss: 0.3005\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3012 - val_loss: 0.3081\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3002 - val_loss: 0.2969\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3001 - val_loss: 0.3096\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2992 - val_loss: 0.2923\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2983 - val_loss: 0.2921\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2977 - val_loss: 0.2985\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2975 - val_loss: 0.2905\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2967 - val_loss: 0.3019\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2966 - val_loss: 0.2951\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2956 - val_loss: 0.3239\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.2763\n",
      "[CV 4/5] END learning_rate=0.0031087150872009988, n_hidden=2, n_neurons=80;, score=-0.276 total time=  59.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.2100 - val_loss: 10.7056\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6539 - val_loss: 1.6238\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5004 - val_loss: 0.5424\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4455 - val_loss: 0.4354\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4152 - val_loss: 0.4188\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3976 - val_loss: 0.4193\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3857 - val_loss: 0.4419\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3786 - val_loss: 0.4342\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3718 - val_loss: 0.4096\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3664 - val_loss: 0.4378\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3623 - val_loss: 0.3867\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3583 - val_loss: 0.4246\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3547 - val_loss: 0.3959\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3515 - val_loss: 0.3994\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3483 - val_loss: 0.4216\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3459 - val_loss: 0.3670\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3437 - val_loss: 0.4071\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3419 - val_loss: 0.3553\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3396 - val_loss: 0.3798\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3374 - val_loss: 0.4293\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3365 - val_loss: 0.3841\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3339 - val_loss: 0.3457\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3327 - val_loss: 0.3935\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3308 - val_loss: 0.3880\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3299 - val_loss: 0.3326\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3283 - val_loss: 0.3961\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3268 - val_loss: 0.4218\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3258 - val_loss: 0.3293\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3245 - val_loss: 0.3518\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3231 - val_loss: 0.3952\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3226 - val_loss: 0.3368\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3211 - val_loss: 0.3387\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3202 - val_loss: 0.3999\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3191 - val_loss: 0.3418\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3179 - val_loss: 0.3343\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3167 - val_loss: 0.3892\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3162 - val_loss: 0.3805\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3149 - val_loss: 0.3131\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3141 - val_loss: 0.3925\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3129 - val_loss: 0.3613\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3127 - val_loss: 0.3625\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3113 - val_loss: 0.3150\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3104 - val_loss: 0.3334\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3099 - val_loss: 0.3904\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3091 - val_loss: 0.3074\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3080 - val_loss: 0.3694\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3067 - val_loss: 0.3467\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3056 - val_loss: 0.3053\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3058 - val_loss: 0.3353\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3047 - val_loss: 0.3656\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3045 - val_loss: 0.3476\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3033 - val_loss: 0.3016\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3031 - val_loss: 0.4072\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3027 - val_loss: 0.3021\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3015 - val_loss: 0.3659\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3012 - val_loss: 0.3000\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2999 - val_loss: 0.4359\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3004 - val_loss: 0.3063\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3000 - val_loss: 0.3035\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2980 - val_loss: 0.3959\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3006 - val_loss: 0.3001\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2972 - val_loss: 0.5487\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2988 - val_loss: 0.3050\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2970 - val_loss: 0.4511\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2959 - val_loss: 0.3152\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2943 - val_loss: 0.3500\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3343\n",
      "[CV 5/5] END learning_rate=0.0031087150872009988, n_hidden=2, n_neurons=80;, score=-0.334 total time=  39.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 3.9427 - val_loss: 2.2080\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.4636 - val_loss: 1.1089\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.8713 - val_loss: 0.9360\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7087 - val_loss: 1.0036\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6591 - val_loss: 0.8844\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6372 - val_loss: 0.7706\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6227 - val_loss: 0.9318\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6130 - val_loss: 1.0778\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6063 - val_loss: 1.0676\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6004 - val_loss: 0.7691\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5922 - val_loss: 0.8488\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5876 - val_loss: 0.6850\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5816 - val_loss: 0.6929\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5753 - val_loss: 0.9016\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5735 - val_loss: 0.8089\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5692 - val_loss: 0.7829\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5663 - val_loss: 0.6778\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5627 - val_loss: 0.6814\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5600 - val_loss: 0.6537\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5551 - val_loss: 0.8948\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5546 - val_loss: 0.9513\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5528 - val_loss: 0.9741\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5519 - val_loss: 0.8837\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5506 - val_loss: 0.7015\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5482 - val_loss: 0.5903\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5467 - val_loss: 0.6095\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5453 - val_loss: 0.6077\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5441 - val_loss: 0.5926\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5411 - val_loss: 0.8557\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5418 - val_loss: 0.9171\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5427 - val_loss: 0.7739\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5414 - val_loss: 0.6533\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5404 - val_loss: 0.6199\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5379 - val_loss: 0.8024\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5398 - val_loss: 0.6014\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.5075\n",
      "[CV 1/5] END learning_rate=0.0011672126382307405, n_hidden=0, n_neurons=34;, score=-0.508 total time=  18.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 96us/sample - loss: 4.3833 - val_loss: 4.6901\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.5056 - val_loss: 1.0391\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9260 - val_loss: 0.9477\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7708 - val_loss: 1.1464\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7194 - val_loss: 1.0928\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6915 - val_loss: 1.2063\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6732 - val_loss: 1.3067\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6598 - val_loss: 1.2310\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6469 - val_loss: 1.1116\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6357 - val_loss: 0.9075\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6237 - val_loss: 0.9622\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6153 - val_loss: 0.8571\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6065 - val_loss: 0.7331\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5976 - val_loss: 0.8663\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5914 - val_loss: 0.8439\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5851 - val_loss: 0.8177\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5793 - val_loss: 0.8001\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5725 - val_loss: 0.9835\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5706 - val_loss: 0.7458\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5633 - val_loss: 0.9742\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5624 - val_loss: 0.7203\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5580 - val_loss: 0.6659\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5529 - val_loss: 0.8948\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5516 - val_loss: 0.8998\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5487 - val_loss: 0.9471\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5480 - val_loss: 0.6727\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5429 - val_loss: 0.8880\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5432 - val_loss: 0.7966\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5415 - val_loss: 0.6297\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5377 - val_loss: 0.8500\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5387 - val_loss: 0.6590\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5346 - val_loss: 0.9024\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5345 - val_loss: 1.0128\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5349 - val_loss: 0.9384\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5347 - val_loss: 0.6670\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5310 - val_loss: 0.8340\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5324 - val_loss: 0.6953\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5296 - val_loss: 0.8433\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5300 - val_loss: 0.8676\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.5676\n",
      "[CV 2/5] END learning_rate=0.0011672126382307405, n_hidden=0, n_neurons=34;, score=-0.568 total time=  20.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 5.3262 - val_loss: 6.3383\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.7114 - val_loss: 6.1295\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8800 - val_loss: 6.9114\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6614 - val_loss: 7.9095\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5955 - val_loss: 8.8637\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5712 - val_loss: 9.7643\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5590 - val_loss: 10.5940\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5511 - val_loss: 11.4329\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5450 - val_loss: 12.0797\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5400 - val_loss: 12.7258\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5354 - val_loss: 13.2787\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5316 - val_loss: 13.7763\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.0599\n",
      "[CV 3/5] END learning_rate=0.0011672126382307405, n_hidden=0, n_neurons=34;, score=-1.060 total time=   6.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 4.5850 - val_loss: 4.5437\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.5766 - val_loss: 1.0680\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9781 - val_loss: 0.9758\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8188 - val_loss: 1.2233\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7663 - val_loss: 1.2737\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7386 - val_loss: 1.2294\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7213 - val_loss: 1.0466\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7037 - val_loss: 1.0126\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6889 - val_loss: 1.0653\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6775 - val_loss: 0.8018\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6632 - val_loss: 0.9923\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6551 - val_loss: 0.8336\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6447 - val_loss: 0.8473\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6346 - val_loss: 1.0416\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6291 - val_loss: 0.8440\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6200 - val_loss: 0.8258\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6145 - val_loss: 0.7544\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6083 - val_loss: 0.6645\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6026 - val_loss: 0.7029\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5963 - val_loss: 0.8452\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5928 - val_loss: 0.8658\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5895 - val_loss: 0.7088\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5833 - val_loss: 0.8966\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5808 - val_loss: 0.9135\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5793 - val_loss: 0.7389\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5747 - val_loss: 0.7823\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5729 - val_loss: 0.7409\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5703 - val_loss: 0.7367\n",
      "2322/2322 [==============================] - 0s 27us/sample - loss: 0.4917\n",
      "[CV 4/5] END learning_rate=0.0011672126382307405, n_hidden=0, n_neurons=34;, score=-0.492 total time=  14.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 4.2108 - val_loss: 3.6539\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.5238 - val_loss: 1.2448\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9627 - val_loss: 1.1540\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8053 - val_loss: 1.1716\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7481 - val_loss: 1.2283\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7199 - val_loss: 0.8950\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6962 - val_loss: 1.1586\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6811 - val_loss: 1.1368\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6669 - val_loss: 0.9664\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6522 - val_loss: 1.0104\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6402 - val_loss: 1.0192\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6299 - val_loss: 1.0124\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6190 - val_loss: 1.1286\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6123 - val_loss: 0.7896\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6010 - val_loss: 0.9917\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5965 - val_loss: 0.7661\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5884 - val_loss: 0.8705\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5830 - val_loss: 0.7966\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5768 - val_loss: 0.8624\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5730 - val_loss: 0.6834\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5670 - val_loss: 0.8686\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5644 - val_loss: 0.7641\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5602 - val_loss: 0.7525\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5567 - val_loss: 0.6557\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5535 - val_loss: 0.6089\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5505 - val_loss: 0.6639\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5465 - val_loss: 0.8756\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5466 - val_loss: 0.8837\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5446 - val_loss: 0.7591\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5413 - val_loss: 0.8514\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5411 - val_loss: 0.6904\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5387 - val_loss: 0.6547\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5369 - val_loss: 0.6380\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5352 - val_loss: 0.7545\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5348 - val_loss: 0.6852\n",
      "2322/2322 [==============================] - 0s 27us/sample - loss: 0.5740\n",
      "[CV 5/5] END learning_rate=0.0011672126382307405, n_hidden=0, n_neurons=34;, score=-0.574 total time=  18.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 2.0985 - val_loss: 3.0210\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.8449 - val_loss: 1.4075\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7272 - val_loss: 0.6665\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6627 - val_loss: 0.5958\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6139 - val_loss: 0.5850\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5729 - val_loss: 0.5195\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5398 - val_loss: 0.5028\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5139 - val_loss: 0.5052\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4928 - val_loss: 0.4626\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4763 - val_loss: 0.4353\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4648 - val_loss: 0.4234\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4556 - val_loss: 0.4357\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4489 - val_loss: 0.4079\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4435 - val_loss: 0.4180\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4391 - val_loss: 0.4039\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4359 - val_loss: 0.4225\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4330 - val_loss: 0.3928\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4301 - val_loss: 0.4060\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4282 - val_loss: 0.3888\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4256 - val_loss: 0.4463\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4245 - val_loss: 0.3872\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4226 - val_loss: 0.4362\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4209 - val_loss: 0.3830\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4190 - val_loss: 0.3804\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4176 - val_loss: 0.3793\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4161 - val_loss: 0.4097\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4149 - val_loss: 0.3964\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4140 - val_loss: 0.3753\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4124 - val_loss: 0.3768\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4112 - val_loss: 0.3875\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4097 - val_loss: 0.3765\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4086 - val_loss: 0.3950\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4075 - val_loss: 0.3701\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4063 - val_loss: 0.4015\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4056 - val_loss: 0.3823\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4041 - val_loss: 0.3725\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4027 - val_loss: 0.3721\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4012 - val_loss: 0.4886\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4014 - val_loss: 0.3640\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3990 - val_loss: 0.3661\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3981 - val_loss: 0.3637\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3964 - val_loss: 0.4180\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3966 - val_loss: 0.3646\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3947 - val_loss: 0.3672\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3936 - val_loss: 0.4720\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3938 - val_loss: 0.3580\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3918 - val_loss: 0.3586\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3910 - val_loss: 0.3856\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3899 - val_loss: 0.3939\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3900 - val_loss: 0.4983\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3893 - val_loss: 0.4531\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3890 - val_loss: 0.3535\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3861 - val_loss: 0.4482\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3876 - val_loss: 0.3590\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3858 - val_loss: 0.3814\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3852 - val_loss: 0.3518\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3837 - val_loss: 0.3822\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3837 - val_loss: 0.3802\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3834 - val_loss: 0.3762\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3824 - val_loss: 0.3517\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3818 - val_loss: 0.3594\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3815 - val_loss: 0.3569\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3806 - val_loss: 0.3507\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3804 - val_loss: 0.4041\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3793 - val_loss: 0.5165\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3812 - val_loss: 0.3772\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3786 - val_loss: 0.3477\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3783 - val_loss: 0.3642\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3772 - val_loss: 0.4866\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3793 - val_loss: 0.3757\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3768 - val_loss: 0.3612\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3768 - val_loss: 0.3456\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3757 - val_loss: 0.3704\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3763 - val_loss: 0.5617\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3759 - val_loss: 0.4735\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3767 - val_loss: 0.5339\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3754 - val_loss: 0.7552\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3795 - val_loss: 1.1524\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3772 - val_loss: 0.6522\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3774 - val_loss: 0.9517\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3765 - val_loss: 0.8597\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3804 - val_loss: 0.7044\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3550\n",
      "[CV 1/5] END learning_rate=0.001668105735917552, n_hidden=1, n_neurons=12;, score=-0.355 total time=  44.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 3.3581 - val_loss: 1.7353\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 1.1874 - val_loss: 1.0435\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.8240 - val_loss: 0.7906\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6587 - val_loss: 0.6120\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5897 - val_loss: 0.5781\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5566 - val_loss: 0.5260\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5346 - val_loss: 0.5187\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5166 - val_loss: 0.4951\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5008 - val_loss: 0.4770\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4867 - val_loss: 0.5046\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4752 - val_loss: 0.4425\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4640 - val_loss: 0.4429\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4542 - val_loss: 0.4267\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4459 - val_loss: 0.4247\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4385 - val_loss: 0.4173\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4323 - val_loss: 0.4044\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4265 - val_loss: 0.4057\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4217 - val_loss: 0.3985\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4175 - val_loss: 0.3934\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4137 - val_loss: 0.3892\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4102 - val_loss: 0.3876\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4073 - val_loss: 0.3833\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4044 - val_loss: 0.3803\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4019 - val_loss: 0.3784\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3993 - val_loss: 0.3778\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3974 - val_loss: 0.3746\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3954 - val_loss: 0.3723\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3934 - val_loss: 0.3715\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3919 - val_loss: 0.3694\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3903 - val_loss: 0.3677\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3889 - val_loss: 0.3698\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3877 - val_loss: 0.3690\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3865 - val_loss: 0.3669\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3854 - val_loss: 0.3649\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3844 - val_loss: 0.3668\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3834 - val_loss: 0.3629\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3824 - val_loss: 0.3678\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3816 - val_loss: 0.3620\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3807 - val_loss: 0.3630\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3799 - val_loss: 0.3654\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3794 - val_loss: 0.3628\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3786 - val_loss: 0.3648\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3778 - val_loss: 0.3615\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3774 - val_loss: 0.3641\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3768 - val_loss: 0.3617\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3761 - val_loss: 0.3594\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3757 - val_loss: 0.3653\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3754 - val_loss: 0.3652\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3748 - val_loss: 0.3611\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3743 - val_loss: 0.3621\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3738 - val_loss: 0.3604\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3735 - val_loss: 0.3637\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3731 - val_loss: 0.3586\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3728 - val_loss: 0.3586\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3724 - val_loss: 0.3566\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3722 - val_loss: 0.3574\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3718 - val_loss: 0.3640\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3715 - val_loss: 0.3555\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3711 - val_loss: 0.3623\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3708 - val_loss: 0.3568\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3705 - val_loss: 0.3615\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3703 - val_loss: 0.3567\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3699 - val_loss: 0.3594\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3696 - val_loss: 0.3552\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3694 - val_loss: 0.3566\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3690 - val_loss: 0.3577\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3688 - val_loss: 0.3570\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3685 - val_loss: 0.3587\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3683 - val_loss: 0.3533\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3680 - val_loss: 0.3614\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3677 - val_loss: 0.3542\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3675 - val_loss: 0.3530\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3672 - val_loss: 0.3519\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3669 - val_loss: 0.3604\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3666 - val_loss: 0.3577\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3665 - val_loss: 0.3503\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3662 - val_loss: 0.3554\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3659 - val_loss: 0.3547\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3657 - val_loss: 0.3508\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3655 - val_loss: 0.3563\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3653 - val_loss: 0.3527\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3652 - val_loss: 0.3497\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3648 - val_loss: 0.3592\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3647 - val_loss: 0.3552\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3644 - val_loss: 0.3515\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3641 - val_loss: 0.3485\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3639 - val_loss: 0.3552\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3637 - val_loss: 0.3546\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3633 - val_loss: 0.3525\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3633 - val_loss: 0.3505\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3629 - val_loss: 0.3531\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3627 - val_loss: 0.3603\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3624 - val_loss: 0.3480\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3623 - val_loss: 0.3494\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3621 - val_loss: 0.3535\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3619 - val_loss: 0.3512\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3617 - val_loss: 0.3562\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3615 - val_loss: 0.3478\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3611 - val_loss: 0.3494\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3610 - val_loss: 0.3472\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.4123\n",
      "[CV 2/5] END learning_rate=0.001668105735917552, n_hidden=1, n_neurons=12;, score=-0.412 total time=  53.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 1.9686 - val_loss: 1.7104\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.8067 - val_loss: 0.8111\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.7075 - val_loss: 0.6567\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6416 - val_loss: 0.7960\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5913 - val_loss: 1.0735\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5524 - val_loss: 1.2737\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5226 - val_loss: 1.3581\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4999 - val_loss: 1.4615\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4826 - val_loss: 1.3969\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4694 - val_loss: 1.2600\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4591 - val_loss: 1.0950\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4506 - val_loss: 0.9294\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4437 - val_loss: 0.8204\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.4799\n",
      "[CV 3/5] END learning_rate=0.001668105735917552, n_hidden=1, n_neurons=12;, score=-0.480 total time=   7.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 101us/sample - loss: 1.8383 - val_loss: 4.8413\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8875 - val_loss: 0.9814\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.7405 - val_loss: 0.7832\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6788 - val_loss: 0.6645\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6343 - val_loss: 0.6053\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5976 - val_loss: 0.5756\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5690 - val_loss: 0.5150\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5464 - val_loss: 0.4938\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5270 - val_loss: 0.4773\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5125 - val_loss: 0.4582\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5003 - val_loss: 0.4463\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4907 - val_loss: 0.4367\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4829 - val_loss: 0.4293\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4768 - val_loss: 0.4233\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4716 - val_loss: 0.4201\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4671 - val_loss: 0.4157\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4636 - val_loss: 0.4167\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4607 - val_loss: 0.4097\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4580 - val_loss: 0.4095\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4555 - val_loss: 0.4102\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4533 - val_loss: 0.4078\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4514 - val_loss: 0.4039\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4493 - val_loss: 0.4067\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4478 - val_loss: 0.4040\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4463 - val_loss: 0.3957\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4447 - val_loss: 0.3949\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4434 - val_loss: 0.3942\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4419 - val_loss: 0.3950\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4406 - val_loss: 0.3966\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4394 - val_loss: 0.3930\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4384 - val_loss: 0.3892\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4370 - val_loss: 0.3962\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4361 - val_loss: 0.3956\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4349 - val_loss: 0.3871\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4335 - val_loss: 0.3923\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4330 - val_loss: 0.3939\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4316 - val_loss: 0.3861\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4302 - val_loss: 0.3846\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4289 - val_loss: 0.3829\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4281 - val_loss: 0.3858\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4272 - val_loss: 0.3875\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4262 - val_loss: 0.3922\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4249 - val_loss: 0.3850\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4244 - val_loss: 0.3882\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4234 - val_loss: 0.3779\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4224 - val_loss: 0.3807\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4214 - val_loss: 0.3750\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4207 - val_loss: 0.3748\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4196 - val_loss: 0.3737\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4187 - val_loss: 0.3747\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4180 - val_loss: 0.3729\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4168 - val_loss: 0.3717\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4161 - val_loss: 0.3750\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4154 - val_loss: 0.3752\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4145 - val_loss: 0.3701\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4139 - val_loss: 0.3731\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4132 - val_loss: 0.3704\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4121 - val_loss: 0.3761\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4114 - val_loss: 0.3725\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4108 - val_loss: 0.3671\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4100 - val_loss: 0.3669\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4089 - val_loss: 0.3674\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4084 - val_loss: 0.3676\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4079 - val_loss: 0.3648\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4070 - val_loss: 0.3651\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4061 - val_loss: 0.3643\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4053 - val_loss: 0.3673\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4048 - val_loss: 0.3632\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4040 - val_loss: 0.3624\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4033 - val_loss: 0.3612\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4031 - val_loss: 0.3647\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4019 - val_loss: 0.3622\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4014 - val_loss: 0.3596\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4005 - val_loss: 0.3589\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3998 - val_loss: 0.3587\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3994 - val_loss: 0.3582\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3983 - val_loss: 0.3601\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3981 - val_loss: 0.3564\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3971 - val_loss: 0.3583\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3963 - val_loss: 0.3559\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3957 - val_loss: 0.3580\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3944 - val_loss: 0.3600\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.396 - 1s 59us/sample - loss: 0.3945 - val_loss: 0.3542\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3936 - val_loss: 0.3551\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3936 - val_loss: 0.3577\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3928 - val_loss: 0.3530\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3921 - val_loss: 0.3573\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3911 - val_loss: 0.3558\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3911 - val_loss: 0.3534\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3901 - val_loss: 0.3566\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3901 - val_loss: 0.3514\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3894 - val_loss: 0.3508\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3887 - val_loss: 0.3523\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3879 - val_loss: 0.3488\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3876 - val_loss: 0.3508\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3874 - val_loss: 0.3565\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3867 - val_loss: 0.3480\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3857 - val_loss: 0.3489\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3854 - val_loss: 0.3500\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3849 - val_loss: 0.3505\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3307\n",
      "[CV 4/5] END learning_rate=0.001668105735917552, n_hidden=1, n_neurons=12;, score=-0.331 total time=  55.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 2.4177 - val_loss: 2.1598\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.8352 - val_loss: 0.7329\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7059 - val_loss: 0.7271\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6586 - val_loss: 0.7036\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6242 - val_loss: 0.6614\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5958 - val_loss: 0.6333\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5713 - val_loss: 0.6101\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5497 - val_loss: 0.5787\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5318 - val_loss: 0.5591\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5158 - val_loss: 0.5375\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5020 - val_loss: 0.5202\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4900 - val_loss: 0.5011\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4793 - val_loss: 0.4870\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4704 - val_loss: 0.4695\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4624 - val_loss: 0.4547\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4554 - val_loss: 0.4403\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4494 - val_loss: 0.4307\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4436 - val_loss: 0.4215\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4384 - val_loss: 0.4160\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4343 - val_loss: 0.4101\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4301 - val_loss: 0.4055\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4268 - val_loss: 0.4023\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4236 - val_loss: 0.3990\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4209 - val_loss: 0.3964\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4182 - val_loss: 0.3933\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4160 - val_loss: 0.3915\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4140 - val_loss: 0.3893\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4121 - val_loss: 0.3879\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4102 - val_loss: 0.3869\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4089 - val_loss: 0.3858\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4075 - val_loss: 0.3847\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4063 - val_loss: 0.3838\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4050 - val_loss: 0.3834\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4039 - val_loss: 0.3832\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4025 - val_loss: 0.3823\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4014 - val_loss: 0.3836\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4005 - val_loss: 0.3824\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3993 - val_loss: 0.3828\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3987 - val_loss: 0.3824\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3975 - val_loss: 0.3823\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3968 - val_loss: 0.3815\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3961 - val_loss: 0.3832\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3954 - val_loss: 0.3826\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3943 - val_loss: 0.3838\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3936 - val_loss: 0.3833\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3931 - val_loss: 0.3851\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3923 - val_loss: 0.3844\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3919 - val_loss: 0.3845\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3912 - val_loss: 0.3844\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3904 - val_loss: 0.3821\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3897 - val_loss: 0.3848\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.4216\n",
      "[CV 5/5] END learning_rate=0.001668105735917552, n_hidden=1, n_neurons=12;, score=-0.422 total time=  27.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 88us/sample - loss: 1.0047 - val_loss: 12.1645\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5267 - val_loss: 39.9383\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7308 - val_loss: 2.5944\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4069 - val_loss: 1.3712\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3939 - val_loss: 2.4091\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3853 - val_loss: 4.7155\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3988 - val_loss: 0.8472\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3661 - val_loss: 0.3688\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3519 - val_loss: 0.3456\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3477 - val_loss: 0.3235\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3464 - val_loss: 0.3143\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3398 - val_loss: 0.3128\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3348 - val_loss: 0.3217\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3327 - val_loss: 0.3589\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3304 - val_loss: 0.3433\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3283 - val_loss: 0.3525\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3242 - val_loss: 0.3279\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3209 - val_loss: 0.3316\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3182 - val_loss: 0.3231\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3163 - val_loss: 0.3148\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3147 - val_loss: 0.3250\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3128 - val_loss: 0.3142\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3090\n",
      "[CV 1/5] END learning_rate=0.00901264216211302, n_hidden=2, n_neurons=89;, score=-0.309 total time=  13.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 0.7501 - val_loss: 7.3178\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5447 - val_loss: 9.0581\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4932 - val_loss: 1.7063\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3875 - val_loss: 1.1351\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3715 - val_loss: 0.3632\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3563 - val_loss: 0.3544\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3482 - val_loss: 0.3637\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3423 - val_loss: 0.3582\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3366 - val_loss: 0.3369\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3316 - val_loss: 0.3614\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3285 - val_loss: 0.3674\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3240 - val_loss: 0.3274\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3200 - val_loss: 0.3249\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3172 - val_loss: 0.3249\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3142 - val_loss: 0.3253\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3105 - val_loss: 0.3497\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3087 - val_loss: 0.3057\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3073 - val_loss: 0.3993\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3057 - val_loss: 0.3271\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3037 - val_loss: 0.4227\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3012 - val_loss: 0.3429\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2996 - val_loss: 0.4377\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2990 - val_loss: 0.3281\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2966 - val_loss: 0.3575\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2936 - val_loss: 0.3191\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2925 - val_loss: 0.3807\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2900 - val_loss: 0.3531\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3452\n",
      "[CV 2/5] END learning_rate=0.00901264216211302, n_hidden=2, n_neurons=89;, score=-0.345 total time=  16.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 0.8023 - val_loss: 0.6204\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4400 - val_loss: 0.8877\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4017 - val_loss: 1.5329\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3787 - val_loss: 0.7554\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3714 - val_loss: 1.3534\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3556 - val_loss: 1.5290\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3501 - val_loss: 1.1243\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3455 - val_loss: 1.2885\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3381 - val_loss: 0.9836\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3343 - val_loss: 1.2744\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3290 - val_loss: 0.5152\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3255 - val_loss: 1.7939\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3256 - val_loss: 0.5708\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3276 - val_loss: 0.4357\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3170 - val_loss: 1.3291\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3185 - val_loss: 1.3540\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3293 - val_loss: 0.3479\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3152 - val_loss: 2.0146\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3107 - val_loss: 2.0239\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3102 - val_loss: 1.4659\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3062 - val_loss: 1.9999\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3289 - val_loss: 0.4363\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3064 - val_loss: 0.9187\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3023 - val_loss: 0.3735\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2984 - val_loss: 0.9948\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2978 - val_loss: 1.0210\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2960 - val_loss: 0.2919\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2948 - val_loss: 1.2142\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2929 - val_loss: 0.8490\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2927 - val_loss: 1.6287\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2906 - val_loss: 0.7188\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2890 - val_loss: 0.7665\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2895 - val_loss: 1.4955\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2882 - val_loss: 0.7824\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2863 - val_loss: 0.6831\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2840 - val_loss: 0.4500\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2816 - val_loss: 1.1522\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3537\n",
      "[CV 3/5] END learning_rate=0.00901264216211302, n_hidden=2, n_neurons=89;, score=-0.354 total time=  22.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 0.7758 - val_loss: 1.3009\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4605 - val_loss: 8.6980\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5023 - val_loss: 5.9398\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4505 - val_loss: 3.0168\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4099 - val_loss: 1.1035\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3908 - val_loss: 0.3902\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3734 - val_loss: 0.3673\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3698 - val_loss: 0.3704\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3614 - val_loss: 0.3997\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3565 - val_loss: 0.3501\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3524 - val_loss: 0.3744\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3481 - val_loss: 0.3424\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3469 - val_loss: 0.3285\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3421 - val_loss: 0.3329\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3381 - val_loss: 0.3392\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3357 - val_loss: 0.3418\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3323 - val_loss: 0.3200\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3283 - val_loss: 0.4689\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3277 - val_loss: 0.3103\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3238 - val_loss: 0.3550\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3219 - val_loss: 0.3244\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3208 - val_loss: 0.3322\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3211 - val_loss: 0.3043\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3188 - val_loss: 0.3359\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3130 - val_loss: 0.3090\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3121 - val_loss: 0.3142\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3096 - val_loss: 0.3048\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3068 - val_loss: 0.3120\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3046 - val_loss: 0.3168\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3043 - val_loss: 0.2930\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3020 - val_loss: 0.3119\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3000 - val_loss: 0.3457\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3000 - val_loss: 0.5104\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2999 - val_loss: 0.3352\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2956 - val_loss: 0.4637\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2951 - val_loss: 0.3774\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2943 - val_loss: 0.5551\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2969 - val_loss: 0.3257\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2929 - val_loss: 0.6860\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2919 - val_loss: 0.9267\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.2701\n",
      "[CV 4/5] END learning_rate=0.00901264216211302, n_hidden=2, n_neurons=89;, score=-0.270 total time=  24.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 0.7426 - val_loss: 1.6627\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4285 - val_loss: 0.6098\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4034 - val_loss: 6.4199\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3999 - val_loss: 6.9533\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4303 - val_loss: 2.6836\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3674 - val_loss: 4.0178\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3903 - val_loss: 1.4046\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3568 - val_loss: 0.9032\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3511 - val_loss: 0.5194\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3464 - val_loss: 0.3785\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3360 - val_loss: 0.3344\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3304 - val_loss: 0.3841\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3282 - val_loss: 0.3456\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3250 - val_loss: 0.5157\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3351 - val_loss: 0.3238\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3197 - val_loss: 0.3148\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3195 - val_loss: 0.3593\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3169 - val_loss: 0.3090\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3145 - val_loss: 0.3412\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3145 - val_loss: 0.3057\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3085 - val_loss: 0.5544\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3102 - val_loss: 0.3316\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3038 - val_loss: 0.6753\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3038 - val_loss: 0.5440\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3048 - val_loss: 0.4615\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3030 - val_loss: 0.2931\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2977 - val_loss: 0.3093\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2971 - val_loss: 0.3342\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2964 - val_loss: 0.2980\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2951 - val_loss: 0.3378\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2936 - val_loss: 0.2922\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2911 - val_loss: 0.3174\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2904 - val_loss: 0.3581\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2891 - val_loss: 0.3089\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2885 - val_loss: 0.4473\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2860 - val_loss: 0.2931\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2860 - val_loss: 0.2891\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2873 - val_loss: 0.7740\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2866 - val_loss: 0.5014\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2858 - val_loss: 1.0035\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2886 - val_loss: 0.3827\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2840 - val_loss: 0.9239\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2875 - val_loss: 0.4989\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2835 - val_loss: 5.8170\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3371 - val_loss: 0.3563\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2868 - val_loss: 0.3861\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.283 - 1s 64us/sample - loss: 0.2830 - val_loss: 0.3074\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3419\n",
      "[CV 5/5] END learning_rate=0.00901264216211302, n_hidden=2, n_neurons=89;, score=-0.342 total time=  28.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 96us/sample - loss: 0.9933 - val_loss: 20.2251\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.0446 - val_loss: 900.4824\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.9984 - val_loss: 16.0540\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.9238 - val_loss: 150.8379\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.0476 - val_loss: 164.3128\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 3.4831 - val_loss: 484.6904\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 7.9523 - val_loss: 592.2285\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 33.8360 - val_loss: 716.7338\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 11.4218 - val_loss: 946.3522\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 7.0106 - val_loss: 2329.3739\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 12.6853 - val_loss: 2522.8314\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 20.3860 - val_loss: 15536.7469\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 588.2623 - val_loss: 22558.9640\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 69.2750\n",
      "[CV 1/5] END learning_rate=0.023051467197032174, n_hidden=0, n_neurons=35;, score=-69.275 total time=   7.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.2080 - val_loss: 66.4883\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1.5949 - val_loss: 220.1822\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.8011 - val_loss: 265.1178\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1.6529 - val_loss: 2921.1150\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 71.3331 - val_loss: 2530.4163\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 141.8625 - val_loss: 2060.4182\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 133.5365 - val_loss: 2083.0539\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 3.3872 - val_loss: 7120.0807\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 259.0698 - val_loss: 6467.5265\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 7.9901 - val_loss: 10193.0366\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 321.0159 - val_loss: 9211.1851\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 27.7517\n",
      "[CV 2/5] END learning_rate=0.023051467197032174, n_hidden=0, n_neurons=35;, score=-27.752 total time=   5.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7515 - val_loss: 24.6798\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5190 - val_loss: 6.0458\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5476 - val_loss: 4.4337\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5957 - val_loss: 19.1828\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5368 - val_loss: 3.8761\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5124 - val_loss: 21.9562\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5115 - val_loss: 5.1744\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5344 - val_loss: 10.0602\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5089 - val_loss: 16.8767\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5119 - val_loss: 7.3842\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5104 - val_loss: 17.3698\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5270 - val_loss: 4.4706\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5727 - val_loss: 18.0858\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5489 - val_loss: 14.3882\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5065 - val_loss: 20.5078\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.4445\n",
      "[CV 3/5] END learning_rate=0.023051467197032174, n_hidden=0, n_neurons=35;, score=-1.445 total time=   8.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 2.4693 - val_loss: 57.0610\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.7935 - val_loss: 2.3614\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.1538 - val_loss: 239.6997\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.8122 - val_loss: 73.1036\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8709 - val_loss: 521.8045\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.9607 - val_loss: 104.0870\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 5.0700 - val_loss: 16.0140\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.6162 - val_loss: 27.8438\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7487 - val_loss: 1200.2742\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 33.3776 - val_loss: 286.8619\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.5649 - val_loss: 222.4543\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.8249 - val_loss: 11.6936\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.5426\n",
      "[CV 4/5] END learning_rate=0.023051467197032174, n_hidden=0, n_neurons=35;, score=-0.543 total time=   6.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 1.0715 - val_loss: 0.5614\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.8069 - val_loss: 0.8950\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6444 - val_loss: 1689.1903\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 104.3497 - val_loss: 439.2254\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 3.2789 - val_loss: 82.1538\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7225 - val_loss: 465.7674\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.8400 - val_loss: 208.4466\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.8301 - val_loss: 544.9395\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 2.7408 - val_loss: 81.1610\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.3312 - val_loss: 1491.5928\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.4753 - val_loss: 436.1813\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.4187\n",
      "[CV 5/5] END learning_rate=0.023051467197032174, n_hidden=0, n_neurons=35;, score=-1.419 total time=   5.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.8949 - val_loss: 0.6174\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4848 - val_loss: 0.4187\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4210 - val_loss: 0.6331\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4130 - val_loss: 8.7846\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6712 - val_loss: 4.9932\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4178 - val_loss: 8.2505\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5004 - val_loss: 4.8688\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4366 - val_loss: 0.5474\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3767 - val_loss: 0.4027\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3698 - val_loss: 0.4200\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3655 - val_loss: 0.4482\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3610 - val_loss: 0.4450\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3576 - val_loss: 0.3979\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3537 - val_loss: 0.4441\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3504 - val_loss: 0.4265\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3481 - val_loss: 0.4271\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3456 - val_loss: 0.3780\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3435 - val_loss: 0.3681\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3417 - val_loss: 0.3801\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3384 - val_loss: 0.3760\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3380 - val_loss: 0.3533\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3359 - val_loss: 0.3954\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3334 - val_loss: 0.4240\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3330 - val_loss: 0.3307\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3312 - val_loss: 0.3930\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3301 - val_loss: 0.3716\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3283 - val_loss: 0.3294\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3278 - val_loss: 0.3354\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3252 - val_loss: 0.3548\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3240 - val_loss: 0.3608\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3232 - val_loss: 0.3097\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3224 - val_loss: 0.4160\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3208 - val_loss: 0.4229\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3206 - val_loss: 0.3420\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3191 - val_loss: 0.4649\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3183 - val_loss: 0.3539\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3162 - val_loss: 0.3139\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3158 - val_loss: 0.4182\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3147 - val_loss: 0.3516\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3128 - val_loss: 0.3982\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3131 - val_loss: 0.2988\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3125 - val_loss: 0.4266\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3120 - val_loss: 0.2997\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3099 - val_loss: 0.3688\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3094 - val_loss: 0.3050\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3079 - val_loss: 0.5360\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3088 - val_loss: 0.3045\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3051 - val_loss: 0.3128\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3062 - val_loss: 0.3808\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3052 - val_loss: 0.3595\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3060 - val_loss: 0.6328\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3034\n",
      "[CV 1/5] END learning_rate=0.0061271619002212915, n_hidden=2, n_neurons=35;, score=-0.303 total time=  30.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.9547 - val_loss: 3.8751\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6826 - val_loss: 13.0142\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5479 - val_loss: 3.4847\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4519 - val_loss: 0.3925\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3972 - val_loss: 0.3729\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3810 - val_loss: 0.3622\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3721 - val_loss: 0.3558\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3660 - val_loss: 0.3544\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3609 - val_loss: 0.3567\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3568 - val_loss: 0.3498\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3531 - val_loss: 0.3442\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3500 - val_loss: 0.3421\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3468 - val_loss: 0.3458\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3438 - val_loss: 0.3592\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3411 - val_loss: 0.3522\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3384 - val_loss: 0.3493\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3360 - val_loss: 0.3431\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3334 - val_loss: 0.3482\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3315 - val_loss: 0.3442\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3291 - val_loss: 0.3400\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3278 - val_loss: 0.3378\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3259 - val_loss: 0.3316\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3235 - val_loss: 0.3368\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3225 - val_loss: 0.3352\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3202 - val_loss: 0.3265\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3190 - val_loss: 0.3388\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3177 - val_loss: 0.3333\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3155 - val_loss: 0.3306\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3144 - val_loss: 0.3223\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3128 - val_loss: 0.3351\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3122 - val_loss: 0.3420\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3109 - val_loss: 0.3133\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3099 - val_loss: 0.3296\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3088 - val_loss: 0.3148\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3065 - val_loss: 0.3256\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3064 - val_loss: 0.3076\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3049 - val_loss: 0.3370\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3041 - val_loss: 0.3221\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3023 - val_loss: 0.3075\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3017 - val_loss: 0.3344\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3009 - val_loss: 0.3152\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2986 - val_loss: 0.3163\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2987 - val_loss: 0.3285\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2979 - val_loss: 0.3005\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2969 - val_loss: 0.3194\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2956 - val_loss: 0.3074\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2952 - val_loss: 0.3104\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2944 - val_loss: 0.3321\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2934 - val_loss: 0.3158\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2932 - val_loss: 0.3806\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2918 - val_loss: 0.2918\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2906 - val_loss: 0.3444\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2905 - val_loss: 0.3107\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2898 - val_loss: 0.3085\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2898 - val_loss: 0.3342\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2883 - val_loss: 0.3033\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2870 - val_loss: 0.3125\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2871 - val_loss: 0.3141\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2861 - val_loss: 0.3027\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2855 - val_loss: 0.2990\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2842 - val_loss: 0.3009\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3487\n",
      "[CV 2/5] END learning_rate=0.0061271619002212915, n_hidden=2, n_neurons=35;, score=-0.349 total time=  36.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 109us/sample - loss: 0.9102 - val_loss: 1.3604\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5252 - val_loss: 0.5314\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4438 - val_loss: 0.5287\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4178 - val_loss: 0.5486\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3987 - val_loss: 0.7800\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3859 - val_loss: 0.8590\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3772 - val_loss: 0.6842\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3691 - val_loss: 0.8466\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3652 - val_loss: 0.8284\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3645 - val_loss: 0.8946\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3606 - val_loss: 1.0151\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3546 - val_loss: 1.0885\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3536 - val_loss: 0.8912\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3999\n",
      "[CV 3/5] END learning_rate=0.0061271619002212915, n_hidden=2, n_neurons=35;, score=-0.400 total time=   8.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.0486 - val_loss: 11.9289\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5535 - val_loss: 1.0749\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4515 - val_loss: 0.3902\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4209 - val_loss: 0.4050\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4071 - val_loss: 0.3873\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3966 - val_loss: 0.4036\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3888 - val_loss: 0.4438\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3841 - val_loss: 0.4147\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3791 - val_loss: 0.3569\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3746 - val_loss: 0.4626\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3715 - val_loss: 0.3389\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3681 - val_loss: 0.4098\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3677 - val_loss: 0.3636\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3627 - val_loss: 0.3822\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3603 - val_loss: 0.4069\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3573 - val_loss: 0.3596\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3546 - val_loss: 0.3251\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3526 - val_loss: 0.4263\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3508 - val_loss: 0.3209\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3479 - val_loss: 0.3382\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3481 - val_loss: 0.3767\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3449 - val_loss: 0.3321\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3423 - val_loss: 0.3620\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3400 - val_loss: 0.3211\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3385 - val_loss: 0.3453\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3358 - val_loss: 0.3390\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3333 - val_loss: 0.3116\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3329 - val_loss: 0.3891\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3311 - val_loss: 0.3108\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3292 - val_loss: 0.3463\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3280 - val_loss: 0.3456\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3257 - val_loss: 0.3694\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3250 - val_loss: 0.3051\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3238 - val_loss: 0.3329\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3218 - val_loss: 0.3513\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3216 - val_loss: 0.3062\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3180 - val_loss: 0.3730\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3193 - val_loss: 0.3073\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3169 - val_loss: 0.3731\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3153 - val_loss: 0.3602\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3140 - val_loss: 0.3223\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3132 - val_loss: 0.3848\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3129 - val_loss: 0.3504\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.2818\n",
      "[CV 4/5] END learning_rate=0.0061271619002212915, n_hidden=2, n_neurons=35;, score=-0.282 total time=  25.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 0.9766 - val_loss: 2.4351\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5177 - val_loss: 5.6110\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4698 - val_loss: 4.1442\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4363 - val_loss: 0.4602\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3930 - val_loss: 0.4544\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3804 - val_loss: 0.3719\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3737 - val_loss: 0.3898\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3670 - val_loss: 0.3675\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3609 - val_loss: 0.3910\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3557 - val_loss: 0.3583\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3528 - val_loss: 0.3548\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3499 - val_loss: 0.3761\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3453 - val_loss: 0.3958\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3449 - val_loss: 0.3424\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3398 - val_loss: 0.3945\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3372 - val_loss: 0.3747\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3368 - val_loss: 0.3227\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3339 - val_loss: 0.3503\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3339 - val_loss: 0.4238\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3300 - val_loss: 0.3430\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3287 - val_loss: 0.3238\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3258 - val_loss: 0.3236\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3244 - val_loss: 0.3257\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3226 - val_loss: 0.3613\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3212 - val_loss: 0.3898\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3209 - val_loss: 0.3295\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3188 - val_loss: 0.4454\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3568\n",
      "[CV 5/5] END learning_rate=0.0061271619002212915, n_hidden=2, n_neurons=35;, score=-0.357 total time=  16.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 96us/sample - loss: 0.5534 - val_loss: 0.4723\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3844 - val_loss: 2.5231\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4014 - val_loss: 13.8789\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.6543 - val_loss: 0.4906\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4595 - val_loss: 6.9299\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4794 - val_loss: 0.4900\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3868 - val_loss: 4.2444\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3728 - val_loss: 2.1858\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3723 - val_loss: 0.3963\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3510 - val_loss: 0.3180\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3407 - val_loss: 0.4476\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3377 - val_loss: 0.3183\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3283 - val_loss: 0.3018\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3244 - val_loss: 0.3234\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3212 - val_loss: 0.3249\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3141 - val_loss: 0.3797\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3143 - val_loss: 0.4933\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3141 - val_loss: 0.2976\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3082 - val_loss: 0.3547\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3080 - val_loss: 0.3958\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3039 - val_loss: 0.2987\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3013 - val_loss: 0.2853\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2996 - val_loss: 0.4247\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2970 - val_loss: 0.3307\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2938 - val_loss: 0.2811\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2955 - val_loss: 0.3142\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2928 - val_loss: 0.3324\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2911 - val_loss: 0.2876\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2925 - val_loss: 0.3595\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2872 - val_loss: 0.2772\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2881 - val_loss: 0.2793\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2850 - val_loss: 0.2746\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2832 - val_loss: 0.3016\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2820 - val_loss: 0.3706\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2830 - val_loss: 0.2712\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2782 - val_loss: 0.2819\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2766 - val_loss: 0.3035\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2764 - val_loss: 0.3271\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2759 - val_loss: 0.2900\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2733 - val_loss: 0.3111\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2749 - val_loss: 0.2762\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2748 - val_loss: 0.2836\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2716 - val_loss: 0.2819\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2729 - val_loss: 0.2663\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2680 - val_loss: 0.2954\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2667 - val_loss: 0.2805\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2681 - val_loss: 0.3052\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2668 - val_loss: 0.2743\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2656 - val_loss: 0.2675\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2645 - val_loss: 0.3064\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2644 - val_loss: 0.2968\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2641 - val_loss: 0.3115\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2613 - val_loss: 0.2785\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2588 - val_loss: 0.2965\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3029\n",
      "[CV 1/5] END learning_rate=0.024780436744340287, n_hidden=3, n_neurons=90;, score=-0.303 total time=  35.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.5247 - val_loss: 2.7483\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3755 - val_loss: 1.1307\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3833 - val_loss: 0.7841\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3497 - val_loss: 0.3222\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3330 - val_loss: 1.1264\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3417 - val_loss: 1.7757\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3353 - val_loss: 0.6510\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3197 - val_loss: 0.3572\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3098 - val_loss: 0.6081\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3051 - val_loss: 0.3110\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2969 - val_loss: 0.4071\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2944 - val_loss: 0.2796\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2936 - val_loss: 0.3703\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2839 - val_loss: 0.3431\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2830 - val_loss: 0.3452\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2802 - val_loss: 0.2743\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2824 - val_loss: 0.2919\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2773 - val_loss: 0.3312\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2726 - val_loss: 0.2839\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2756 - val_loss: 0.3904\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2705 - val_loss: 0.2809\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2701 - val_loss: 0.3718\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2666 - val_loss: 0.3072\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2644 - val_loss: 0.4645\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2624 - val_loss: 0.4012\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2618 - val_loss: 0.5088\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.5335\n",
      "[CV 2/5] END learning_rate=0.024780436744340287, n_hidden=3, n_neurons=90;, score=-0.534 total time=  17.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6077 - val_loss: 0.6087\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3786 - val_loss: 2.1258\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3562 - val_loss: 4.1608\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3399 - val_loss: 0.6427\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3327 - val_loss: 1.0341\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3295 - val_loss: 1.2607\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3213 - val_loss: 0.5404\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3137 - val_loss: 1.8095\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3116 - val_loss: 0.3375\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3098 - val_loss: 0.9786\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3026 - val_loss: 1.1359\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2963 - val_loss: 0.4338\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2956 - val_loss: 0.3915\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2938 - val_loss: 0.7537\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2894 - val_loss: 0.7754\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2871 - val_loss: 1.6909\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2817 - val_loss: 1.5190\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2793 - val_loss: 0.8334\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2760 - val_loss: 1.2333\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3974\n",
      "[CV 3/5] END learning_rate=0.024780436744340287, n_hidden=3, n_neurons=90;, score=-0.397 total time=  12.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 116us/sample - loss: 0.7315 - val_loss: 4.6646\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: nan\n",
      "[CV 4/5] END learning_rate=0.024780436744340287, n_hidden=3, n_neurons=90;, score=nan total time=   7.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 0.5747 - val_loss: 8.2467\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4325 - val_loss: 1.0497\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3842 - val_loss: 38.0743\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3745 - val_loss: 0.6645\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3967 - val_loss: 0.4831\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3458 - val_loss: 0.9857\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3455 - val_loss: 0.9836\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3325 - val_loss: 0.7068\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3331 - val_loss: 0.6479\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3218 - val_loss: 0.3597\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3119 - val_loss: 0.5871\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3114 - val_loss: 0.3050\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3038 - val_loss: 0.3153\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3006 - val_loss: 0.3022\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2962 - val_loss: 0.4964\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2940 - val_loss: 0.4842\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2946 - val_loss: 0.7098\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2915 - val_loss: 0.2839\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2872 - val_loss: 0.3111\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2857 - val_loss: 0.3102\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2864 - val_loss: 0.3358\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2808 - val_loss: 0.2896\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2787 - val_loss: 0.3018\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2750 - val_loss: 0.3146\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2777 - val_loss: 0.2808\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2719 - val_loss: 0.2995\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2701 - val_loss: 0.3866\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2693 - val_loss: 0.3248\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2683 - val_loss: 0.3114\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2678 - val_loss: 0.3658\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2655 - val_loss: 0.3160\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2659 - val_loss: 0.3130\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2633 - val_loss: 0.3423\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2613 - val_loss: 0.4489\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2618 - val_loss: 0.5577\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.5478\n",
      "[CV 5/5] END learning_rate=0.024780436744340287, n_hidden=3, n_neurons=90;, score=-0.548 total time=  23.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 2.3889 - val_loss: 3.8926\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6659 - val_loss: 1.0222\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5875 - val_loss: 0.6375\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5710 - val_loss: 0.6085\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5589 - val_loss: 1.0130\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5682 - val_loss: 1.8462\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5494 - val_loss: 2.5398\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5795 - val_loss: 1.0701\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5362 - val_loss: 2.9659\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5817 - val_loss: 0.8391\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5423 - val_loss: 0.7545\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5440 - val_loss: 0.8065\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5447 - val_loss: 0.7102\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5450 - val_loss: 0.5708\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5459 - val_loss: 0.6073\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5422 - val_loss: 0.9087\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5330 - val_loss: 1.7292\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5597 - val_loss: 0.6843\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5332 - val_loss: 1.3638\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5544 - val_loss: 0.5177\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5305 - val_loss: 1.5660\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5577 - val_loss: 1.0336\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5307 - val_loss: 2.1657\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5636 - val_loss: 1.4317\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5400 - val_loss: 0.8427\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5493 - val_loss: 0.7386\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5351 - val_loss: 1.0083\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5498 - val_loss: 0.5273\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5276 - val_loss: 1.8576\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5542 - val_loss: 0.7001\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.5119\n",
      "[CV 1/5] END learning_rate=0.0029664348311075318, n_hidden=0, n_neurons=74;, score=-0.512 total time=  15.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 3.0069 - val_loss: 2.4954\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.9234 - val_loss: 1.3600\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7560 - val_loss: 1.5008\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7012 - val_loss: 1.2699\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6733 - val_loss: 1.3238\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6261 - val_loss: 0.9314\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6039 - val_loss: 1.0157\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5869 - val_loss: 1.1227\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5853 - val_loss: 1.1526\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5520 - val_loss: 2.6139\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5827 - val_loss: 1.0193\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5487 - val_loss: 0.5388\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5470 - val_loss: 0.5374\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5415 - val_loss: 0.6024\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5343 - val_loss: 0.7490\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5339 - val_loss: 0.9897\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5460 - val_loss: 0.6197\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5330 - val_loss: 0.5327\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5328 - val_loss: 0.6572\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5318 - val_loss: 0.5452\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5256 - val_loss: 0.9213\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5326 - val_loss: 0.8080\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5400 - val_loss: 1.0991\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5250 - val_loss: 1.2975\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5374 - val_loss: 0.6944\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5363 - val_loss: 1.8241\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5306 - val_loss: 0.9160\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5289 - val_loss: 1.0072\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.5534\n",
      "[CV 2/5] END learning_rate=0.0029664348311075318, n_hidden=0, n_neurons=74;, score=-0.553 total time=  14.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 2.8255 - val_loss: 4.8978\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7897 - val_loss: 8.2004\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6784 - val_loss: 10.5388\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6384 - val_loss: 12.4281\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6069 - val_loss: 14.0364\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5828 - val_loss: 14.7617\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5665 - val_loss: 15.1010\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5524 - val_loss: 15.1697\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5415 - val_loss: 15.1068\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5348 - val_loss: 15.0007\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5256 - val_loss: 15.0030\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 1.0976\n",
      "[CV 3/5] END learning_rate=0.0029664348311075318, n_hidden=0, n_neurons=74;, score=-1.098 total time=   6.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 2.8024 - val_loss: 1.4040\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6879 - val_loss: 1.1260\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6213 - val_loss: 0.5468\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5974 - val_loss: 0.5335\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5860 - val_loss: 0.5966\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5661 - val_loss: 1.6630\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5876 - val_loss: 0.5946\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5622 - val_loss: 0.6339\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5607 - val_loss: 0.6216\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5628 - val_loss: 1.2189\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5488 - val_loss: 2.0345\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5737 - val_loss: 1.1145\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5448 - val_loss: 2.0049\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5731 - val_loss: 0.9506\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.4964\n",
      "[CV 4/5] END learning_rate=0.0029664348311075318, n_hidden=0, n_neurons=74;, score=-0.496 total time=   7.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 3.0336 - val_loss: 13.5192\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.9091 - val_loss: 3.3366\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7637 - val_loss: 1.9529\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6845 - val_loss: 1.7030\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6638 - val_loss: 0.6034\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6147 - val_loss: 1.3901\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6060 - val_loss: 0.9432\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5929 - val_loss: 0.5569\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5734 - val_loss: 0.8639\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5641 - val_loss: 0.6081\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5528 - val_loss: 0.6728\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5550 - val_loss: 0.5318\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5342 - val_loss: 1.6673\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5506 - val_loss: 0.9052\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5445 - val_loss: 1.0422\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5348 - val_loss: 0.7879\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5324 - val_loss: 0.9261\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5314 - val_loss: 0.9729\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5357 - val_loss: 0.6431\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5365 - val_loss: 0.6533\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5290 - val_loss: 0.4932\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5280 - val_loss: 0.6195\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5328 - val_loss: 0.5849\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5275 - val_loss: 0.5262\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5283 - val_loss: 0.5338\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5216 - val_loss: 1.2346\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5398 - val_loss: 0.7121\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5203 - val_loss: 1.3034\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5379 - val_loss: 0.5754\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5306 - val_loss: 0.6037\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5266 - val_loss: 0.5373\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.5643\n",
      "[CV 5/5] END learning_rate=0.0029664348311075318, n_hidden=0, n_neurons=74;, score=-0.564 total time=  16.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 3.4662 - val_loss: 2.7451\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.3570 - val_loss: 1.8897\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.9467 - val_loss: 0.9380\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7962 - val_loss: 0.7352\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7271 - val_loss: 0.6787\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6871 - val_loss: 0.6358\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6597 - val_loss: 0.6117\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6384 - val_loss: 0.5887\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6204 - val_loss: 0.5719\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6054 - val_loss: 0.5587\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5912 - val_loss: 0.5461\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5786 - val_loss: 0.5344\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5670 - val_loss: 0.5287\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5561 - val_loss: 0.5290\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5461 - val_loss: 0.5239\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5368 - val_loss: 0.4973\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5282 - val_loss: 0.4988\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5199 - val_loss: 0.4859\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5123 - val_loss: 0.4806\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5052 - val_loss: 0.4815\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4986 - val_loss: 0.4694\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4924 - val_loss: 0.4692\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4866 - val_loss: 0.4580\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4812 - val_loss: 0.4480\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4763 - val_loss: 0.4517\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4717 - val_loss: 0.4555\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4674 - val_loss: 0.4488\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4634 - val_loss: 0.4447\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4597 - val_loss: 0.4364\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4563 - val_loss: 0.4390\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4532 - val_loss: 0.4352\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4501 - val_loss: 0.4338\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4473 - val_loss: 0.4239\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4447 - val_loss: 0.4192\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4422 - val_loss: 0.4196\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4398 - val_loss: 0.4209\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4375 - val_loss: 0.4187\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4354 - val_loss: 0.4214\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4333 - val_loss: 0.4175\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4314 - val_loss: 0.4169\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4295 - val_loss: 0.4202\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4278 - val_loss: 0.4110\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4260 - val_loss: 0.4096\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4245 - val_loss: 0.4120\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4230 - val_loss: 0.4130\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4216 - val_loss: 0.4148\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4202 - val_loss: 0.4082\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4188 - val_loss: 0.4124\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4175 - val_loss: 0.4093\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4164 - val_loss: 0.4015\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4151 - val_loss: 0.4065\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4140 - val_loss: 0.4118\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4129 - val_loss: 0.3986\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4118 - val_loss: 0.4059\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4108 - val_loss: 0.4061\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4097 - val_loss: 0.4086\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4087 - val_loss: 0.3993\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4078 - val_loss: 0.4031\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4069 - val_loss: 0.4044\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4060 - val_loss: 0.4045\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4052 - val_loss: 0.4102\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4044 - val_loss: 0.4037\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4035 - val_loss: 0.4040\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.3752\n",
      "[CV 1/5] END learning_rate=0.0005064129375837458, n_hidden=1, n_neurons=40;, score=-0.375 total time=  34.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 103us/sample - loss: 3.0772 - val_loss: 5.3760\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.2843 - val_loss: 2.2371\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.8981 - val_loss: 0.9900\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7871 - val_loss: 0.7331\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7427 - val_loss: 0.7049\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7169 - val_loss: 0.7035\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6969 - val_loss: 0.6970\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6786 - val_loss: 0.7154\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6630 - val_loss: 0.7009\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6486 - val_loss: 0.6426\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6338 - val_loss: 0.6638\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6210 - val_loss: 0.6592\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6091 - val_loss: 0.5879\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5971 - val_loss: 0.5777\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5861 - val_loss: 0.5704\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5750 - val_loss: 0.5997\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5659 - val_loss: 0.5563\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5560 - val_loss: 0.5641\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5473 - val_loss: 0.5402\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5388 - val_loss: 0.5209\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5305 - val_loss: 0.5203\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5226 - val_loss: 0.5275\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5156 - val_loss: 0.5067\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5085 - val_loss: 0.5027\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5020 - val_loss: 0.4950\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4957 - val_loss: 0.4807\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4898 - val_loss: 0.4708\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4841 - val_loss: 0.4714\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4787 - val_loss: 0.4673\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4738 - val_loss: 0.4588\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4689 - val_loss: 0.4577\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4645 - val_loss: 0.4550\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4603 - val_loss: 0.4455\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4563 - val_loss: 0.4418\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4525 - val_loss: 0.4405\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4490 - val_loss: 0.4329\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4457 - val_loss: 0.4290\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4427 - val_loss: 0.4258\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4398 - val_loss: 0.4233\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4372 - val_loss: 0.4206\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4346 - val_loss: 0.4184\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4322 - val_loss: 0.4169\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4299 - val_loss: 0.4137\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4278 - val_loss: 0.4122\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4257 - val_loss: 0.4107\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4239 - val_loss: 0.4091\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4220 - val_loss: 0.4064\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4204 - val_loss: 0.4060\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4187 - val_loss: 0.4035\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4172 - val_loss: 0.4052\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4159 - val_loss: 0.4029\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4145 - val_loss: 0.4007\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4132 - val_loss: 0.3989\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4120 - val_loss: 0.4003\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4108 - val_loss: 0.3988\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4096 - val_loss: 0.3978\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4085 - val_loss: 0.3965\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4074 - val_loss: 0.3970\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4064 - val_loss: 0.3951\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4054 - val_loss: 0.3941\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4045 - val_loss: 0.3932\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4036 - val_loss: 0.3928\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4027 - val_loss: 0.3925\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4017 - val_loss: 0.3927\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4010 - val_loss: 0.3915\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4002 - val_loss: 0.3912\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3994 - val_loss: 0.3922\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3987 - val_loss: 0.3916\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3979 - val_loss: 0.3902\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3973 - val_loss: 0.3915\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3966 - val_loss: 0.3902\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3958 - val_loss: 0.3909\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3952 - val_loss: 0.3904\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3945 - val_loss: 0.3909\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3939 - val_loss: 0.3875\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3933 - val_loss: 0.3909\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3927 - val_loss: 0.3884\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3920 - val_loss: 0.3915\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3914 - val_loss: 0.3893\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3909 - val_loss: 0.3870\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3902 - val_loss: 0.3867\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3897 - val_loss: 0.3873\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3892 - val_loss: 0.3868\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3886 - val_loss: 0.3916\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3881 - val_loss: 0.3836\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3875 - val_loss: 0.3901\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3872 - val_loss: 0.3836\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3866 - val_loss: 0.3840\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3862 - val_loss: 0.3901\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3857 - val_loss: 0.3833\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3853 - val_loss: 0.3835\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3847 - val_loss: 0.3866\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3844 - val_loss: 0.3795\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3839 - val_loss: 0.3847\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3835 - val_loss: 0.3866\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3831 - val_loss: 0.3840\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3827 - val_loss: 0.3819\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3823 - val_loss: 0.3825\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3819 - val_loss: 0.3824\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3815 - val_loss: 0.3838\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4267\n",
      "[CV 2/5] END learning_rate=0.0005064129375837458, n_hidden=1, n_neurons=40;, score=-0.427 total time=  55.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 3.2105 - val_loss: 2.2375\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.2271 - val_loss: 2.1870\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.8353 - val_loss: 1.8010\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7374 - val_loss: 1.3437\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6996 - val_loss: 1.0177\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6765 - val_loss: 0.8184\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6582 - val_loss: 0.6930\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6424 - val_loss: 0.6271\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6283 - val_loss: 0.6058\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6152 - val_loss: 0.6095\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6031 - val_loss: 0.6379\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5920 - val_loss: 0.6737\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5815 - val_loss: 0.7137\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5716 - val_loss: 0.7601\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5623 - val_loss: 0.8152\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5537 - val_loss: 0.8626\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5455 - val_loss: 0.9097\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5378 - val_loss: 0.9312\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5306 - val_loss: 0.9476\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.5756\n",
      "[CV 3/5] END learning_rate=0.0005064129375837458, n_hidden=1, n_neurons=40;, score=-0.576 total time=  10.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 3.1073 - val_loss: 7.5031\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 1.4198 - val_loss: 2.8726\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.9140 - val_loss: 1.0896\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.7538 - val_loss: 0.7151\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6977 - val_loss: 0.6635\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6709 - val_loss: 0.6482\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6517 - val_loss: 0.6269\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6352 - val_loss: 0.6115\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6204 - val_loss: 0.6011\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6070 - val_loss: 0.5887\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5945 - val_loss: 0.5726\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5830 - val_loss: 0.5564\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5721 - val_loss: 0.5430\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5621 - val_loss: 0.5326\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5527 - val_loss: 0.5232\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5440 - val_loss: 0.5165\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5360 - val_loss: 0.5065\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5287 - val_loss: 0.4935\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5218 - val_loss: 0.4853\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5154 - val_loss: 0.4800\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5095 - val_loss: 0.4749\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5041 - val_loss: 0.4685\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4990 - val_loss: 0.4638\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4943 - val_loss: 0.4589\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4899 - val_loss: 0.4530\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4859 - val_loss: 0.4498\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4821 - val_loss: 0.4466\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4785 - val_loss: 0.4460\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4753 - val_loss: 0.4410\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4721 - val_loss: 0.4417\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4693 - val_loss: 0.4389\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4666 - val_loss: 0.4334\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4639 - val_loss: 0.4319\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4616 - val_loss: 0.4320\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4593 - val_loss: 0.4286\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4571 - val_loss: 0.4295\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4551 - val_loss: 0.4297\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4531 - val_loss: 0.4299\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4513 - val_loss: 0.4275\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4496 - val_loss: 0.4284\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4479 - val_loss: 0.4250\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4463 - val_loss: 0.4234\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4448 - val_loss: 0.4192\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4433 - val_loss: 0.4199\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4419 - val_loss: 0.4217\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4405 - val_loss: 0.4263\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4392 - val_loss: 0.4175\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4379 - val_loss: 0.4221\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4367 - val_loss: 0.4249\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4355 - val_loss: 0.4249\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4343 - val_loss: 0.4224\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4331 - val_loss: 0.4187\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4321 - val_loss: 0.4140\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4310 - val_loss: 0.4154\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4299 - val_loss: 0.4141\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4290 - val_loss: 0.4181\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4279 - val_loss: 0.4187\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4271 - val_loss: 0.4224\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4261 - val_loss: 0.4238\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4252 - val_loss: 0.4222\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4244 - val_loss: 0.4129\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4235 - val_loss: 0.4113\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4227 - val_loss: 0.4084\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4219 - val_loss: 0.4152\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4211 - val_loss: 0.4128\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4202 - val_loss: 0.4145\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4196 - val_loss: 0.4105\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4188 - val_loss: 0.4068\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4179 - val_loss: 0.4090\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4173 - val_loss: 0.4167\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4165 - val_loss: 0.4128\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4159 - val_loss: 0.4176\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4152 - val_loss: 0.4172\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4145 - val_loss: 0.4069\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4138 - val_loss: 0.4189\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4133 - val_loss: 0.4201\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4126 - val_loss: 0.4161\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4120 - val_loss: 0.4147\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.3601\n",
      "[CV 4/5] END learning_rate=0.0005064129375837458, n_hidden=1, n_neurons=40;, score=-0.360 total time=  42.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 2.9771 - val_loss: 20.2760\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 1.3592 - val_loss: 9.2255\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.9205 - val_loss: 5.4492\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.7737 - val_loss: 3.0783\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.7078 - val_loss: 1.8710\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6679 - val_loss: 1.2499\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6397 - val_loss: 0.9043\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6177 - val_loss: 0.7297\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5997 - val_loss: 0.6307\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5847 - val_loss: 0.5816\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5717 - val_loss: 0.5524\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5600 - val_loss: 0.5367\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5496 - val_loss: 0.5270\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5401 - val_loss: 0.5195\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5315 - val_loss: 0.5119\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5235 - val_loss: 0.5040\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5160 - val_loss: 0.4990\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5090 - val_loss: 0.4905\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5026 - val_loss: 0.4846\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4966 - val_loss: 0.4773\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4911 - val_loss: 0.4704\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4859 - val_loss: 0.4652\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4810 - val_loss: 0.4601\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4763 - val_loss: 0.4557\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4720 - val_loss: 0.4511\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4679 - val_loss: 0.4475\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4640 - val_loss: 0.4443\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4604 - val_loss: 0.4416\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4569 - val_loss: 0.4390\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4537 - val_loss: 0.4376\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4507 - val_loss: 0.4371\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4478 - val_loss: 0.4341\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4450 - val_loss: 0.4338\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4423 - val_loss: 0.4322\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4398 - val_loss: 0.4313\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4374 - val_loss: 0.4300\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4351 - val_loss: 0.4311\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4329 - val_loss: 0.4304\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4308 - val_loss: 0.4320\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4288 - val_loss: 0.4319\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4269 - val_loss: 0.4334\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4251 - val_loss: 0.4318\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4235 - val_loss: 0.4314\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4219 - val_loss: 0.4347\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4204 - val_loss: 0.4337\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4189 - val_loss: 0.4360\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.4598\n",
      "[CV 5/5] END learning_rate=0.0005064129375837458, n_hidden=1, n_neurons=40;, score=-0.460 total time=  25.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.7954 - val_loss: 4.8954\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4593 - val_loss: 14.8722\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3908 - val_loss: 0.3359\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3773 - val_loss: 4.0784\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3890 - val_loss: 4.5169\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4210 - val_loss: 0.6731\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3397 - val_loss: 7.5806\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3557 - val_loss: 5.6458\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3674 - val_loss: 0.6024\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3292 - val_loss: 0.3551\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3169 - val_loss: 0.3038\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3114 - val_loss: 0.3059\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3051 - val_loss: 0.3064\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2983 - val_loss: 0.3786\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2996 - val_loss: 0.3050\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2954 - val_loss: 0.3127\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2917 - val_loss: 0.3189\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2894 - val_loss: 0.2973\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2886 - val_loss: 0.3098\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2843 - val_loss: 0.3106\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2803 - val_loss: 0.3066\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2783 - val_loss: 0.3109\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2776 - val_loss: 0.3237\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2759 - val_loss: 0.3760\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2753 - val_loss: 0.2948\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2736 - val_loss: 0.2913\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2731 - val_loss: 0.3334\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2691 - val_loss: 0.2803\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2694 - val_loss: 0.2830\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2669 - val_loss: 0.2861\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2673 - val_loss: 0.2994\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2668 - val_loss: 0.2988\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2642 - val_loss: 0.2848\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2634 - val_loss: 0.3393\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2633 - val_loss: 0.2741\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2616 - val_loss: 0.2743\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2591 - val_loss: 0.3220\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2594 - val_loss: 0.2803\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2573 - val_loss: 0.2919\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2561 - val_loss: 0.3051\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2543 - val_loss: 0.2944\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2588 - val_loss: 0.3098\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2569 - val_loss: 0.3380\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2529 - val_loss: 0.2756\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2538 - val_loss: 0.3069\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3001\n",
      "[CV 1/5] END learning_rate=0.014675655181032702, n_hidden=3, n_neurons=75;, score=-0.300 total time=  29.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6344 - val_loss: 0.5922\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3874 - val_loss: 3.5635\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3813 - val_loss: 2.9958\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3507 - val_loss: 4.9854\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3534 - val_loss: 17.9230\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4498 - val_loss: 0.9549\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3456 - val_loss: 2.2342\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3428 - val_loss: 7.6239\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4015 - val_loss: 0.5031\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3328 - val_loss: 1.1839\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3281 - val_loss: 1.3517\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3271 - val_loss: 0.5128\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3121 - val_loss: 0.3942\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3070 - val_loss: 0.3762\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3034 - val_loss: 0.3509\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2990 - val_loss: 0.3754\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2944 - val_loss: 0.2973\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2920 - val_loss: 0.3032\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2907 - val_loss: 0.3036\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2862 - val_loss: 0.3237\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2844 - val_loss: 0.3496\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2846 - val_loss: 0.2945\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2802 - val_loss: 0.3170\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2805 - val_loss: 0.3056\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2818 - val_loss: 0.2820\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2762 - val_loss: 0.3040\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2748 - val_loss: 0.2974\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2725 - val_loss: 0.3108\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2714 - val_loss: 0.2754\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2717 - val_loss: 0.3004\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2670 - val_loss: 0.3331\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2663 - val_loss: 0.3389\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2676 - val_loss: 0.2913\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2652 - val_loss: 0.3042\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2638 - val_loss: 0.3370\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2628 - val_loss: 0.2796\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2616 - val_loss: 0.3486\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2602 - val_loss: 0.2773\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2607 - val_loss: 0.2991\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3366\n",
      "[CV 2/5] END learning_rate=0.014675655181032702, n_hidden=3, n_neurons=75;, score=-0.337 total time=  25.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6742 - val_loss: 0.6341\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4068 - val_loss: 0.8289\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3614 - val_loss: 0.8180\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3491 - val_loss: 1.6921\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3391 - val_loss: 2.0491\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3356 - val_loss: 0.4028\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3270 - val_loss: 1.1510\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3239 - val_loss: 1.6615\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3198 - val_loss: 0.7312\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3124 - val_loss: 0.4155\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3170 - val_loss: 1.4492\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3098 - val_loss: 1.8444\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3042 - val_loss: 0.7774\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3013 - val_loss: 0.5642\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2987 - val_loss: 0.5233\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2963 - val_loss: 1.3678\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3706\n",
      "[CV 3/5] END learning_rate=0.014675655181032702, n_hidden=3, n_neurons=75;, score=-0.371 total time=  10.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.7220 - val_loss: 1.2496\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4257 - val_loss: 9.0427\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4125 - val_loss: 2.4638\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3847 - val_loss: 0.7953\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3626 - val_loss: 0.3681\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3527 - val_loss: 0.4288\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3461 - val_loss: 0.3556\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3377 - val_loss: 0.3326\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3329 - val_loss: 0.4014\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3275 - val_loss: 0.4345\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3224 - val_loss: 0.3138\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3187 - val_loss: 0.3877\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3255 - val_loss: 0.4471\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3182 - val_loss: 0.3326\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3116 - val_loss: 0.2963\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3081 - val_loss: 0.2984\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3044 - val_loss: 0.3146\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3003 - val_loss: 0.3004\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2977 - val_loss: 0.2909\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2941 - val_loss: 0.3121\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2931 - val_loss: 0.2989\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2905 - val_loss: 0.3708\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2883 - val_loss: 0.3296\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2863 - val_loss: 0.3333\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2855 - val_loss: 0.2989\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2822 - val_loss: 0.3494\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2877 - val_loss: 0.3009\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2790 - val_loss: 0.3001\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2785 - val_loss: 0.3828\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3573\n",
      "[CV 4/5] END learning_rate=0.014675655181032702, n_hidden=3, n_neurons=75;, score=-0.357 total time=  18.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 1.1648 - val_loss: 1.4510\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5099 - val_loss: 8.5602\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4257 - val_loss: 7.0820\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5787 - val_loss: 0.3645\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3557 - val_loss: 0.5536\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3403 - val_loss: 1.1690\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3349 - val_loss: 1.2670\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3302 - val_loss: 0.4892\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3120 - val_loss: 0.3051\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3109 - val_loss: 1.6712\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3183 - val_loss: 0.3138\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3021 - val_loss: 0.3418\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2977 - val_loss: 0.3086\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2929 - val_loss: 0.2941\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2911 - val_loss: 0.3868\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2885 - val_loss: 1.2536\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2939 - val_loss: 0.7700\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2872 - val_loss: 0.9129\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2865 - val_loss: 0.5093\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2819 - val_loss: 0.3189\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2779 - val_loss: 0.5281\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2789 - val_loss: 0.3098\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2759 - val_loss: 0.2802\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2747 - val_loss: 0.2784\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2708 - val_loss: 0.2996\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2693 - val_loss: 0.2779\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2687 - val_loss: 0.2739\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2666 - val_loss: 0.2997\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2658 - val_loss: 0.3003\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2632 - val_loss: 0.2758\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2619 - val_loss: 0.2972\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2624 - val_loss: 0.2862\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2599 - val_loss: 0.2877\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2582 - val_loss: 0.2682\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2596 - val_loss: 0.2927\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2570 - val_loss: 0.2717\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2547 - val_loss: 0.3309\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2529 - val_loss: 0.2869\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2522 - val_loss: 0.2682\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2516 - val_loss: 0.2809\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2520 - val_loss: 0.3189\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2514 - val_loss: 0.2768\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2498 - val_loss: 0.2777\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2492 - val_loss: 0.3496\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3852\n",
      "[CV 5/5] END learning_rate=0.014675655181032702, n_hidden=3, n_neurons=75;, score=-0.385 total time=  28.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.7797 - val_loss: 0.6286\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4792 - val_loss: 0.4173\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4354 - val_loss: 0.4056\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4157 - val_loss: 0.4067\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4020 - val_loss: 0.4131\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3938 - val_loss: 0.4031\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3842 - val_loss: 0.3912\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3795 - val_loss: 0.4192\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3742 - val_loss: 0.3759\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3701 - val_loss: 0.3661\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3649 - val_loss: 0.3709\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3636 - val_loss: 0.3768\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3596 - val_loss: 0.3871\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3552 - val_loss: 0.4072\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3525 - val_loss: 0.4229\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3496 - val_loss: 0.4269\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3468 - val_loss: 0.4150\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3448 - val_loss: 0.4633\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3428 - val_loss: 0.3607\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3384 - val_loss: 0.3969\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3392 - val_loss: 0.3255\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3372 - val_loss: 0.3550\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3347 - val_loss: 0.3590\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3307 - val_loss: 0.3678\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3299 - val_loss: 0.4079\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3282 - val_loss: 0.3169\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3266 - val_loss: 0.4249\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3247 - val_loss: 0.3639\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3213 - val_loss: 0.3607\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3192 - val_loss: 0.4051\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3178 - val_loss: 0.3420\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3148 - val_loss: 0.4046\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3138 - val_loss: 0.3596\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3117 - val_loss: 0.3530\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3095 - val_loss: 0.3492\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3079 - val_loss: 0.4072\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3534\n",
      "[CV 1/5] END learning_rate=0.009210265757774031, n_hidden=3, n_neurons=19;, score=-0.353 total time=  22.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.7569 - val_loss: 0.5522\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4392 - val_loss: 0.7131\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3979 - val_loss: 0.4809\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3809 - val_loss: 0.4370\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3691 - val_loss: 0.4049\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3636 - val_loss: 0.3613\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3567 - val_loss: 0.3784\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3533 - val_loss: 0.3720\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3489 - val_loss: 0.3798\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3434 - val_loss: 0.3820\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3394 - val_loss: 0.3336\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3366 - val_loss: 0.3374\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3318 - val_loss: 0.3366\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3289 - val_loss: 0.3220\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3254 - val_loss: 0.3542\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3231 - val_loss: 0.3546\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3213 - val_loss: 0.4520\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3197 - val_loss: 0.6678\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3211 - val_loss: 10.2605\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4746 - val_loss: 0.5683\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3370 - val_loss: 1.6996\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3346 - val_loss: 3.7102\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3602 - val_loss: 0.4813\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3182 - val_loss: 0.4119\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3793\n",
      "[CV 2/5] END learning_rate=0.009210265757774031, n_hidden=3, n_neurons=19;, score=-0.379 total time=  15.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.8216 - val_loss: 0.6138\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4715 - val_loss: 0.4501\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4195 - val_loss: 0.5142\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3968 - val_loss: 0.8518\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3862 - val_loss: 0.8177\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3773 - val_loss: 0.8380\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3692 - val_loss: 1.8524\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3617 - val_loss: 0.8087\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3559 - val_loss: 1.5141\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3563 - val_loss: 0.9155\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3494 - val_loss: 1.0396\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3446 - val_loss: 1.3601\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3978\n",
      "[CV 3/5] END learning_rate=0.009210265757774031, n_hidden=3, n_neurons=19;, score=-0.398 total time=   7.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 1.0090 - val_loss: 3.1497\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5362 - val_loss: 2.1890\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4489 - val_loss: 0.7813\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4159 - val_loss: 0.7211\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3991 - val_loss: 0.3967\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3887 - val_loss: 0.3847\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3800 - val_loss: 0.3646\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3752 - val_loss: 0.3582\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3704 - val_loss: 0.3812\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3671 - val_loss: 0.3488\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3633 - val_loss: 0.3384\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3595 - val_loss: 0.3891\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3581 - val_loss: 0.3400\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3541 - val_loss: 0.3687\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3514 - val_loss: 0.3513\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3484 - val_loss: 0.3441\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3461 - val_loss: 0.5306\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3498 - val_loss: 0.4293\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3423 - val_loss: 0.3454\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3390 - val_loss: 0.3509\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3397 - val_loss: 0.3781\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3652\n",
      "[CV 4/5] END learning_rate=0.009210265757774031, n_hidden=3, n_neurons=19;, score=-0.365 total time=  13.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 115us/sample - loss: 0.8095 - val_loss: 0.6083\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4852 - val_loss: 0.6787\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4383 - val_loss: 0.6630\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4063 - val_loss: 0.6076\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3871 - val_loss: 0.4593\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3826 - val_loss: 0.4753\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3682 - val_loss: 0.6119\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3704 - val_loss: 0.4284\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3572 - val_loss: 0.8452\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3711 - val_loss: 0.8844\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3602 - val_loss: 0.7964\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3510 - val_loss: 0.3409\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3438 - val_loss: 0.3762\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3435 - val_loss: 0.4251\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3409 - val_loss: 0.3309\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3354 - val_loss: 0.3766\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3375 - val_loss: 0.6410\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3375 - val_loss: 0.3225\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3305 - val_loss: 0.3214\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3290 - val_loss: 0.3746\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3270 - val_loss: 0.3641\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3230 - val_loss: 0.3950\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3242 - val_loss: 0.3576\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3217 - val_loss: 0.3121\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3198 - val_loss: 0.3088\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3201 - val_loss: 0.3295\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3171 - val_loss: 0.3372\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3164 - val_loss: 0.3972\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3138 - val_loss: 0.9529\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3171 - val_loss: 0.7176\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3182 - val_loss: 0.3141\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3137 - val_loss: 0.3576\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3097 - val_loss: 0.3564\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3121 - val_loss: 0.3096\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3078 - val_loss: 0.5178\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3449\n",
      "[CV 5/5] END learning_rate=0.009210265757774031, n_hidden=3, n_neurons=19;, score=-0.345 total time=  22.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.5810 - val_loss: 171.2588\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7164 - val_loss: 105.3574\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 5.7623 - val_loss: 356.7652\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 12.6714 - val_loss: 856.4097\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 4.6703 - val_loss: 1566.8163\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 32.2962 - val_loss: 3762.2657\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 37.1513 - val_loss: 8226.3338\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 22.1510 - val_loss: 25820.1127\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 543.5122 - val_loss: 56607.6645\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1036.5170 - val_loss: 127601.7382\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 143.4844 - val_loss: 268586.9884\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 329.0830 - val_loss: 640766.9640\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 2067.2921\n",
      "[CV 1/5] END learning_rate=0.01887628855032731, n_hidden=0, n_neurons=77;, score=-2067.292 total time=   6.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.6555 - val_loss: 1864.9654\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.1323 - val_loss: 955.4813\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 34.7462 - val_loss: 1597.9785\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 54.2328 - val_loss: 2760.5826\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 6.4114 - val_loss: 8663.1405\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 282.2335 - val_loss: 14673.0332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 347.0710 - val_loss: 25580.3291\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 192.9517 - val_loss: 39145.2650\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 445.6809 - val_loss: 64864.0190\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1476.4649 - val_loss: 108586.4367\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 611.5842 - val_loss: 185290.2487\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1576.0068 - val_loss: 316109.0850\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 980.1270\n",
      "[CV 2/5] END learning_rate=0.01887628855032731, n_hidden=0, n_neurons=77;, score=-980.127 total time=   6.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.8077 - val_loss: 19.9501\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5333 - val_loss: 21.3254\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5147 - val_loss: 8.4620\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5191 - val_loss: 12.7997\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5055 - val_loss: 11.7989\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5264 - val_loss: 14.7166\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5151 - val_loss: 8.7190\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5116 - val_loss: 14.5378\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5314 - val_loss: 16.6756\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5105 - val_loss: 12.7241\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5057 - val_loss: 6.3228\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5051 - val_loss: 17.7214\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5297 - val_loss: 11.8195\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5186 - val_loss: 10.2474\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5436 - val_loss: 8.8156\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5076 - val_loss: 18.3686\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5125 - val_loss: 18.0058\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5451 - val_loss: 12.9436\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5080 - val_loss: 12.3337\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5099 - val_loss: 16.5017\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5097 - val_loss: 16.3282\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 1.1507\n",
      "[CV 3/5] END learning_rate=0.01887628855032731, n_hidden=0, n_neurons=77;, score=-1.151 total time=  11.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 1.8265 - val_loss: 27.8284\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 2.3050 - val_loss: 11.1843\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7736 - val_loss: 9.8740\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5859 - val_loss: 5.0413\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.4943 - val_loss: 12.0252\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.7590 - val_loss: 24.4210\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.8387 - val_loss: 379.3138\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8260 - val_loss: 15.7672\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.6063 - val_loss: 1.2485\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8208 - val_loss: 362.0675\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.4941 - val_loss: 89.1387\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.8106 - val_loss: 190.6726\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8079 - val_loss: 32.0735\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5876 - val_loss: 9.1262\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.9267 - val_loss: 0.5983\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6528 - val_loss: 899.2886\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 2.0097 - val_loss: 188.0260\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 9.1445 - val_loss: 57.6469\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 3.3334 - val_loss: 67.4255\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.8389 - val_loss: 53.7036\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9594 - val_loss: 68.4035\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.2650 - val_loss: 38.7026\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.7856 - val_loss: 41.0206\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.9015 - val_loss: 8.5566\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6725 - val_loss: 3.1380\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 0.5115\n",
      "[CV 4/5] END learning_rate=0.01887628855032731, n_hidden=0, n_neurons=77;, score=-0.512 total time=  13.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 4.6178 - val_loss: 19.0005\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5801 - val_loss: 9.7665\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6971 - val_loss: 587.2957\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9201 - val_loss: 88.1774\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 5.6740 - val_loss: 19.2738\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6098 - val_loss: 0.6299\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.9103 - val_loss: 273.8520\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7622 - val_loss: 112.6988\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 3.5208 - val_loss: 84.8302\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 6.0239 - val_loss: 12.9592\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7280 - val_loss: 16.5897\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.7351 - val_loss: 565.8538\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.0751 - val_loss: 227.0037\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.3955 - val_loss: 1044.3980\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.5692 - val_loss: 331.6332\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.8184 - val_loss: 433.6958\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.4300\n",
      "[CV 5/5] END learning_rate=0.01887628855032731, n_hidden=0, n_neurons=77;, score=-1.430 total time=   8.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.7507 - val_loss: 0.5105\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4374 - val_loss: 2.7790\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4046 - val_loss: 7.1762\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4362 - val_loss: 0.7340\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3786 - val_loss: 2.9823\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3953 - val_loss: 0.3577\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3634 - val_loss: 0.3293\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3556 - val_loss: 0.3703\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3510 - val_loss: 0.3397\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3480 - val_loss: 0.3465\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3457 - val_loss: 0.3847\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3429 - val_loss: 0.3466\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3354 - val_loss: 0.3342\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3338 - val_loss: 0.3298\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3299 - val_loss: 0.3963\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3282 - val_loss: 0.3055\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3252 - val_loss: 0.3243\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3195 - val_loss: 0.3455\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3190 - val_loss: 0.3091\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3130 - val_loss: 0.2980\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3112 - val_loss: 0.2948\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3079 - val_loss: 0.2918\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3073 - val_loss: 0.3136\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3053 - val_loss: 0.3451\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3011 - val_loss: 0.3076\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3002 - val_loss: 0.4806\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2990 - val_loss: 0.2952\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2966 - val_loss: 0.3242\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2967 - val_loss: 0.2901\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2927 - val_loss: 0.2842\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2911 - val_loss: 0.4134\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2917 - val_loss: 0.2848\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2891 - val_loss: 0.2850\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2861 - val_loss: 0.2814\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2841 - val_loss: 0.2745\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2830 - val_loss: 0.2906\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2843 - val_loss: 0.3022\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2888 - val_loss: 0.2774\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2829 - val_loss: 0.2875\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2822 - val_loss: 0.2925\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2808 - val_loss: 0.2752\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.2798 - val_loss: 0.3818\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2785 - val_loss: 0.2740\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2754 - val_loss: 0.2943\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2761 - val_loss: 0.2911\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2733 - val_loss: 0.3210\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2740 - val_loss: 0.3488\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2730 - val_loss: 0.2988\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2718 - val_loss: 0.3236\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2709 - val_loss: 0.4150\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2709 - val_loss: 0.3404\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2704 - val_loss: 0.3052\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2705 - val_loss: 0.3053\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.2982\n",
      "[CV 1/5] END learning_rate=0.022498305658483677, n_hidden=2, n_neurons=43;, score=-0.298 total time=  31.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 0.5601 - val_loss: 0.8015\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4132 - val_loss: 0.5851\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3722 - val_loss: 3.0007\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3841 - val_loss: 0.3570\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3477 - val_loss: 0.4123\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3368 - val_loss: 0.3225\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3302 - val_loss: 0.3803\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3264 - val_loss: 0.3538\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3275 - val_loss: 0.7809\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3253 - val_loss: 0.4844\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3173 - val_loss: 0.6635\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3134 - val_loss: 0.3599\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3072 - val_loss: 0.6824\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3039 - val_loss: 0.7463\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3063 - val_loss: 0.3306\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2996 - val_loss: 0.3591\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4151\n",
      "[CV 2/5] END learning_rate=0.022498305658483677, n_hidden=2, n_neurons=43;, score=-0.415 total time=   9.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 110us/sample - loss: 0.5621 - val_loss: 0.6560\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4086 - val_loss: 1.9200\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3729 - val_loss: 2.7978\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3600 - val_loss: 3.2645\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3534 - val_loss: 3.0380\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3455 - val_loss: 1.5495\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3366 - val_loss: 1.7202\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3369 - val_loss: 2.2406\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3317 - val_loss: 1.4120\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3245 - val_loss: 0.3740\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3213 - val_loss: 4.0475\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3218 - val_loss: 1.2802\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3206 - val_loss: 0.4252\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3127 - val_loss: 1.0505\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3117 - val_loss: 0.4030\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3097 - val_loss: 0.6778\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3087 - val_loss: 1.1612\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3072 - val_loss: 0.4243\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3040 - val_loss: 1.4623\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3035 - val_loss: 11.6655\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.8624\n",
      "[CV 3/5] END learning_rate=0.022498305658483677, n_hidden=2, n_neurons=43;, score=-0.862 total time=  12.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 0.6599 - val_loss: 8.6321\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4707 - val_loss: 22.3342\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 1.4551 - val_loss: 0.6775\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4030 - val_loss: 1.7968\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3700 - val_loss: 0.9123\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3608 - val_loss: 0.7095\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3597 - val_loss: 1.1308\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3418 - val_loss: 0.5674\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3420 - val_loss: 0.3134\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3309 - val_loss: 0.2947\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3246 - val_loss: 0.3370\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3221 - val_loss: 0.3600\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3176 - val_loss: 0.3203\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3172 - val_loss: 0.3166\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3167 - val_loss: 0.3074\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3109 - val_loss: 0.3894\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3114 - val_loss: 0.3176\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3083 - val_loss: 0.3158\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3075 - val_loss: 0.3220\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3053 - val_loss: 0.2799\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3024 - val_loss: 0.2907\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3029 - val_loss: 0.2851\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3040 - val_loss: 0.3323\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3079 - val_loss: 0.3485\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2982 - val_loss: 0.2863\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2992 - val_loss: 0.2986\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2948 - val_loss: 0.2973\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2953 - val_loss: 0.3200\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2933 - val_loss: 0.3753\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2932 - val_loss: 0.3338\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.2923\n",
      "[CV 4/5] END learning_rate=0.022498305658483677, n_hidden=2, n_neurons=43;, score=-0.292 total time=  17.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 0.6127 - val_loss: 1.7586\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4280 - val_loss: 4.7189\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4513 - val_loss: 0.5375\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3909 - val_loss: 0.4464\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3670 - val_loss: 0.5450\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3581 - val_loss: 0.4530\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3535 - val_loss: 0.6087\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3457 - val_loss: 0.7627\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3437 - val_loss: 0.3596\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3369 - val_loss: 0.3320\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3324 - val_loss: 0.3667\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3277 - val_loss: 0.3184\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3256 - val_loss: 0.3491\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3218 - val_loss: 0.4577\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3219 - val_loss: 0.3703\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3153 - val_loss: 0.3479\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3196 - val_loss: 0.4070\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3161 - val_loss: 0.4948\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3116 - val_loss: 0.3911\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3094 - val_loss: 0.3376\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3063 - val_loss: 0.2939\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3026 - val_loss: 0.3036\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3006 - val_loss: 0.3235\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3010 - val_loss: 0.3092\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3001 - val_loss: 0.2918\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2980 - val_loss: 0.3391\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2932 - val_loss: 0.3683\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2923 - val_loss: 0.3164\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2919 - val_loss: 0.2876\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2897 - val_loss: 0.3374\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2895 - val_loss: 0.3011\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2851 - val_loss: 0.3170\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2856 - val_loss: 0.2868\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2837 - val_loss: 0.2905\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2826 - val_loss: 0.3037\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2822 - val_loss: 0.2849\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2810 - val_loss: 0.2860\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2777 - val_loss: 0.3302\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2805 - val_loss: 0.3110\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2786 - val_loss: 0.4208\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2825 - val_loss: 0.2942\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2760 - val_loss: 0.6444\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2793 - val_loss: 0.4466\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2761 - val_loss: 0.5405\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2778 - val_loss: 1.2463\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.2796 - val_loss: 0.7796\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3419\n",
      "[CV 5/5] END learning_rate=0.022498305658483677, n_hidden=2, n_neurons=43;, score=-0.342 total time=  27.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.3254 - val_loss: 8.0940\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8385 - val_loss: 0.6646\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6704 - val_loss: 0.5949\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6230 - val_loss: 0.5538\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5842 - val_loss: 0.5180\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5504 - val_loss: 0.4893\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5214 - val_loss: 0.4642\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4983 - val_loss: 0.4468\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4772 - val_loss: 0.4285\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4592 - val_loss: 0.4185\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4442 - val_loss: 0.4092\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4325 - val_loss: 0.4220\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4220 - val_loss: 0.4043\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4133 - val_loss: 0.4133\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4050 - val_loss: 0.4147\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3995 - val_loss: 0.4192\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3928 - val_loss: 0.4188\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3884 - val_loss: 0.4154\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3846 - val_loss: 0.3887\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3805 - val_loss: 0.4007\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3780 - val_loss: 0.3975\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3744 - val_loss: 0.3995\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3716 - val_loss: 0.4006\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3691 - val_loss: 0.4103\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3668 - val_loss: 0.3800\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3650 - val_loss: 0.3980\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3625 - val_loss: 0.3830\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3615 - val_loss: 0.3767\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3595 - val_loss: 0.3817\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3577 - val_loss: 0.3990\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3564 - val_loss: 0.3924\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3547 - val_loss: 0.3764\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3535 - val_loss: 0.3606\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3515 - val_loss: 0.3739\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3510 - val_loss: 0.3857\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3495 - val_loss: 0.3679\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3483 - val_loss: 0.3576\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3471 - val_loss: 0.3550\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3462 - val_loss: 0.3769\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3454 - val_loss: 0.3676\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3439 - val_loss: 0.3663\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3432 - val_loss: 0.3657\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3422 - val_loss: 0.3612\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3414 - val_loss: 0.3499\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3404 - val_loss: 0.3495\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3395 - val_loss: 0.3648\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3385 - val_loss: 0.3690\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3380 - val_loss: 0.3475\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3374 - val_loss: 0.3428\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3362 - val_loss: 0.3439\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3357 - val_loss: 0.3545\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3349 - val_loss: 0.3587\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3345 - val_loss: 0.3598\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3335 - val_loss: 0.3411\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3331 - val_loss: 0.3325\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3321 - val_loss: 0.3635\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3319 - val_loss: 0.3540\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3312 - val_loss: 0.3407\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3306 - val_loss: 0.3371\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3300 - val_loss: 0.3429\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3288 - val_loss: 0.3324\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3284 - val_loss: 0.3411\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3283 - val_loss: 0.3331\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3275 - val_loss: 0.3292\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3270 - val_loss: 0.3483\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3267 - val_loss: 0.3352\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3259 - val_loss: 0.3277\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3256 - val_loss: 0.3314\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3250 - val_loss: 0.3400\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3241 - val_loss: 0.3423\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3235 - val_loss: 0.3275\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3233 - val_loss: 0.3175\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3224 - val_loss: 0.3290\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3222 - val_loss: 0.3190\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3216 - val_loss: 0.3143\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3209 - val_loss: 0.3196\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3206 - val_loss: 0.3155\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3200 - val_loss: 0.3224\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3197 - val_loss: 0.3137\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3189 - val_loss: 0.3129\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3186 - val_loss: 0.3235\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3175 - val_loss: 0.3155\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3173 - val_loss: 0.3264\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3167 - val_loss: 0.3152\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3162 - val_loss: 0.3220\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3160 - val_loss: 0.3153\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3153 - val_loss: 0.3131\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3151 - val_loss: 0.3504\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3148 - val_loss: 0.3117\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3143 - val_loss: 0.3093\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3139 - val_loss: 0.3232\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3134 - val_loss: 0.3404\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3130 - val_loss: 0.3100\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3125 - val_loss: 0.3279\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3123 - val_loss: 0.3055\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3118 - val_loss: 0.3005\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3113 - val_loss: 0.3022\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3105 - val_loss: 0.3089\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3103 - val_loss: 0.2973\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3102 - val_loss: 0.3028\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3050\n",
      "[CV 1/5] END learning_rate=0.0011294997042702987, n_hidden=3, n_neurons=50;, score=-0.305 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 2.1495 - val_loss: 1.1095\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7707 - val_loss: 0.8315\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6320 - val_loss: 0.5791\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5747 - val_loss: 0.5445\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5361 - val_loss: 0.5244\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5061 - val_loss: 0.4820\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4803 - val_loss: 0.4659\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4605 - val_loss: 0.4508\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4435 - val_loss: 0.4234\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4303 - val_loss: 0.4107\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4192 - val_loss: 0.4108\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4105 - val_loss: 0.3931\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4026 - val_loss: 0.3871\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3962 - val_loss: 0.3957\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3913 - val_loss: 0.3812\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3863 - val_loss: 0.3763\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3815 - val_loss: 0.4016\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3778 - val_loss: 0.3708\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3745 - val_loss: 0.3829\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3708 - val_loss: 0.3713\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3681 - val_loss: 0.3871\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3659 - val_loss: 0.3660\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3629 - val_loss: 0.3490\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3608 - val_loss: 0.3889\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3589 - val_loss: 0.3510\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3567 - val_loss: 0.3805\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3549 - val_loss: 0.3482\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3534 - val_loss: 0.3710\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3515 - val_loss: 0.3485\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3501 - val_loss: 0.3387\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3491 - val_loss: 0.3720\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3477 - val_loss: 0.3405\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3466 - val_loss: 0.3872\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3452 - val_loss: 0.3429\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3441 - val_loss: 0.3638\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3433 - val_loss: 0.3364\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3417 - val_loss: 0.3795\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3417 - val_loss: 0.3403\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3401 - val_loss: 0.3354\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3391 - val_loss: 0.3352\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3379 - val_loss: 0.3813\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3377 - val_loss: 0.3486\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3366 - val_loss: 0.3492\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3359 - val_loss: 0.3665\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3352 - val_loss: 0.3272\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3342 - val_loss: 0.3853\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3335 - val_loss: 0.3254\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3328 - val_loss: 0.3250\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3318 - val_loss: 0.3614\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3315 - val_loss: 0.3268\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3303 - val_loss: 0.3674\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3302 - val_loss: 0.3474\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3294 - val_loss: 0.3293\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3284 - val_loss: 0.3617\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3282 - val_loss: 0.3391\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3272 - val_loss: 0.3288\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3265 - val_loss: 0.4031\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3264 - val_loss: 0.3233\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3256 - val_loss: 0.3654\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3249 - val_loss: 0.3385\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3244 - val_loss: 0.3334\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3238 - val_loss: 0.3990\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3239 - val_loss: 0.3219\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3228 - val_loss: 0.3609\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3225 - val_loss: 0.3282\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3221 - val_loss: 0.3638\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3213 - val_loss: 0.3152\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3204 - val_loss: 0.3420\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3201 - val_loss: 0.3190\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3197 - val_loss: 0.3587\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3192 - val_loss: 0.3503\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3189 - val_loss: 0.3888\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3188 - val_loss: 0.3130\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3176 - val_loss: 0.4199\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3183 - val_loss: 0.3463\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3169 - val_loss: 0.3860\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3164 - val_loss: 0.3127\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3158 - val_loss: 0.3263\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3148 - val_loss: 0.3205\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3148 - val_loss: 0.3734\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3145 - val_loss: 0.3145\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3139 - val_loss: 0.5277\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3152 - val_loss: 0.3200\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3133 - val_loss: 0.4948\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3144 - val_loss: 0.3352\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3124 - val_loss: 0.4095\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3121 - val_loss: 0.3309\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3614\n",
      "[CV 2/5] END learning_rate=0.0011294997042702987, n_hidden=3, n_neurons=50;, score=-0.361 total time=  55.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.2712 - val_loss: 17.6083\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.9245 - val_loss: 9.4677\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7836 - val_loss: 5.0040\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7127 - val_loss: 2.3675\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6597 - val_loss: 1.2580\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6159 - val_loss: 0.7238\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5790 - val_loss: 0.5602\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5477 - val_loss: 0.5080\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5211 - val_loss: 0.5250\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4993 - val_loss: 0.5864\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4816 - val_loss: 0.6365\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4662 - val_loss: 0.5868\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4535 - val_loss: 0.5846\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4421 - val_loss: 0.5586\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4326 - val_loss: 0.5213\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4238 - val_loss: 0.4779\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4161 - val_loss: 0.4428\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4094 - val_loss: 0.4076\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4035 - val_loss: 0.3810\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3983 - val_loss: 0.3754\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3933 - val_loss: 0.3797\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3889 - val_loss: 0.4001\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3850 - val_loss: 0.4211\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3813 - val_loss: 0.4656\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3777 - val_loss: 0.4969\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3743 - val_loss: 0.5756\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3713 - val_loss: 0.6346\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3683 - val_loss: 0.7240\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3656 - val_loss: 0.7036\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3636 - val_loss: 0.8406\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3964\n",
      "[CV 3/5] END learning_rate=0.0011294997042702987, n_hidden=3, n_neurons=50;, score=-0.396 total time=  19.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.3378 - val_loss: 1.0022\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8506 - val_loss: 0.8050\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6957 - val_loss: 0.6224\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6376 - val_loss: 0.5707\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5957 - val_loss: 0.5421\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5597 - val_loss: 0.5300\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5300 - val_loss: 0.4706\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5033 - val_loss: 0.4488\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4816 - val_loss: 0.4259\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4639 - val_loss: 0.4145\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4491 - val_loss: 0.4018\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4376 - val_loss: 0.3976\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4277 - val_loss: 0.3803\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4198 - val_loss: 0.3849\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4140 - val_loss: 0.3695\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4082 - val_loss: 0.3791\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4042 - val_loss: 0.3699\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3996 - val_loss: 0.3584\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3964 - val_loss: 0.3594\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3929 - val_loss: 0.3580\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3902 - val_loss: 0.3519\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3869 - val_loss: 0.3561\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3839 - val_loss: 0.3753\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3821 - val_loss: 0.3849\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3800 - val_loss: 0.3558\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3784 - val_loss: 0.3462\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3761 - val_loss: 0.3395\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3741 - val_loss: 0.3389\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3725 - val_loss: 0.3477\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3708 - val_loss: 0.3460\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3687 - val_loss: 0.3435\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3678 - val_loss: 0.3498\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3663 - val_loss: 0.3390\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3650 - val_loss: 0.3352\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3634 - val_loss: 0.3456\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3623 - val_loss: 0.3362\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3609 - val_loss: 0.3433\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3593 - val_loss: 0.3643\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3586 - val_loss: 0.3544\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3574 - val_loss: 0.3272\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3566 - val_loss: 0.3239\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3553 - val_loss: 0.3429\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3543 - val_loss: 0.3774\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3538 - val_loss: 0.3284\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3518 - val_loss: 0.3554\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3514 - val_loss: 0.3432\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3505 - val_loss: 0.3194\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3500 - val_loss: 0.3430\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3489 - val_loss: 0.3214\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3482 - val_loss: 0.3187\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3472 - val_loss: 0.3184\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3463 - val_loss: 0.3393\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3458 - val_loss: 0.3208\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3448 - val_loss: 0.3492\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3442 - val_loss: 0.3287\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3438 - val_loss: 0.3153\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3428 - val_loss: 0.3250\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3420 - val_loss: 0.3168\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3411 - val_loss: 0.3333\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3408 - val_loss: 0.3134\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3400 - val_loss: 0.3207\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3394 - val_loss: 0.3121\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3385 - val_loss: 0.3242\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3378 - val_loss: 0.3335\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3376 - val_loss: 0.3111\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3366 - val_loss: 0.3116\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3359 - val_loss: 0.3120\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3355 - val_loss: 0.3286\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3352 - val_loss: 0.3286\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3343 - val_loss: 0.3094\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3341 - val_loss: 0.3178\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3334 - val_loss: 0.3074\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3324 - val_loss: 0.3146\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3321 - val_loss: 0.3102\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3315 - val_loss: 0.3220\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3306 - val_loss: 0.3140\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3304 - val_loss: 0.3061\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3294 - val_loss: 0.3061\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3289 - val_loss: 0.3105\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3287 - val_loss: 0.3080\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3284 - val_loss: 0.3162\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3275 - val_loss: 0.3044\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3268 - val_loss: 0.3103\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3266 - val_loss: 0.3069\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3259 - val_loss: 0.3072\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3260 - val_loss: 0.3049\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3249 - val_loss: 0.3024\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3246 - val_loss: 0.3026\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3238 - val_loss: 0.3188\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3234 - val_loss: 0.3066\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3233 - val_loss: 0.3154\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3226 - val_loss: 0.3030\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3217 - val_loss: 0.3044\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3218 - val_loss: 0.2995\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3208 - val_loss: 0.3013\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3208 - val_loss: 0.3093\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3200 - val_loss: 0.3056\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3197 - val_loss: 0.2987\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3186 - val_loss: 0.3093\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3189 - val_loss: 0.3087\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.2844\n",
      "[CV 4/5] END learning_rate=0.0011294997042702987, n_hidden=3, n_neurons=50;, score=-0.284 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 2.2602 - val_loss: 0.9695\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7627 - val_loss: 0.6781\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6032 - val_loss: 0.5713\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5417 - val_loss: 0.5063\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5022 - val_loss: 0.4749\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4743 - val_loss: 0.4489\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4537 - val_loss: 0.4312\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4382 - val_loss: 0.4204\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4274 - val_loss: 0.4120\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4181 - val_loss: 0.4168\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4095 - val_loss: 0.3999\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4025 - val_loss: 0.3952\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3968 - val_loss: 0.3839\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3906 - val_loss: 0.3871\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3852 - val_loss: 0.3841\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3818 - val_loss: 0.3731\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3768 - val_loss: 0.3921\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3727 - val_loss: 0.3948\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3696 - val_loss: 0.3675\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3667 - val_loss: 0.3770\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3633 - val_loss: 0.3863\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3614 - val_loss: 0.3716\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3589 - val_loss: 0.3666\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3565 - val_loss: 0.3630\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3544 - val_loss: 0.3485\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3524 - val_loss: 0.3592\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3509 - val_loss: 0.3739\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3491 - val_loss: 0.3812\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3472 - val_loss: 0.3442\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3461 - val_loss: 0.3731\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3444 - val_loss: 0.3693\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3428 - val_loss: 0.3441\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3411 - val_loss: 0.3545\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3407 - val_loss: 0.3793\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3390 - val_loss: 0.3526\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3375 - val_loss: 0.3480\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3363 - val_loss: 0.3272\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3349 - val_loss: 0.3448\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3341 - val_loss: 0.3298\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3332 - val_loss: 0.3388\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3321 - val_loss: 0.3541\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3308 - val_loss: 0.3397\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3300 - val_loss: 0.3286\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3291 - val_loss: 0.3350\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3280 - val_loss: 0.3217\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3275 - val_loss: 0.3260\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3263 - val_loss: 0.3440\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3251 - val_loss: 0.3298\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3250 - val_loss: 0.3184\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3238 - val_loss: 0.3342\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3228 - val_loss: 0.3560\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3223 - val_loss: 0.3195\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3218 - val_loss: 0.3478\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3209 - val_loss: 0.3592\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3205 - val_loss: 0.3162\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3194 - val_loss: 0.3320\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3188 - val_loss: 0.3147\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3182 - val_loss: 0.3389\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3179 - val_loss: 0.3250\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3174 - val_loss: 0.3213\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3160 - val_loss: 0.3421\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3159 - val_loss: 0.3582\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3154 - val_loss: 0.3487\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3150 - val_loss: 0.3143\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3140 - val_loss: 0.3258\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3138 - val_loss: 0.3147\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3132 - val_loss: 0.3157\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3125 - val_loss: 0.3141\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3117 - val_loss: 0.3445\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3114 - val_loss: 0.3483\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3114 - val_loss: 0.3110\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3108 - val_loss: 0.3111\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3099 - val_loss: 0.3411\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3096 - val_loss: 0.3084\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3093 - val_loss: 0.3103\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3086 - val_loss: 0.3412\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3084 - val_loss: 0.3240\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3082 - val_loss: 0.3059\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3072 - val_loss: 0.3091\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3071 - val_loss: 0.3073\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3064 - val_loss: 0.3164\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3060 - val_loss: 0.3070\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3059 - val_loss: 0.3114\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3053 - val_loss: 0.3107\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3047 - val_loss: 0.3039\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3044 - val_loss: 0.3348\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3044 - val_loss: 0.3179\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3037 - val_loss: 0.3248\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3030 - val_loss: 0.3247\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3026 - val_loss: 0.3197\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3022 - val_loss: 0.3036\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3018 - val_loss: 0.3471\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3020 - val_loss: 0.3058\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3012 - val_loss: 0.3042\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3009 - val_loss: 0.3259\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3007 - val_loss: 0.3029\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3004 - val_loss: 0.3029\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2997 - val_loss: 0.3007\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2992 - val_loss: 0.2998\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2991 - val_loss: 0.2992\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3392\n",
      "[CV 5/5] END learning_rate=0.0011294997042702987, n_hidden=3, n_neurons=50;, score=-0.339 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6409 - val_loss: 3.8388\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4421 - val_loss: 4.2831\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5401 - val_loss: 9.5026\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4663 - val_loss: 14.9370\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4010 - val_loss: 1.0179\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3555 - val_loss: 3.4465\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3501 - val_loss: 1.0006\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3442 - val_loss: 3.3966\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3348 - val_loss: 1.0417\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3289 - val_loss: 0.3487\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3171 - val_loss: 0.3190\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3106 - val_loss: 0.3982\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3054 - val_loss: 0.3048\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3016 - val_loss: 0.2995\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2990 - val_loss: 0.2925\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2950 - val_loss: 0.3317\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2918 - val_loss: 0.2838\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2906 - val_loss: 0.3018\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2880 - val_loss: 0.3250\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2864 - val_loss: 0.4351\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2865 - val_loss: 0.2934\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2829 - val_loss: 0.2935\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2827 - val_loss: 0.3531\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2778 - val_loss: 0.2762\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2753 - val_loss: 0.3400\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2750 - val_loss: 0.3207\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2751 - val_loss: 0.3236\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2711 - val_loss: 0.3413\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2701 - val_loss: 0.2906\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2680 - val_loss: 0.3140\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2689 - val_loss: 0.2829\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2648 - val_loss: 0.2787\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2625 - val_loss: 0.2914\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2636 - val_loss: 0.2740\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2633 - val_loss: 0.2719\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2603 - val_loss: 0.2988\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2598 - val_loss: 0.3113\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2585 - val_loss: 0.2726\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2590 - val_loss: 0.2811\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2579 - val_loss: 0.2962\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2579 - val_loss: 0.3199\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2553 - val_loss: 0.3517\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2569 - val_loss: 0.2781\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2539 - val_loss: 0.2678\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2528 - val_loss: 0.3050\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2517 - val_loss: 0.2742\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2511 - val_loss: 0.3367\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2535 - val_loss: 0.2847\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2481 - val_loss: 0.3047\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2503 - val_loss: 0.3159\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2509 - val_loss: 0.3754\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2489 - val_loss: 0.2681\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2471 - val_loss: 0.2703\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2479 - val_loss: 0.2606\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2456 - val_loss: 0.2832\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2457 - val_loss: 0.4015\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2471 - val_loss: 0.2666\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2442 - val_loss: 0.2837\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2400 - val_loss: 0.3021\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2428 - val_loss: 0.2721\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2399 - val_loss: 0.2668\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2451 - val_loss: 0.3037\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2407 - val_loss: 0.2799\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2412 - val_loss: 0.2657\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.2620\n",
      "[CV 1/5] END learning_rate=0.01944764650574291, n_hidden=3, n_neurons=61;, score=-0.262 total time=  40.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.5947 - val_loss: 5.6163\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4296 - val_loss: 3.2682\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4011 - val_loss: 1.5764\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4075 - val_loss: 1.3287\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3490 - val_loss: 4.6899\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3512 - val_loss: 2.4327\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3443 - val_loss: 1.1966\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3303 - val_loss: 0.3686\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3140 - val_loss: 0.3086\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3099 - val_loss: 0.3384\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3031 - val_loss: 0.3336\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3020 - val_loss: 0.3154\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2970 - val_loss: 0.3303\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2912 - val_loss: 0.3370\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2891 - val_loss: 0.3278\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2865 - val_loss: 0.4149\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2831 - val_loss: 0.3291\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2830 - val_loss: 0.3486\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2792 - val_loss: 0.3089\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.3487\n",
      "[CV 2/5] END learning_rate=0.01944764650574291, n_hidden=3, n_neurons=61;, score=-0.349 total time=  12.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.6215 - val_loss: 0.5384\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3951 - val_loss: 0.5035\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3673 - val_loss: 1.4527\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3561 - val_loss: 2.5663\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3431 - val_loss: 1.3286\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3424 - val_loss: 1.2541\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3501 - val_loss: 0.5076\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3261 - val_loss: 1.7840\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3236 - val_loss: 1.0968\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3149 - val_loss: 0.2931\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3148 - val_loss: 0.3106\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3096 - val_loss: 0.9560\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3058 - val_loss: 0.3876\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3026 - val_loss: 0.3187\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2992 - val_loss: 0.3075\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2938 - val_loss: 0.3087\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2935 - val_loss: 0.3584\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2882 - val_loss: 0.4435\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.2864 - val_loss: 0.3084\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2849 - val_loss: 1.1535\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3933\n",
      "[CV 3/5] END learning_rate=0.01944764650574291, n_hidden=3, n_neurons=61;, score=-0.393 total time=  12.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.6461 - val_loss: 6.0771\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4335 - val_loss: 0.4287\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4227 - val_loss: 0.6230\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3805 - val_loss: 3.4628\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3810 - val_loss: 0.5057\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3560 - val_loss: 0.4430\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3694 - val_loss: 0.9402\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3584 - val_loss: 0.7100\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3489 - val_loss: 0.3424\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3425 - val_loss: 0.3429\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3352 - val_loss: 0.3442\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3269 - val_loss: 0.3636\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3253 - val_loss: 0.3419\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3197 - val_loss: 0.3174\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3159 - val_loss: 0.2932\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3124 - val_loss: 0.3999\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3109 - val_loss: 0.3267\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3067 - val_loss: 0.3145\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3020 - val_loss: 0.3334\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2998 - val_loss: 0.3329\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2952 - val_loss: 0.3836\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2969 - val_loss: 0.5585\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2961 - val_loss: 0.3338\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2934 - val_loss: 0.3596\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2919 - val_loss: 0.3102\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.2697\n",
      "[CV 4/5] END learning_rate=0.01944764650574291, n_hidden=3, n_neurons=61;, score=-0.270 total time=  16.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6168 - val_loss: 1.5949\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4529 - val_loss: 1.4561\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4093 - val_loss: 0.7966\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3672 - val_loss: 0.6766\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3480 - val_loss: 0.6241\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3344 - val_loss: 0.3702\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3306 - val_loss: 0.4434\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3208 - val_loss: 0.3742\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3150 - val_loss: 0.3629\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3111 - val_loss: 0.3325\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3047 - val_loss: 0.4486\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3025 - val_loss: 0.3821\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2998 - val_loss: 0.3173\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2984 - val_loss: 0.3628\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2944 - val_loss: 0.3774\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2915 - val_loss: 0.2921\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2887 - val_loss: 0.3068\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2874 - val_loss: 0.3042\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2846 - val_loss: 0.3306\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2844 - val_loss: 0.3362\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2829 - val_loss: 0.2887\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2809 - val_loss: 0.3344\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2795 - val_loss: 0.4563\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2947 - val_loss: 0.2834\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2805 - val_loss: 0.3633\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2754 - val_loss: 0.2844\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2735 - val_loss: 0.3237\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2720 - val_loss: 0.3310\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2710 - val_loss: 0.2886\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2677 - val_loss: 0.2826\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2642 - val_loss: 0.2963\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2637 - val_loss: 0.3993\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2653 - val_loss: 0.3336\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2639 - val_loss: 0.2867\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2643 - val_loss: 0.3197\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2616 - val_loss: 0.2685\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2574 - val_loss: 0.2664\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2590 - val_loss: 0.2922\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2571 - val_loss: 0.2895\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2549 - val_loss: 0.3501\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2556 - val_loss: 0.2621\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2553 - val_loss: 0.2925\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2557 - val_loss: 0.2702\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2531 - val_loss: 0.3147\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2539 - val_loss: 0.2887\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2503 - val_loss: 0.2875\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2486 - val_loss: 0.2702\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2505 - val_loss: 0.3124\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2483 - val_loss: 0.3190\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2477 - val_loss: 0.2749\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2465 - val_loss: 0.3956\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4018\n",
      "[CV 5/5] END learning_rate=0.01944764650574291, n_hidden=3, n_neurons=61;, score=-0.402 total time=  32.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.7734 - val_loss: 278.5196\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.9915 - val_loss: 231.6061\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 3.2940 - val_loss: 1544.9020\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 44.6086 - val_loss: 5490.5488\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 135.5238 - val_loss: 17681.4202\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 150.4948 - val_loss: 56326.9908\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 159.3080 - val_loss: 198638.6023\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 4284.7779 - val_loss: 641945.0521\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1507.6998 - val_loss: 2172918.7445\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 13488.2867 - val_loss: 6852451.5285\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 49738.3153 - val_loss: 22487685.5620\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 49072.5521 - val_loss: 75205825.3868\n",
      "2322/2322 [==============================] - 0s 133us/sample - loss: 243729.1155\n",
      "[CV 1/5] END learning_rate=0.011974974144440249, n_hidden=0, n_neurons=91;, score=-243729.115 total time=   6.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.0596 - val_loss: 42.8286\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 2.0856 - val_loss: 392.3946\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.4520 - val_loss: 764.1115\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 4.0264 - val_loss: 3297.5619\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 14.1283 - val_loss: 8725.2649\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 44.8324 - val_loss: 27718.6713\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 153.9134 - val_loss: 78526.1963\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 414.1170 - val_loss: 233806.3072\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 7330.1588 - val_loss: 700590.1363\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 3151.5365 - val_loss: 2119069.9286\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 67373.9756 - val_loss: 6254959.2330\n",
      "2322/2322 [==============================] - 0s 27us/sample - loss: 19432.5838\n",
      "[CV 2/5] END learning_rate=0.011974974144440249, n_hidden=0, n_neurons=91;, score=-19432.584 total time=   5.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.2201 - val_loss: 20.0735\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5211 - val_loss: 20.3434\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5096 - val_loss: 19.2871\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5093 - val_loss: 15.6582\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5071 - val_loss: 17.1438\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5097 - val_loss: 16.5583\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5281 - val_loss: 17.5629\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5185 - val_loss: 16.2510\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5226 - val_loss: 11.2933\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5109 - val_loss: 13.0049\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 0.5175 - val_loss: 10.6179\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5052 - val_loss: 10.3963\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5181 - val_loss: 12.7416\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5070 - val_loss: 9.6506\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5117 - val_loss: 12.8602\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5010 - val_loss: 17.0297\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5061 - val_loss: 12.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5022 - val_loss: 16.6978\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5114 - val_loss: 11.7177\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5053 - val_loss: 13.4308\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5138 - val_loss: 12.2248\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5389 - val_loss: 17.7085\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5110 - val_loss: 11.9335\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5028 - val_loss: 9.0014\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5203 - val_loss: 14.6367\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5175 - val_loss: 10.5687\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5051 - val_loss: 12.1284\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5061 - val_loss: 11.3000\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5053 - val_loss: 11.0390\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5133 - val_loss: 16.1845\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5257 - val_loss: 10.3843\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5207 - val_loss: 17.4725\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5124 - val_loss: 16.1658\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5140 - val_loss: 12.7514\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.0016\n",
      "[CV 3/5] END learning_rate=0.011974974144440249, n_hidden=0, n_neurons=91;, score=-1.002 total time=  17.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.7776 - val_loss: 194.7069\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8249 - val_loss: 126.6210\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.2547 - val_loss: 310.7896\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.4856 - val_loss: 181.8158\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 3.5437 - val_loss: 293.2038\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.0326 - val_loss: 192.7192\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 6.8029 - val_loss: 211.6795\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 5.2064 - val_loss: 333.0969\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 9.9270 - val_loss: 403.1574\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 1.2196 - val_loss: 310.3292\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 2.1854 - val_loss: 758.4039\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 26.6727 - val_loss: 1234.4144\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 1.1665\n",
      "[CV 4/5] END learning_rate=0.011974974144440249, n_hidden=0, n_neurons=91;, score=-1.166 total time=   6.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.4520 - val_loss: 28.9740\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.8844 - val_loss: 50.4437\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.6933 - val_loss: 417.1258\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.9845 - val_loss: 140.0199\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 1.1755 - val_loss: 609.4393\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 13.3340 - val_loss: 1035.3818\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 4.9276 - val_loss: 2011.2174\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 29.6161 - val_loss: 2725.6927\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 17.5139 - val_loss: 4217.8221\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 32.1969 - val_loss: 5566.9951\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 10.2606 - val_loss: 10254.4670\n",
      "2322/2322 [==============================] - 0s 28us/sample - loss: 14.3087\n",
      "[CV 5/5] END learning_rate=0.011974974144440249, n_hidden=0, n_neurons=91;, score=-14.309 total time=   5.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.9978 - val_loss: 2.1732\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5477 - val_loss: 0.9695\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4621 - val_loss: 0.4053\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4260 - val_loss: 0.4232\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4057 - val_loss: 0.4848\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3929 - val_loss: 0.3980\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3829 - val_loss: 0.4067\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3756 - val_loss: 0.3878\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3701 - val_loss: 0.4244\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3637 - val_loss: 0.3847\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3602 - val_loss: 0.3918\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3542 - val_loss: 0.4139\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3520 - val_loss: 0.3925\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3487 - val_loss: 0.3966\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3459 - val_loss: 0.3441\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3429 - val_loss: 0.3862\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3406 - val_loss: 0.4158\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3380 - val_loss: 0.3998\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3356 - val_loss: 0.3710\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3335 - val_loss: 0.3303\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3312 - val_loss: 0.4097\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3303 - val_loss: 0.3181\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3277 - val_loss: 0.5082\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3275 - val_loss: 0.3366\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3245 - val_loss: 0.3394\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3220 - val_loss: 0.3071\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3214 - val_loss: 0.3391\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3191 - val_loss: 0.3078\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3177 - val_loss: 0.3998\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3164 - val_loss: 0.3316\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3148 - val_loss: 0.3171\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3129 - val_loss: 0.4807\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3134 - val_loss: 0.3196\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3103 - val_loss: 0.5113\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3111 - val_loss: 0.3439\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3087 - val_loss: 0.3968\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3018\n",
      "[CV 1/5] END learning_rate=0.003304044545722164, n_hidden=3, n_neurons=80;, score=-0.302 total time=  23.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 1.0787 - val_loss: 4.6024\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6020 - val_loss: 0.4817\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4717 - val_loss: 0.4250\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4214 - val_loss: 0.3849\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3943 - val_loss: 0.3714\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3772 - val_loss: 0.3712\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3666 - val_loss: 0.3511\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3587 - val_loss: 0.3632\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3528 - val_loss: 0.3409\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3477 - val_loss: 0.3497\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3427 - val_loss: 0.3455\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3394 - val_loss: 0.3476\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3360 - val_loss: 0.3401\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3329 - val_loss: 0.3262\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3300 - val_loss: 0.3510\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3275 - val_loss: 0.3232\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3246 - val_loss: 0.3319\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3224 - val_loss: 0.3230\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3204 - val_loss: 0.3589\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3187 - val_loss: 0.3278\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3172 - val_loss: 0.3285\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3150 - val_loss: 0.3149\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3128 - val_loss: 0.3307\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3120 - val_loss: 0.3271\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3102 - val_loss: 0.3144\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3085 - val_loss: 0.3089\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3073 - val_loss: 0.3056\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3059 - val_loss: 0.3167\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3045 - val_loss: 0.3046\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3034 - val_loss: 0.3132\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3015 - val_loss: 0.3199\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3000 - val_loss: 0.3046\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2987 - val_loss: 0.3102\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2980 - val_loss: 0.3163\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2960 - val_loss: 0.3051\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2950 - val_loss: 0.3048\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2939 - val_loss: 0.2988\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2928 - val_loss: 0.3090\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2911 - val_loss: 0.2947\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2901 - val_loss: 0.3173\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2889 - val_loss: 0.2972\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2876 - val_loss: 0.3146\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2875 - val_loss: 0.3021\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2862 - val_loss: 0.3047\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2854 - val_loss: 0.3214\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2843 - val_loss: 0.3489\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2828 - val_loss: 0.2970\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2814 - val_loss: 0.3165\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2803 - val_loss: 0.2873\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2797 - val_loss: 0.2992\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2790 - val_loss: 0.3095\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2779 - val_loss: 0.3004\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2769 - val_loss: 0.3112\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2760 - val_loss: 0.2847\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2750 - val_loss: 0.2859\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2746 - val_loss: 0.3100\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2741 - val_loss: 0.2995\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2732 - val_loss: 0.3020\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2726 - val_loss: 0.2963\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2724 - val_loss: 0.3251\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2708 - val_loss: 0.2947\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2706 - val_loss: 0.3205\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2697 - val_loss: 0.2786\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2690 - val_loss: 0.2922\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2685 - val_loss: 0.2901\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2663 - val_loss: 0.3056\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2671 - val_loss: 0.2909\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2657 - val_loss: 0.2760\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2659 - val_loss: 0.3024\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2643 - val_loss: 0.2999\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2647 - val_loss: 0.2811\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2635 - val_loss: 0.3140\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2637 - val_loss: 0.2792\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2625 - val_loss: 0.3108\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2618 - val_loss: 0.2752\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2612 - val_loss: 0.3040\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2603 - val_loss: 0.3035\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2604 - val_loss: 0.2860\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2585 - val_loss: 0.3038\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2594 - val_loss: 0.3047\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2578 - val_loss: 0.2839\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2573 - val_loss: 0.2986\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2571 - val_loss: 0.2851\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2570 - val_loss: 0.2941\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2557 - val_loss: 0.2856\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3337\n",
      "[CV 2/5] END learning_rate=0.003304044545722164, n_hidden=3, n_neurons=80;, score=-0.334 total time=  54.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 1.0076 - val_loss: 4.7345\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5481 - val_loss: 1.3369\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4597 - val_loss: 0.7846\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4143 - val_loss: 0.8233\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3903 - val_loss: 0.9039\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3749 - val_loss: 1.1847\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3650 - val_loss: 1.4452\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3590 - val_loss: 1.6454\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3551 - val_loss: 1.4720\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3504 - val_loss: 1.6182\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3468 - val_loss: 1.6043\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3427 - val_loss: 1.6416\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3400 - val_loss: 1.4786\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.4017\n",
      "[CV 3/5] END learning_rate=0.003304044545722164, n_hidden=3, n_neurons=80;, score=-0.402 total time=   8.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 119us/sample - loss: 1.2873 - val_loss: 3.4606\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5979 - val_loss: 0.7994\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4946 - val_loss: 0.4202\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4472 - val_loss: 0.3962\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4202 - val_loss: 0.4147\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4040 - val_loss: 0.3698\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3910 - val_loss: 0.5628\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3860 - val_loss: 0.4752\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3778 - val_loss: 0.4782\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3714 - val_loss: 0.3863\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3659 - val_loss: 0.4043\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3636 - val_loss: 0.3343\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3583 - val_loss: 0.4621\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3559 - val_loss: 0.4085\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3533 - val_loss: 0.3726\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3489 - val_loss: 0.4341\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3473 - val_loss: 0.3236\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3445 - val_loss: 0.3293\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3424 - val_loss: 0.3191\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3409 - val_loss: 0.3930\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3399 - val_loss: 0.3177\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3368 - val_loss: 0.4553\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3361 - val_loss: 0.4161\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3343 - val_loss: 0.5506\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3345 - val_loss: 0.3261\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3305 - val_loss: 0.4962\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3306 - val_loss: 0.3651\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3274 - val_loss: 0.4473\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3277 - val_loss: 0.6879\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3282 - val_loss: 0.6572\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3244 - val_loss: 0.3376\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.2860\n",
      "[CV 4/5] END learning_rate=0.003304044545722164, n_hidden=3, n_neurons=80;, score=-0.286 total time=  20.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 1.0762 - val_loss: 4.4269\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6005 - val_loss: 4.1084\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4878 - val_loss: 0.7715\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4192 - val_loss: 0.6805\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3958 - val_loss: 0.3774\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3785 - val_loss: 0.4194\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3707 - val_loss: 0.4903\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3651 - val_loss: 0.3723\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3587 - val_loss: 0.4419\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3562 - val_loss: 0.3368\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3510 - val_loss: 0.3581\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3479 - val_loss: 0.3315\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3447 - val_loss: 0.3557\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3426 - val_loss: 0.3443\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3399 - val_loss: 0.3402\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3372 - val_loss: 0.4177\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3352 - val_loss: 0.3457\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3314 - val_loss: 0.4650\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3311 - val_loss: 0.3616\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3290 - val_loss: 0.3971\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3279 - val_loss: 0.3404\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3248 - val_loss: 0.3631\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3580\n",
      "[CV 5/5] END learning_rate=0.003304044545722164, n_hidden=3, n_neurons=80;, score=-0.358 total time=  14.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.6374 - val_loss: 1.9451\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7616 - val_loss: 0.7303\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6547 - val_loss: 0.5920\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5962 - val_loss: 0.5391\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5509 - val_loss: 0.4968\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5166 - val_loss: 0.4813\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4892 - val_loss: 0.4557\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4693 - val_loss: 0.4258\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4533 - val_loss: 0.4260\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4404 - val_loss: 0.4093\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4310 - val_loss: 0.3946\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4230 - val_loss: 0.3961\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4163 - val_loss: 0.4265\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4109 - val_loss: 0.3777\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4060 - val_loss: 0.3761\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4016 - val_loss: 0.4074\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3978 - val_loss: 0.3667\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3939 - val_loss: 0.4372\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3911 - val_loss: 0.4044\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3880 - val_loss: 0.3887\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3852 - val_loss: 0.4084\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3833 - val_loss: 0.3572\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3802 - val_loss: 0.3576\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3778 - val_loss: 0.3661\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3759 - val_loss: 0.3873\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3733 - val_loss: 0.4030\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3722 - val_loss: 0.3515\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3707 - val_loss: 0.3466\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3679 - val_loss: 0.3501\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3662 - val_loss: 0.4188\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3656 - val_loss: 0.3910\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3633 - val_loss: 0.3979\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3622 - val_loss: 0.3705\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3606 - val_loss: 0.3447\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3591 - val_loss: 0.3980\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3583 - val_loss: 0.3446\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3564 - val_loss: 0.4228\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3551 - val_loss: 0.3320\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3542 - val_loss: 0.3407\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3529 - val_loss: 0.4189\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3522 - val_loss: 0.3380\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3505 - val_loss: 0.4055\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3502 - val_loss: 0.3517\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3484 - val_loss: 0.3624\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3479 - val_loss: 0.4043\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3470 - val_loss: 0.3538\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3458 - val_loss: 0.3727\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3449 - val_loss: 0.4091\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3275\n",
      "[CV 1/5] END learning_rate=0.001366009855620541, n_hidden=2, n_neurons=98;, score=-0.327 total time=  29.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.4457 - val_loss: 6.5590\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7647 - val_loss: 0.9354\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.622 - 1s 64us/sample - loss: 0.6214 - val_loss: 0.5785\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5710 - val_loss: 0.5530\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5347 - val_loss: 0.5072\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5037 - val_loss: 0.4702\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4780 - val_loss: 0.4461\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4573 - val_loss: 0.4268\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4407 - val_loss: 0.4254\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4272 - val_loss: 0.4089\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4158 - val_loss: 0.4080\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4075 - val_loss: 0.3913\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4000 - val_loss: 0.3800\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3932 - val_loss: 0.3765\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3877 - val_loss: 0.3733\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3829 - val_loss: 0.3693\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3785 - val_loss: 0.3686\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3746 - val_loss: 0.3618\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3709 - val_loss: 0.3795\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3676 - val_loss: 0.3681\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3649 - val_loss: 0.3574\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3621 - val_loss: 0.3522\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3597 - val_loss: 0.3481\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3573 - val_loss: 0.3498\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3552 - val_loss: 0.3576\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3535 - val_loss: 0.3555\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3515 - val_loss: 0.3501\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3498 - val_loss: 0.3573\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3485 - val_loss: 0.3381\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3466 - val_loss: 0.3417\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3451 - val_loss: 0.3476\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3438 - val_loss: 0.3397\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3428 - val_loss: 0.3383\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3412 - val_loss: 0.3327\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3404 - val_loss: 0.3490\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3393 - val_loss: 0.3336\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3380 - val_loss: 0.3651\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3376 - val_loss: 0.3314\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3360 - val_loss: 0.3413\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3354 - val_loss: 0.3295\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3346 - val_loss: 0.3439\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3337 - val_loss: 0.3338\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3326 - val_loss: 0.3410\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3321 - val_loss: 0.3273\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3313 - val_loss: 0.3554\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3306 - val_loss: 0.3327\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3296 - val_loss: 0.3298\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3290 - val_loss: 0.3478\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3284 - val_loss: 0.3398\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3278 - val_loss: 0.3232\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3272 - val_loss: 0.3348\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3265 - val_loss: 0.3420\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3258 - val_loss: 0.3282\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3252 - val_loss: 0.3212\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3247 - val_loss: 0.3423\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3239 - val_loss: 0.3336\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3237 - val_loss: 0.3186\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3229 - val_loss: 0.3336\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3223 - val_loss: 0.3236\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3218 - val_loss: 0.3191\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3214 - val_loss: 0.3298\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3207 - val_loss: 0.3161\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3203 - val_loss: 0.3309\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3195 - val_loss: 0.3204\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3191 - val_loss: 0.3293\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3187 - val_loss: 0.3211\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3183 - val_loss: 0.3198\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3176 - val_loss: 0.3140\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3172 - val_loss: 0.3194\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3170 - val_loss: 0.3330\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3162 - val_loss: 0.3162\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3155 - val_loss: 0.3486\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3155 - val_loss: 0.3272\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3151 - val_loss: 0.3474\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3145 - val_loss: 0.3164\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3136 - val_loss: 0.3589\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3136 - val_loss: 0.3195\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3129 - val_loss: 0.3201\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3721\n",
      "[CV 2/5] END learning_rate=0.001366009855620541, n_hidden=2, n_neurons=98;, score=-0.372 total time=  47.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.8136 - val_loss: 2.9208\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6887 - val_loss: 1.5657\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6133 - val_loss: 0.8691\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5613 - val_loss: 0.6427\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5212 - val_loss: 0.5075\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4894 - val_loss: 0.4540\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4642 - val_loss: 0.4337\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4464 - val_loss: 0.4148\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4314 - val_loss: 0.4068\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4207 - val_loss: 0.4290\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4129 - val_loss: 0.3991\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4032 - val_loss: 0.4402\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3966 - val_loss: 0.4801\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3920 - val_loss: 0.5536\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3870 - val_loss: 0.6284\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3832 - val_loss: 0.7510\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3798 - val_loss: 0.8218\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3765 - val_loss: 1.0433\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3738 - val_loss: 1.0898\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3711 - val_loss: 1.2121\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3686 - val_loss: 1.2468\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.4181\n",
      "[CV 3/5] END learning_rate=0.001366009855620541, n_hidden=2, n_neurons=98;, score=-0.418 total time=  12.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.6494 - val_loss: 0.8191\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.7056 - val_loss: 0.6186\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6326 - val_loss: 0.5527\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5745 - val_loss: 0.5032\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5296 - val_loss: 0.4716\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4950 - val_loss: 0.4408\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4685 - val_loss: 0.4132\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4501 - val_loss: 0.4272\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4366 - val_loss: 0.3906\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4254 - val_loss: 0.3907\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4184 - val_loss: 0.3782\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4114 - val_loss: 0.3697\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4064 - val_loss: 0.3662\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4016 - val_loss: 0.3811\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3979 - val_loss: 0.3754\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3935 - val_loss: 0.4957\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3914 - val_loss: 0.3559\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3872 - val_loss: 0.3586\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3844 - val_loss: 0.3700\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3817 - val_loss: 0.4212\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3802 - val_loss: 0.3586\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3774 - val_loss: 0.3476\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3751 - val_loss: 0.4070\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3742 - val_loss: 0.3486\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3710 - val_loss: 0.4337\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3707 - val_loss: 0.3408\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3678 - val_loss: 0.3841\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3667 - val_loss: 0.3738\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3654 - val_loss: 0.3399\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3634 - val_loss: 0.3378\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3624 - val_loss: 0.3389\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3612 - val_loss: 0.3337\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3594 - val_loss: 0.3926\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3588 - val_loss: 0.3598\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3578 - val_loss: 0.3326\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3565 - val_loss: 0.3286\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3554 - val_loss: 0.3279\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3538 - val_loss: 0.4189\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3541 - val_loss: 0.3788\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3528 - val_loss: 0.3796\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3521 - val_loss: 0.3399\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3506 - val_loss: 0.3253\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3492 - val_loss: 0.3621\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3493 - val_loss: 0.3287\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3478 - val_loss: 0.4819\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.3475 - val_loss: 0.3339\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3465 - val_loss: 0.3269\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3451 - val_loss: 0.3770\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3453 - val_loss: 0.3377\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3439 - val_loss: 0.3253\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 62s 7ms/sample - loss: 0.3427 - val_loss: 0.5186\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3443 - val_loss: 0.3194\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3415 - val_loss: 0.3189\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3408 - val_loss: 0.3189\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.3400 - val_loss: 0.3174\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.3388 - val_loss: 0.4434\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3392 - val_loss: 0.3714\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.3383 - val_loss: 0.6312\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.3392 - val_loss: 0.6339\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.3382 - val_loss: 0.5409\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.3375 - val_loss: 0.5353\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.3372 - val_loss: 0.7530\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3396 - val_loss: 0.3638\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3349 - val_loss: 0.5873\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.3365 - val_loss: 0.3622\n",
      "2322/2322 [==============================] - 0s 24us/sample - loss: 0.2920\n",
      "[CV 4/5] END learning_rate=0.001366009855620541, n_hidden=2, n_neurons=98;, score=-0.292 total time= 1.6min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.8452 - val_loss: 0.7352\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.6814 - val_loss: 0.6331\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.6139 - val_loss: 0.5738\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.5648 - val_loss: 0.5342\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.5274 - val_loss: 0.4959\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.4973 - val_loss: 0.4722\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4731 - val_loss: 0.4623\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4538 - val_loss: 0.4403\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4390 - val_loss: 0.4418\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4276 - val_loss: 0.4401\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4178 - val_loss: 0.4357\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4095 - val_loss: 0.4693\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4037 - val_loss: 0.4374\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3978 - val_loss: 0.4077\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3933 - val_loss: 0.4373\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3889 - val_loss: 0.4222\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3849 - val_loss: 0.4254\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3816 - val_loss: 0.3788\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3782 - val_loss: 0.4140\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3755 - val_loss: 0.3777\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3723 - val_loss: 0.4009\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3699 - val_loss: 0.3902\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3672 - val_loss: 0.4257\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3651 - val_loss: 0.4111\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3631 - val_loss: 0.4036\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3612 - val_loss: 0.3792\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3592 - val_loss: 0.3670\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3574 - val_loss: 0.3629\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3553 - val_loss: 0.4018\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3538 - val_loss: 0.3862\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3523 - val_loss: 0.3767\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3510 - val_loss: 0.4155\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3499 - val_loss: 0.3433\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3486 - val_loss: 0.3916\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.3470 - val_loss: 0.3576\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3460 - val_loss: 0.3871\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.3448 - val_loss: 0.3540\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3435 - val_loss: 0.3754\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3427 - val_loss: 0.3467\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 88us/sample - loss: 0.3415 - val_loss: 0.3867\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3407 - val_loss: 0.3575\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3395 - val_loss: 0.3819\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3386 - val_loss: 0.3868\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3703\n",
      "[CV 5/5] END learning_rate=0.001366009855620541, n_hidden=2, n_neurons=98;, score=-0.370 total time=  26.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 120us/sample - loss: 0.7712 - val_loss: 0.6488\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4840 - val_loss: 0.4630\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4366 - val_loss: 0.4378\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4123 - val_loss: 0.4380\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4055 - val_loss: 0.4810\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3964 - val_loss: 0.3732\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3934 - val_loss: 0.3911\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3912 - val_loss: 0.3830\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3879 - val_loss: 0.3742\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3871 - val_loss: 0.3983\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3869 - val_loss: 0.3953\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3850 - val_loss: 0.4012\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3838 - val_loss: 0.4057\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3817 - val_loss: 0.3576\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3800 - val_loss: 0.3460\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3802 - val_loss: 0.3565\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3798 - val_loss: 0.3522\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3771 - val_loss: 0.3481\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3758 - val_loss: 0.3874\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3743 - val_loss: 0.3629\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3729 - val_loss: 0.3438\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3727 - val_loss: 0.3827\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3710 - val_loss: 0.3901\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3706 - val_loss: 0.3561\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3688 - val_loss: 0.3474\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3656 - val_loss: 0.3614\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3646 - val_loss: 0.3385\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3606 - val_loss: 0.3554\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3616 - val_loss: 0.3429\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3609 - val_loss: 0.3377\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3611 - val_loss: 0.3501\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3596 - val_loss: 0.3360\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3594 - val_loss: 0.3292\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3592 - val_loss: 0.3500\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3591 - val_loss: 0.3551\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3567 - val_loss: 0.3881\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3560 - val_loss: 0.3761\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3582 - val_loss: 0.3863\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3551 - val_loss: 0.3576\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3529 - val_loss: 0.3663\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3529 - val_loss: 0.3487\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3543 - val_loss: 0.3434\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3532 - val_loss: 0.3449\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3382\n",
      "[CV 1/5] END learning_rate=0.015518809280202198, n_hidden=3, n_neurons=5;, score=-0.338 total time=  28.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 1.0552 - val_loss: 0.5446\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.4832 - val_loss: 0.4483\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4409 - val_loss: 0.4151\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4215 - val_loss: 0.3883\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4046 - val_loss: 0.3756\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3981 - val_loss: 0.3795\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3889 - val_loss: 0.3748\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3846 - val_loss: 0.3841\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3796 - val_loss: 0.3684\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3773 - val_loss: 0.3669\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3724 - val_loss: 0.3542\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3716 - val_loss: 0.3515\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3686 - val_loss: 0.3641\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3680 - val_loss: 0.3498\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3676 - val_loss: 0.3442\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3644 - val_loss: 0.3485\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3642 - val_loss: 0.3444\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3617 - val_loss: 0.3529\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3613 - val_loss: 0.3406\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3579 - val_loss: 0.3670\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3596 - val_loss: 0.3610\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3590 - val_loss: 0.3620\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3569 - val_loss: 0.3861\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3565 - val_loss: 0.3520\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3572 - val_loss: 0.4481\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3560 - val_loss: 0.3474\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3543 - val_loss: 0.3630\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3552 - val_loss: 0.3491\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3543 - val_loss: 0.3412\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.4107\n",
      "[CV 2/5] END learning_rate=0.015518809280202198, n_hidden=3, n_neurons=5;, score=-0.411 total time=  18.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 100us/sample - loss: 1.4141 - val_loss: 0.8672\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5971 - val_loss: 0.4327\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4302 - val_loss: 0.4170\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4028 - val_loss: 0.3702\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3858 - val_loss: 0.3859\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3821 - val_loss: 0.3729\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3946 - val_loss: 0.3809\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3813 - val_loss: 0.3564\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3772 - val_loss: 0.3542\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3728 - val_loss: 0.3644\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3735 - val_loss: 0.3773\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3758 - val_loss: 0.3592\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3700 - val_loss: 0.3487\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3731 - val_loss: 0.4066\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3723 - val_loss: 0.3745\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3642 - val_loss: 0.3588\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3651 - val_loss: 0.3420\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.3613 - val_loss: 0.3445\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3561 - val_loss: 0.3907\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3573 - val_loss: 0.3385\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3535 - val_loss: 0.3409\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3537 - val_loss: 0.3439\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3545 - val_loss: 0.3390\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3545 - val_loss: 0.3559\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3536 - val_loss: 0.3424\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3557 - val_loss: 0.3594\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3520 - val_loss: 0.3821\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3536 - val_loss: 0.3460\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3715 - val_loss: 0.3486\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3511 - val_loss: 0.3381\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3538 - val_loss: 0.3482\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3530 - val_loss: 0.3549\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3509 - val_loss: 0.3326\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3591 - val_loss: 0.3392\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3518 - val_loss: 0.3316\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3524 - val_loss: 0.3297\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3516 - val_loss: 0.3543\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3496 - val_loss: 0.3320\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3457 - val_loss: 0.3280\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3468 - val_loss: 0.3652\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3461 - val_loss: 0.3340\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3454 - val_loss: 0.3478\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3442 - val_loss: 0.3411\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3436 - val_loss: 0.3268\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3449 - val_loss: 0.3239\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3461 - val_loss: 0.3228\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3464 - val_loss: 0.3359\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3475 - val_loss: 0.3229\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3434 - val_loss: 0.3224\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3387 - val_loss: 0.3249\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3401 - val_loss: 0.3224\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3478 - val_loss: 0.3699\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3408 - val_loss: 0.3551\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3415 - val_loss: 0.3192\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3437 - val_loss: 0.3151\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3396 - val_loss: 0.3192\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3386 - val_loss: 0.3153\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3378 - val_loss: 0.3234\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3364 - val_loss: 0.3224\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3362 - val_loss: 0.3170\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3443 - val_loss: 0.3277\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3385 - val_loss: 0.3214\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3335 - val_loss: 0.3182\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3340 - val_loss: 0.3568\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3321 - val_loss: 0.3154\n",
      "2322/2322 [==============================] - 0s 37us/sample - loss: 0.3559\n",
      "[CV 3/5] END learning_rate=0.015518809280202198, n_hidden=3, n_neurons=5;, score=-0.356 total time=  41.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 1.1110 - val_loss: 1.1982\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.5173 - val_loss: 0.5688\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4755 - val_loss: 0.4738\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4496 - val_loss: 0.4852\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4329 - val_loss: 0.4150\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4167 - val_loss: 0.4134\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4067 - val_loss: 0.3941\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3998 - val_loss: 0.3881\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3954 - val_loss: 0.3643\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3915 - val_loss: 0.3562\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3861 - val_loss: 0.3526\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3838 - val_loss: 0.3527\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3794 - val_loss: 0.3495\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3765 - val_loss: 0.3367\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3741 - val_loss: 0.3549\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3719 - val_loss: 0.3457\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3707 - val_loss: 0.3439\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3683 - val_loss: 0.3366\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3677 - val_loss: 0.3272\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3667 - val_loss: 0.3346\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3640 - val_loss: 0.3693\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3630 - val_loss: 0.3249\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3612 - val_loss: 0.3855\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3588 - val_loss: 0.3329\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3593 - val_loss: 0.3233\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3570 - val_loss: 0.3489\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3579 - val_loss: 0.3286\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3571 - val_loss: 0.3589\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3538 - val_loss: 0.3156\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3511 - val_loss: 0.3210\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.3513 - val_loss: 0.3152\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3507 - val_loss: 0.3506\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.3488 - val_loss: 0.3255\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3465 - val_loss: 0.3161\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3473 - val_loss: 0.3273\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3457 - val_loss: 0.3144\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3454 - val_loss: 0.3111\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3445 - val_loss: 0.3646\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3444 - val_loss: 0.3084\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3427 - val_loss: 0.3249\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3435 - val_loss: 0.3111\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3425 - val_loss: 0.3170\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3410 - val_loss: 0.3393\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3422 - val_loss: 0.3243\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3416 - val_loss: 0.3108\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 124us/sample - loss: 0.3428 - val_loss: 0.3161\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 0.3383 - val_loss: 0.3706\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3395 - val_loss: 0.3153\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.3407 - val_loss: 0.3284\n",
      "2322/2322 [==============================] - 0s 25us/sample - loss: 0.2902\n",
      "[CV 4/5] END learning_rate=0.015518809280202198, n_hidden=3, n_neurons=5;, score=-0.290 total time=  31.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.6327 - val_loss: 0.4787\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.4453 - val_loss: 0.4203\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4247 - val_loss: 0.4318\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4159 - val_loss: 0.3894\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.4110 - val_loss: 0.4090\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 0.6598 - val_loss: 0.4688\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4675 - val_loss: 0.4396\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4279 - val_loss: 0.4003\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4086 - val_loss: 0.3950\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3994 - val_loss: 0.3706\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3923 - val_loss: 0.4806\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3877 - val_loss: 0.3713\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3902 - val_loss: 0.3900\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3816 - val_loss: 0.4476\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3850 - val_loss: 0.3613\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3782 - val_loss: 0.3791\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3766 - val_loss: 0.3598\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3747 - val_loss: 0.3978\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.3722 - val_loss: 0.3935\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.3770 - val_loss: 0.4121\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3721 - val_loss: 0.3718\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3745 - val_loss: 0.3517\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3724 - val_loss: 0.4106\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3690 - val_loss: 0.3665\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3690 - val_loss: 0.4180\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3694 - val_loss: 0.3628\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3704 - val_loss: 0.3473\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3659 - val_loss: 0.5207\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3672 - val_loss: 0.5622\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3697 - val_loss: 0.5038\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3673 - val_loss: 0.3670\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3689 - val_loss: 0.4121\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3668 - val_loss: 0.3403\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3632 - val_loss: 0.4162\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3641 - val_loss: 0.3612\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3641 - val_loss: 0.3480\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3629 - val_loss: 0.3747\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3622 - val_loss: 0.3531\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3634 - val_loss: 0.3926\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3625 - val_loss: 0.3514\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3619 - val_loss: 0.3783\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3606 - val_loss: 0.3516\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3605 - val_loss: 0.3620\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3837\n",
      "[CV 5/5] END learning_rate=0.015518809280202198, n_hidden=3, n_neurons=5;, score=-0.384 total time=  27.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 2.0665 - val_loss: 0.7983\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.7602 - val_loss: 0.6878\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.6921 - val_loss: 0.6470\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.6520 - val_loss: 0.6012\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.6171 - val_loss: 0.5595\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.5881 - val_loss: 0.5534\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.5616 - val_loss: 0.5176\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.5390 - val_loss: 0.5257\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5192 - val_loss: 0.4872\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5024 - val_loss: 0.4789\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4883 - val_loss: 0.4877\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4764 - val_loss: 0.4833\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4667 - val_loss: 0.4588\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4586 - val_loss: 0.4874\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4517 - val_loss: 0.4452\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4461 - val_loss: 0.4665\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4411 - val_loss: 0.4353\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4370 - val_loss: 0.4406\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4334 - val_loss: 0.4430\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4298 - val_loss: 0.4716\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4269 - val_loss: 0.4660\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4241 - val_loss: 0.4920\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4216 - val_loss: 0.4303\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4190 - val_loss: 0.4352\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4168 - val_loss: 0.4929\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4147 - val_loss: 0.4361\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4124 - val_loss: 0.4877\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4109 - val_loss: 0.3978\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4088 - val_loss: 0.4285\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4067 - val_loss: 0.4825\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4052 - val_loss: 0.5036\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4038 - val_loss: 0.3918\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4018 - val_loss: 0.4067\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4003 - val_loss: 0.4785\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3992 - val_loss: 0.4162\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3976 - val_loss: 0.4451\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3961 - val_loss: 0.4901\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3952 - val_loss: 0.3892\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3932 - val_loss: 0.4995\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3928 - val_loss: 0.3895\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3913 - val_loss: 0.3979\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3897 - val_loss: 0.5096\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3891 - val_loss: 0.4336\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3877 - val_loss: 0.4809\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3870 - val_loss: 0.3876\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3855 - val_loss: 0.5011\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3849 - val_loss: 0.4208\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3838 - val_loss: 0.3862\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.3829 - val_loss: 0.3817\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3819 - val_loss: 0.3799\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3813 - val_loss: 0.4026\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3803 - val_loss: 0.3828\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3794 - val_loss: 0.3828\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3786 - val_loss: 0.4433\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3779 - val_loss: 0.4433\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3771 - val_loss: 0.4021\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3758 - val_loss: 0.4637\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3754 - val_loss: 0.4545\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3751 - val_loss: 0.3847\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3740 - val_loss: 0.4959\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3534\n",
      "[CV 1/5] END learning_rate=0.0012072770546551804, n_hidden=1, n_neurons=55;, score=-0.353 total time=  35.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 2.2173 - val_loss: 14.4416\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.9363 - val_loss: 0.7733\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6782 - val_loss: 0.6335\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6312 - val_loss: 0.6176\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5985 - val_loss: 0.5734\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5699 - val_loss: 0.5405\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.5447 - val_loss: 0.5234\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5226 - val_loss: 0.5160\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5045 - val_loss: 0.4814\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4880 - val_loss: 0.4643\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4742 - val_loss: 0.4542\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4626 - val_loss: 0.4406\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4529 - val_loss: 0.4311\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4447 - val_loss: 0.4250\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4375 - val_loss: 0.4166\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4316 - val_loss: 0.4147\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4265 - val_loss: 0.4085\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4221 - val_loss: 0.4107\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4182 - val_loss: 0.4035\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4147 - val_loss: 0.4033\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.4117 - val_loss: 0.4043\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 0s 54us/sample - loss: 0.4090 - val_loss: 0.3981\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.4063 - val_loss: 0.4010\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.4041 - val_loss: 0.4051\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.4020 - val_loss: 0.3895\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.4001 - val_loss: 0.3923\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3981 - val_loss: 0.4054\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.3965 - val_loss: 0.3945\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3949 - val_loss: 0.3825\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3934 - val_loss: 0.3907\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3919 - val_loss: 0.3961\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3904 - val_loss: 0.3928\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3890 - val_loss: 0.3922\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3876 - val_loss: 0.3762\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3862 - val_loss: 0.3987\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.3850 - val_loss: 0.3939\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3839 - val_loss: 0.3776\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3827 - val_loss: 0.3918\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3816 - val_loss: 0.3763\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3805 - val_loss: 0.3771\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3796 - val_loss: 0.3739\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3784 - val_loss: 0.3824\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3775 - val_loss: 0.3814\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3765 - val_loss: 0.3762\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3756 - val_loss: 0.3895\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3749 - val_loss: 0.3728\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3740 - val_loss: 0.3747\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3732 - val_loss: 0.3734\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3724 - val_loss: 0.3798\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3717 - val_loss: 0.3713\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3710 - val_loss: 0.3713\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3702 - val_loss: 0.3805\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3695 - val_loss: 0.3855\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3687 - val_loss: 0.3684\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3681 - val_loss: 0.3861\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3675 - val_loss: 0.3691\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3669 - val_loss: 0.3759\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3661 - val_loss: 0.3673\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3656 - val_loss: 0.3850\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3649 - val_loss: 0.3635\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3644 - val_loss: 0.3895\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3638 - val_loss: 0.3757\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3633 - val_loss: 0.3779\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3627 - val_loss: 0.3699\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3622 - val_loss: 0.3746\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3617 - val_loss: 0.3753\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3611 - val_loss: 0.3829\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3606 - val_loss: 0.3635\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3601 - val_loss: 0.3767\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3595 - val_loss: 0.3648\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4095\n",
      "[CV 2/5] END learning_rate=0.0012072770546551804, n_hidden=1, n_neurons=55;, score=-0.409 total time=  39.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 108us/sample - loss: 1.9709 - val_loss: 4.0652\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.7066 - val_loss: 2.1095\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6464 - val_loss: 1.2431\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6094 - val_loss: 0.8184\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5793 - val_loss: 0.5782\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5534 - val_loss: 0.5402\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5324 - val_loss: 0.5356\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5138 - val_loss: 0.5613\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4980 - val_loss: 0.5561\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4840 - val_loss: 0.5973\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4733 - val_loss: 0.5744\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4632 - val_loss: 0.5494\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4544 - val_loss: 0.4946\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4469 - val_loss: 0.4716\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4400 - val_loss: 0.4463\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4341 - val_loss: 0.4177\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4288 - val_loss: 0.4074\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4241 - val_loss: 0.4031\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4201 - val_loss: 0.4194\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4163 - val_loss: 0.4456\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4131 - val_loss: 0.4565\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4101 - val_loss: 0.5078\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4072 - val_loss: 0.5407\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4049 - val_loss: 0.5763\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4026 - val_loss: 0.6381\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4006 - val_loss: 0.7117\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3986 - val_loss: 0.7561\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3967 - val_loss: 0.8207\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4317\n",
      "[CV 3/5] END learning_rate=0.0012072770546551804, n_hidden=1, n_neurons=55;, score=-0.432 total time=  15.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 78us/sample - loss: 2.0998 - val_loss: 1.2699\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.8234 - val_loss: 0.6831\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6814 - val_loss: 0.7144\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6338 - val_loss: 0.5924\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6035 - val_loss: 0.5538\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5754 - val_loss: 0.5612\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5523 - val_loss: 0.5167\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5323 - val_loss: 0.5059\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5150 - val_loss: 0.5159\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5006 - val_loss: 0.4735\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4884 - val_loss: 0.4383\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4788 - val_loss: 0.4343\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4705 - val_loss: 0.4616\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4631 - val_loss: 0.4186\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4579 - val_loss: 0.4596\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4523 - val_loss: 0.4082\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4478 - val_loss: 0.4423\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4441 - val_loss: 0.4324\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4406 - val_loss: 0.3965\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4371 - val_loss: 0.4349\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4348 - val_loss: 0.3918\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4318 - val_loss: 0.4423\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4298 - val_loss: 0.4202\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4274 - val_loss: 0.3864\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4253 - val_loss: 0.4431\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4235 - val_loss: 0.4090\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4219 - val_loss: 0.3955\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4200 - val_loss: 0.4176\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4186 - val_loss: 0.3825\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4168 - val_loss: 0.4546\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4161 - val_loss: 0.3787\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4141 - val_loss: 0.4545\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4134 - val_loss: 0.3734\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4114 - val_loss: 0.4548\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4109 - val_loss: 0.3806\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4093 - val_loss: 0.5331\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4092 - val_loss: 0.3703\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4070 - val_loss: 0.3997\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4058 - val_loss: 0.4210\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4049 - val_loss: 0.4089\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4040 - val_loss: 0.3877\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4029 - val_loss: 0.3665\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4016 - val_loss: 0.4693\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4015 - val_loss: 0.3655\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4002 - val_loss: 0.4301\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3992 - val_loss: 0.4087\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3989 - val_loss: 0.3627\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3974 - val_loss: 0.4731\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3974 - val_loss: 0.3927\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3959 - val_loss: 0.4405\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3960 - val_loss: 0.3635\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3946 - val_loss: 0.3624\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3937 - val_loss: 0.4328\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3937 - val_loss: 0.3593\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3926 - val_loss: 0.4323\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3925 - val_loss: 0.3574\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3912 - val_loss: 0.4535\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3915 - val_loss: 0.3584\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3900 - val_loss: 0.4018\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3895 - val_loss: 0.3966\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3894 - val_loss: 0.3547\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3886 - val_loss: 0.3584\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3879 - val_loss: 0.3538\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3871 - val_loss: 0.4294\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3870 - val_loss: 0.3678\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.387 - 1s 59us/sample - loss: 0.3860 - val_loss: 0.4299\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3860 - val_loss: 0.3599\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3851 - val_loss: 0.3679\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3844 - val_loss: 0.4049\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3843 - val_loss: 0.3699\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3837 - val_loss: 0.3506\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3828 - val_loss: 0.3524\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3826 - val_loss: 0.3600\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3821 - val_loss: 0.3822\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3818 - val_loss: 0.3882\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3818 - val_loss: 0.3827\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3811 - val_loss: 0.3484\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3801 - val_loss: 0.4260\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3805 - val_loss: 0.3479\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3795 - val_loss: 0.3777\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3788 - val_loss: 0.4277\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3790 - val_loss: 0.3575\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3784 - val_loss: 0.3499\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3775 - val_loss: 0.4657\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3782 - val_loss: 0.4206\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3776 - val_loss: 0.4728\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3776 - val_loss: 0.4114\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3767 - val_loss: 0.4151\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3765 - val_loss: 0.3525\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3239\n",
      "[CV 4/5] END learning_rate=0.0012072770546551804, n_hidden=1, n_neurons=55;, score=-0.324 total time=  49.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 2.1059 - val_loss: 1.1510\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.7466 - val_loss: 0.6993\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6798 - val_loss: 0.6611\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6409 - val_loss: 0.6034\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6075 - val_loss: 0.5725\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5792 - val_loss: 0.5448\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5546 - val_loss: 0.5200\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5326 - val_loss: 0.5210\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5127 - val_loss: 0.4862\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4962 - val_loss: 0.4771\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4820 - val_loss: 0.4719\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4694 - val_loss: 0.4732\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4587 - val_loss: 0.4648\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4500 - val_loss: 0.4628\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4417 - val_loss: 0.4424\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4357 - val_loss: 0.4244\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4300 - val_loss: 0.4606\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4250 - val_loss: 0.4657\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4211 - val_loss: 0.4638\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4170 - val_loss: 0.4445\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4139 - val_loss: 0.4510\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4107 - val_loss: 0.4361\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4084 - val_loss: 0.4154\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4058 - val_loss: 0.4484\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4039 - val_loss: 0.4244\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4018 - val_loss: 0.4216\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3999 - val_loss: 0.4721\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3986 - val_loss: 0.4044\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3966 - val_loss: 0.4219\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3952 - val_loss: 0.4123\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3946 - val_loss: 0.4730\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3922 - val_loss: 0.4629\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3912 - val_loss: 0.3906\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3898 - val_loss: 0.3981\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3887 - val_loss: 0.4123\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3873 - val_loss: 0.4758\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3868 - val_loss: 0.4153\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3856 - val_loss: 0.3704\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3843 - val_loss: 0.4629\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3836 - val_loss: 0.4340\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3827 - val_loss: 0.3904\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3814 - val_loss: 0.4324\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3809 - val_loss: 0.4564\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3801 - val_loss: 0.4039\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3791 - val_loss: 0.4371\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3786 - val_loss: 0.3602\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3775 - val_loss: 0.4345\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3770 - val_loss: 0.3591\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3763 - val_loss: 0.4347\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3757 - val_loss: 0.3771\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3749 - val_loss: 0.4083\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3739 - val_loss: 0.3914\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3737 - val_loss: 0.3596\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3730 - val_loss: 0.3631\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3721 - val_loss: 0.3768\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3717 - val_loss: 0.3857\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3711 - val_loss: 0.3836\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3702 - val_loss: 0.4298\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4028\n",
      "[CV 5/5] END learning_rate=0.0012072770546551804, n_hidden=1, n_neurons=55;, score=-0.403 total time=  32.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.6564 - val_loss: 9.3551\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 1.0946 - val_loss: 1.0241\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7362 - val_loss: 0.6403\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6366 - val_loss: 0.5675\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5876 - val_loss: 0.5277\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5521 - val_loss: 0.4953\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5215 - val_loss: 0.4751\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4970 - val_loss: 0.4568\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4771 - val_loss: 0.4456\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4603 - val_loss: 0.4217\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4462 - val_loss: 0.4244\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4348 - val_loss: 0.4130\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4256 - val_loss: 0.3971\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4178 - val_loss: 0.3942\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4109 - val_loss: 0.4247\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4051 - val_loss: 0.4275\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4000 - val_loss: 0.4078\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3956 - val_loss: 0.3719\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3915 - val_loss: 0.3955\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3882 - val_loss: 0.3664\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3848 - val_loss: 0.3731\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3819 - val_loss: 0.3640\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3793 - val_loss: 0.3666\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3771 - val_loss: 0.3594\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3748 - val_loss: 0.3664\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3728 - val_loss: 0.3730\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3713 - val_loss: 0.3926\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3692 - val_loss: 0.3792\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3678 - val_loss: 0.3599\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3664 - val_loss: 0.3475\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3645 - val_loss: 0.4002\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3640 - val_loss: 0.3800\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3626 - val_loss: 0.3882\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3619 - val_loss: 0.3446\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3597 - val_loss: 0.3609\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3597 - val_loss: 0.3822\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3585 - val_loss: 0.3465\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3573 - val_loss: 0.3678\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3566 - val_loss: 0.3445\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3554 - val_loss: 0.3972\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3549 - val_loss: 0.3391\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3542 - val_loss: 0.3405\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3529 - val_loss: 0.3922\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3522 - val_loss: 0.3941\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3518 - val_loss: 0.3504\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3509 - val_loss: 0.3901\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3505 - val_loss: 0.3537\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3494 - val_loss: 0.3474\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3489 - val_loss: 0.3369\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3481 - val_loss: 0.3508\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3479 - val_loss: 0.3397\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3470 - val_loss: 0.3928\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3464 - val_loss: 0.3512\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3453 - val_loss: 0.3834\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3451 - val_loss: 0.3837\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3446 - val_loss: 0.3508\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3439 - val_loss: 0.3416\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3433 - val_loss: 0.3580\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3429 - val_loss: 0.3384\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3346\n",
      "[CV 1/5] END learning_rate=0.000849484301442017, n_hidden=3, n_neurons=49;, score=-0.335 total time=  38.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 2.2941 - val_loss: 1.4771\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7799 - val_loss: 0.6703\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6493 - val_loss: 0.5938\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6022 - val_loss: 0.5949\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5721 - val_loss: 0.5556\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5471 - val_loss: 0.5154\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5253 - val_loss: 0.5075\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5069 - val_loss: 0.4871\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4907 - val_loss: 0.4619\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4758 - val_loss: 0.4496\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4634 - val_loss: 0.4403\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4529 - val_loss: 0.4278\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4433 - val_loss: 0.4253\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4356 - val_loss: 0.4133\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4285 - val_loss: 0.4136\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4223 - val_loss: 0.4085\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4171 - val_loss: 0.4048\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4121 - val_loss: 0.4038\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4079 - val_loss: 0.3918\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4041 - val_loss: 0.3960\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4004 - val_loss: 0.3983\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3971 - val_loss: 0.4018\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3941 - val_loss: 0.3919\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3912 - val_loss: 0.3891\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3885 - val_loss: 0.3889\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3857 - val_loss: 0.3865\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3835 - val_loss: 0.3884\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3810 - val_loss: 0.3879\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3788 - val_loss: 0.3819\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3766 - val_loss: 0.3795\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3745 - val_loss: 0.3704\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3728 - val_loss: 0.3681\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3706 - val_loss: 0.3684\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3692 - val_loss: 0.3692\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3673 - val_loss: 0.3818\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3657 - val_loss: 0.3641\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3642 - val_loss: 0.3642\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3628 - val_loss: 0.3685\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3615 - val_loss: 0.3657\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3599 - val_loss: 0.3715\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3587 - val_loss: 0.3631\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3574 - val_loss: 0.3715\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3562 - val_loss: 0.3645\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3551 - val_loss: 0.3556\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3538 - val_loss: 0.3577\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3528 - val_loss: 0.3663\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3516 - val_loss: 0.3571\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3503 - val_loss: 0.3816\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3495 - val_loss: 0.3649\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3486 - val_loss: 0.3549\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3474 - val_loss: 0.3524\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3464 - val_loss: 0.3612\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3455 - val_loss: 0.3522\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3448 - val_loss: 0.3676\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3440 - val_loss: 0.3474\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3431 - val_loss: 0.3532\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3425 - val_loss: 0.3449\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3417 - val_loss: 0.3531\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3409 - val_loss: 0.3458\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3401 - val_loss: 0.3425\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3392 - val_loss: 0.3438\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3386 - val_loss: 0.3558\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3381 - val_loss: 0.3395\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3374 - val_loss: 0.3416\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3367 - val_loss: 0.3354\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3361 - val_loss: 0.3558\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3355 - val_loss: 0.3353\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3350 - val_loss: 0.3351\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3343 - val_loss: 0.3381\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3336 - val_loss: 0.3424\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3333 - val_loss: 0.3344\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3325 - val_loss: 0.3401\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3324 - val_loss: 0.3357\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3316 - val_loss: 0.3358\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3311 - val_loss: 0.3354\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3304 - val_loss: 0.3420\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3301 - val_loss: 0.3387\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3297 - val_loss: 0.3346\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3290 - val_loss: 0.3430\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3284 - val_loss: 0.3338\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3280 - val_loss: 0.3478\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3277 - val_loss: 0.3311\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3274 - val_loss: 0.3354\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3262 - val_loss: 0.3334\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3260 - val_loss: 0.3471\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3259 - val_loss: 0.3367\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3253 - val_loss: 0.3364\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3250 - val_loss: 0.3294\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3244 - val_loss: 0.3281\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3238 - val_loss: 0.3340\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3237 - val_loss: 0.3284\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3233 - val_loss: 0.3299\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3227 - val_loss: 0.3342\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3224 - val_loss: 0.3314\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3217 - val_loss: 0.3383\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3216 - val_loss: 0.3266\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3213 - val_loss: 0.3308\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3207 - val_loss: 0.3285\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3204 - val_loss: 0.3284\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3199 - val_loss: 0.3260\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3684\n",
      "[CV 2/5] END learning_rate=0.000849484301442017, n_hidden=3, n_neurons=49;, score=-0.368 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.0888 - val_loss: 11.1522\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7718 - val_loss: 8.2987\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7031 - val_loss: 6.5140\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6616 - val_loss: 4.8973\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6265 - val_loss: 3.6768\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5955 - val_loss: 2.7843\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5683 - val_loss: 2.1341\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5439 - val_loss: 1.6587\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5220 - val_loss: 1.3671\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5026 - val_loss: 1.1959\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4860 - val_loss: 1.1502\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4716 - val_loss: 1.0485\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4591 - val_loss: 0.9994\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4483 - val_loss: 0.9659\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4388 - val_loss: 0.9000\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4303 - val_loss: 0.8970\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4230 - val_loss: 0.8811\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4163 - val_loss: 0.8698\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4103 - val_loss: 0.8880\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4046 - val_loss: 0.8913\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3998 - val_loss: 0.9040\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3949 - val_loss: 0.9161\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3910 - val_loss: 0.9509\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3873 - val_loss: 0.9778\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3840 - val_loss: 0.9995\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3806 - val_loss: 1.0244\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3777 - val_loss: 1.0477\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3750 - val_loss: 1.0885\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.4097\n",
      "[CV 3/5] END learning_rate=0.000849484301442017, n_hidden=3, n_neurons=49;, score=-0.410 total time=  18.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 2.3876 - val_loss: 2.6588\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.8501 - val_loss: 0.8518\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6524 - val_loss: 0.6289\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5960 - val_loss: 0.5521\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5609 - val_loss: 0.5105\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5335 - val_loss: 0.4874\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5112 - val_loss: 0.4678\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4927 - val_loss: 0.4494\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4774 - val_loss: 0.4361\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4643 - val_loss: 0.4252\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4529 - val_loss: 0.4175\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4437 - val_loss: 0.4110\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4352 - val_loss: 0.4073\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4285 - val_loss: 0.4023\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4219 - val_loss: 0.4011\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4166 - val_loss: 0.4035\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4116 - val_loss: 0.4076\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4078 - val_loss: 0.3986\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4036 - val_loss: 0.3964\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3998 - val_loss: 0.4059\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3966 - val_loss: 0.3965\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3929 - val_loss: 0.3976\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3907 - val_loss: 0.3883\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3879 - val_loss: 0.3886\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3854 - val_loss: 0.4078\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3830 - val_loss: 0.3928\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3806 - val_loss: 0.3885\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3787 - val_loss: 0.3802\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3767 - val_loss: 0.3877\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3746 - val_loss: 0.3720\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3728 - val_loss: 0.3770\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3710 - val_loss: 0.3796\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3696 - val_loss: 0.3590\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3680 - val_loss: 0.3932\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3663 - val_loss: 0.3874\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3650 - val_loss: 0.3793\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3638 - val_loss: 0.3848\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3626 - val_loss: 0.3687\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3613 - val_loss: 0.3667\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3606 - val_loss: 0.3544\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3594 - val_loss: 0.3636\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3584 - val_loss: 0.3512\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3577 - val_loss: 0.3515\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3566 - val_loss: 0.3606\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3558 - val_loss: 0.3502\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3550 - val_loss: 0.3435\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3539 - val_loss: 0.3563\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3531 - val_loss: 0.3562\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3525 - val_loss: 0.3606\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3517 - val_loss: 0.3529\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3510 - val_loss: 0.3679\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3502 - val_loss: 0.3523\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3497 - val_loss: 0.3481\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3487 - val_loss: 0.3347\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3485 - val_loss: 0.3381\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3477 - val_loss: 0.3333\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3471 - val_loss: 0.3386\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3467 - val_loss: 0.3526\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3462 - val_loss: 0.3306\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3455 - val_loss: 0.3542\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3451 - val_loss: 0.3306\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3444 - val_loss: 0.3368\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3439 - val_loss: 0.3458\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3435 - val_loss: 0.3579\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3432 - val_loss: 0.3316\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3424 - val_loss: 0.3279\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3417 - val_loss: 0.3259\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3409 - val_loss: 0.3572\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3411 - val_loss: 0.3221\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3405 - val_loss: 0.3379\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3401 - val_loss: 0.3506\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3397 - val_loss: 0.3305\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3392 - val_loss: 0.3222\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3387 - val_loss: 0.3292\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3381 - val_loss: 0.3354\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3380 - val_loss: 0.3307\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3374 - val_loss: 0.3215\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3370 - val_loss: 0.3219\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3366 - val_loss: 0.3289\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3359 - val_loss: 0.3427\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3360 - val_loss: 0.3298\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3354 - val_loss: 0.3317\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3350 - val_loss: 0.3223\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3343 - val_loss: 0.3379\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3337 - val_loss: 0.3206\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3338 - val_loss: 0.3278\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3334 - val_loss: 0.3501\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3331 - val_loss: 0.3319\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3326 - val_loss: 0.3283\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3324 - val_loss: 0.3166\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3319 - val_loss: 0.3315\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3314 - val_loss: 0.3331\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3314 - val_loss: 0.3295\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3308 - val_loss: 0.3274\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3308 - val_loss: 0.3234\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3304 - val_loss: 0.3348\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3302 - val_loss: 0.3236\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3298 - val_loss: 0.3331\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3294 - val_loss: 0.3180\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3289 - val_loss: 0.3191\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.2891\n",
      "[CV 4/5] END learning_rate=0.000849484301442017, n_hidden=3, n_neurons=49;, score=-0.289 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 1.8904 - val_loss: 1.5235\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7779 - val_loss: 1.1537\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6313 - val_loss: 0.9189\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5765 - val_loss: 0.7606\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5423 - val_loss: 0.6426\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5156 - val_loss: 0.5729\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4933 - val_loss: 0.5337\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4748 - val_loss: 0.4992\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4597 - val_loss: 0.4810\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4469 - val_loss: 0.4665\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4356 - val_loss: 0.4551\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4262 - val_loss: 0.4503\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4184 - val_loss: 0.4391\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4118 - val_loss: 0.4372\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4061 - val_loss: 0.4389\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4008 - val_loss: 0.4394\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3965 - val_loss: 0.4392\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3923 - val_loss: 0.4280\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3892 - val_loss: 0.4296\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3856 - val_loss: 0.4253\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3829 - val_loss: 0.4258\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3800 - val_loss: 0.4254\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3766 - val_loss: 0.4202\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3745 - val_loss: 0.4112\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3724 - val_loss: 0.4081\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3699 - val_loss: 0.3982\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3682 - val_loss: 0.3994\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3667 - val_loss: 0.4023\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3648 - val_loss: 0.4147\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3634 - val_loss: 0.3984\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3620 - val_loss: 0.3939\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3606 - val_loss: 0.3983\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3589 - val_loss: 0.4029\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3580 - val_loss: 0.3934\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3568 - val_loss: 0.3870\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3555 - val_loss: 0.3818\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3534 - val_loss: 0.3707\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3533 - val_loss: 0.3944\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3521 - val_loss: 0.3801\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3510 - val_loss: 0.3881\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3500 - val_loss: 0.3751\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3491 - val_loss: 0.3793\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3482 - val_loss: 0.3646\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3472 - val_loss: 0.3721\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3467 - val_loss: 0.3723\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3456 - val_loss: 0.3735\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3447 - val_loss: 0.3793\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3439 - val_loss: 0.3629\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3430 - val_loss: 0.3898\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3426 - val_loss: 0.3778\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3416 - val_loss: 0.3627\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3411 - val_loss: 0.3706\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3404 - val_loss: 0.3624\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3395 - val_loss: 0.3752\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3386 - val_loss: 0.3678\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3382 - val_loss: 0.3681\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3375 - val_loss: 0.3539\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3369 - val_loss: 0.3591\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3362 - val_loss: 0.3613\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3353 - val_loss: 0.3736\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3349 - val_loss: 0.3709\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3343 - val_loss: 0.3708\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3334 - val_loss: 0.3673\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3331 - val_loss: 0.3652\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3324 - val_loss: 0.3676\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3316 - val_loss: 0.3612\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3313 - val_loss: 0.3621\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3713\n",
      "[CV 5/5] END learning_rate=0.000849484301442017, n_hidden=3, n_neurons=49;, score=-0.371 total time=  43.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 0.6655 - val_loss: 3.0535\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5142 - val_loss: 4.6739\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4736 - val_loss: 0.4259\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3705 - val_loss: 0.4532\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3568 - val_loss: 0.5438\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3465 - val_loss: 0.3106\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3333 - val_loss: 0.3229\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3262 - val_loss: 0.3655\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3271 - val_loss: 0.4307\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3198 - val_loss: 0.6579\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3123 - val_loss: 0.3111\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3089 - val_loss: 0.3778\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3059 - val_loss: 0.3519\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3028 - val_loss: 0.3254\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2958 - val_loss: 0.4853\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2967 - val_loss: 0.3082\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2929 - val_loss: 0.2794\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2902 - val_loss: 0.3028\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2841 - val_loss: 0.3316\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2836 - val_loss: 0.3312\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2828 - val_loss: 0.2876\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2819 - val_loss: 0.2750\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2779 - val_loss: 0.3385\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2757 - val_loss: 0.3146\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2760 - val_loss: 0.2925\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2760 - val_loss: 0.2969\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2747 - val_loss: 0.2858\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2730 - val_loss: 0.3283\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2721 - val_loss: 0.3103\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2664 - val_loss: 0.2986\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2674 - val_loss: 0.3325\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2652 - val_loss: 0.2875\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.2942\n",
      "[CV 1/5] END learning_rate=0.02339766725509075, n_hidden=3, n_neurons=95;, score=-0.294 total time=  21.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 0.7567 - val_loss: 3.0576\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3744 - val_loss: 23.6836\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5582 - val_loss: 0.9060\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3464 - val_loss: 82.6481\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3459 - val_loss: 20.6958\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3648 - val_loss: 0.6120\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3229 - val_loss: 0.3400\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3100 - val_loss: 0.3487\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3028 - val_loss: 0.3777\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2951 - val_loss: 0.2929\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2923 - val_loss: 0.3151\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2885 - val_loss: 0.2846\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2820 - val_loss: 0.2890\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2813 - val_loss: 0.2871\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2771 - val_loss: 0.2964\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2754 - val_loss: 0.2853\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2723 - val_loss: 0.3260\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2729 - val_loss: 0.2716\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2677 - val_loss: 0.3061\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2652 - val_loss: 0.2809\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2660 - val_loss: 0.2814\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2605 - val_loss: 0.3115\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2604 - val_loss: 0.3044\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2582 - val_loss: 0.2951\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2569 - val_loss: 0.2832\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2529 - val_loss: 0.3079\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2552 - val_loss: 0.2736\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2517 - val_loss: 0.3175\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3200\n",
      "[CV 2/5] END learning_rate=0.02339766725509075, n_hidden=3, n_neurons=95;, score=-0.320 total time=  18.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 0.5641 - val_loss: 1.5237\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3822 - val_loss: 4.1910\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3616 - val_loss: 0.5919\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3511 - val_loss: 3.3558\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3390 - val_loss: 0.7393\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 100us/sample - loss: 0.3320 - val_loss: 3.8412\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3213 - val_loss: 1.5913\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3167 - val_loss: 1.5444\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3105 - val_loss: 0.3459\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.3075 - val_loss: 0.6537\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3023 - val_loss: 2.4862\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2968 - val_loss: 3.7849\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2939 - val_loss: 0.3388\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2920 - val_loss: 5.6944\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.2997 - val_loss: 2.7279\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.2876 - val_loss: 1.6897\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2859 - val_loss: 0.7811\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2848 - val_loss: 0.5401\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2820 - val_loss: 2.6224\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2769 - val_loss: 0.6672\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.2744 - val_loss: 1.0238\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.2721 - val_loss: 0.4645\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.2753 - val_loss: 0.3615\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.3338\n",
      "[CV 3/5] END learning_rate=0.02339766725509075, n_hidden=3, n_neurons=95;, score=-0.334 total time=  15.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 0.6155 - val_loss: 0.5924\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3995 - val_loss: 1.9351\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4074 - val_loss: 0.7662\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3849 - val_loss: 2.1458\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3699 - val_loss: 0.3905\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3473 - val_loss: 0.4271\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3429 - val_loss: 0.3599\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3374 - val_loss: 0.3328\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3274 - val_loss: 0.4513\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3238 - val_loss: 0.3422\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3163 - val_loss: 0.3340\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3140 - val_loss: 0.4603\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3215 - val_loss: 0.3271\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3101 - val_loss: 0.3454\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3054 - val_loss: 0.3462\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3059 - val_loss: 0.4636\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3031 - val_loss: 0.3031\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2962 - val_loss: 0.3302\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2944 - val_loss: 0.3817\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2945 - val_loss: 0.4400\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2937 - val_loss: 0.2900\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2881 - val_loss: 0.3269\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2831 - val_loss: 0.2854\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2825 - val_loss: 0.4691\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2794 - val_loss: 0.3861\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2770 - val_loss: 0.4421\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2807 - val_loss: 0.3465\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2764 - val_loss: 0.2877\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2736 - val_loss: 0.3119\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2721 - val_loss: 0.2945\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2701 - val_loss: 0.3413\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2707 - val_loss: 0.2913\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2683 - val_loss: 0.3831\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3079\n",
      "[CV 4/5] END learning_rate=0.02339766725509075, n_hidden=3, n_neurons=95;, score=-0.308 total time=  22.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 0.5978 - val_loss: 0.8137\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3960 - val_loss: 10.6907\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3627 - val_loss: 1.4017\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3601 - val_loss: 1.7501\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3419 - val_loss: 0.7959\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3321 - val_loss: 0.4368\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3220 - val_loss: 0.3880\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3147 - val_loss: 0.3038\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3119 - val_loss: 0.3394\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3037 - val_loss: 0.4358\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3031 - val_loss: 0.3110\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3008 - val_loss: 0.5534\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2976 - val_loss: 0.7626\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3013 - val_loss: 0.3737\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2949 - val_loss: 0.7817\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2927 - val_loss: 0.4643\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2870 - val_loss: 0.2838\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2816 - val_loss: 0.3107\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2813 - val_loss: 0.3556\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2830 - val_loss: 0.2818\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2762 - val_loss: 0.2986\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2720 - val_loss: 0.2955\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2738 - val_loss: 0.2845\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2704 - val_loss: 0.2982\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2700 - val_loss: 0.2761\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2661 - val_loss: 0.3148\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2629 - val_loss: 0.4167\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2650 - val_loss: 0.3910\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2706 - val_loss: 0.3953\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2630 - val_loss: 0.2926\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2614 - val_loss: 0.3730\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2617 - val_loss: 0.4075\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.2588 - val_loss: 0.5804\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2565 - val_loss: 0.3349\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2531 - val_loss: 0.3529\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3193\n",
      "[CV 5/5] END learning_rate=0.02339766725509075, n_hidden=3, n_neurons=95;, score=-0.319 total time=  23.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 1.8186 - val_loss: 0.8482\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.7874 - val_loss: 0.6983\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.7052 - val_loss: 0.6461\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6701 - val_loss: 0.6156\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6411 - val_loss: 0.5947\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6165 - val_loss: 0.5596\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5950 - val_loss: 0.5389\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5761 - val_loss: 0.5258\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5592 - val_loss: 0.5107\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5449 - val_loss: 0.4971\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5313 - val_loss: 0.4804\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5196 - val_loss: 0.4690\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5091 - val_loss: 0.4600\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4995 - val_loss: 0.4569\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4911 - val_loss: 0.4433\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4830 - val_loss: 0.4400\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4762 - val_loss: 0.4314\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4699 - val_loss: 0.4244\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4641 - val_loss: 0.4197\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4593 - val_loss: 0.4244\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4549 - val_loss: 0.4134\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4508 - val_loss: 0.4132\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4472 - val_loss: 0.4113\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4439 - val_loss: 0.4226\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4408 - val_loss: 0.4117\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4382 - val_loss: 0.4008\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4355 - val_loss: 0.3986\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4330 - val_loss: 0.4005\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4308 - val_loss: 0.3989\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4287 - val_loss: 0.3941\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4267 - val_loss: 0.3951\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4248 - val_loss: 0.3951\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4230 - val_loss: 0.4041\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4213 - val_loss: 0.3998\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4196 - val_loss: 0.4234\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4184 - val_loss: 0.3838\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4167 - val_loss: 0.3990\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4152 - val_loss: 0.4158\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4141 - val_loss: 0.4271\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4128 - val_loss: 0.4193\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4116 - val_loss: 0.4162\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4104 - val_loss: 0.4194\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4092 - val_loss: 0.4118\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4080 - val_loss: 0.4335\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4071 - val_loss: 0.3770\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4061 - val_loss: 0.4230\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4052 - val_loss: 0.3914\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4040 - val_loss: 0.4200\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4032 - val_loss: 0.3715\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4019 - val_loss: 0.4250\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4015 - val_loss: 0.3839\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4012 - val_loss: 0.3766\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3997 - val_loss: 0.3785\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3987 - val_loss: 0.4131\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3982 - val_loss: 0.3708\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3972 - val_loss: 0.3983\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3965 - val_loss: 0.3883\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3958 - val_loss: 0.3681\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3948 - val_loss: 0.4247\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3944 - val_loss: 0.4092\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3941 - val_loss: 0.3731\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3932 - val_loss: 0.3695\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3923 - val_loss: 0.3800\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3918 - val_loss: 0.4109\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3914 - val_loss: 0.3763\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3906 - val_loss: 0.4093\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3904 - val_loss: 0.3652\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3898 - val_loss: 0.3972\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3892 - val_loss: 0.3851\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3882 - val_loss: 0.3949\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3879 - val_loss: 0.4056\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3874 - val_loss: 0.3822\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3870 - val_loss: 0.3664\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3861 - val_loss: 0.3966\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3861 - val_loss: 0.3758\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3854 - val_loss: 0.3831\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3850 - val_loss: 0.3573\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3842 - val_loss: 0.3924\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3840 - val_loss: 0.3978\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3836 - val_loss: 0.3733\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3829 - val_loss: 0.3760\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3824 - val_loss: 0.3767\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3819 - val_loss: 0.3817\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3814 - val_loss: 0.3968\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3808 - val_loss: 0.3562\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3803 - val_loss: 0.4300\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3803 - val_loss: 0.4065\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3798 - val_loss: 0.3788\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3794 - val_loss: 0.3550\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3787 - val_loss: 0.4163\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3787 - val_loss: 0.3825\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3781 - val_loss: 0.3632\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3775 - val_loss: 0.3727\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3771 - val_loss: 0.4075\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3771 - val_loss: 0.3510\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3761 - val_loss: 0.3840\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3761 - val_loss: 0.8878\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3816 - val_loss: 0.4494\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3758 - val_loss: 0.3631\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3749 - val_loss: 0.3613\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3582\n",
      "[CV 1/5] END learning_rate=0.0009947592837178595, n_hidden=1, n_neurons=34;, score=-0.358 total time=  59.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 2.4624 - val_loss: 1.2915\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.8434 - val_loss: 0.7321\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7251 - val_loss: 0.6716\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6750 - val_loss: 0.6289\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6379 - val_loss: 0.5974\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6078 - val_loss: 0.5695\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5828 - val_loss: 0.5466\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5622 - val_loss: 0.5321\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5444 - val_loss: 0.5176\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5290 - val_loss: 0.5241\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5156 - val_loss: 0.4864\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5046 - val_loss: 0.4980\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4946 - val_loss: 0.4764\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4859 - val_loss: 0.4679\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4782 - val_loss: 0.4614\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4715 - val_loss: 0.4700\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4654 - val_loss: 0.4496\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4599 - val_loss: 0.4576\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4550 - val_loss: 0.4472\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4506 - val_loss: 0.4388\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4466 - val_loss: 0.4406\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4429 - val_loss: 0.4351\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4395 - val_loss: 0.4387\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4363 - val_loss: 0.4297\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4333 - val_loss: 0.4297\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4306 - val_loss: 0.4219\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4279 - val_loss: 0.4253\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4254 - val_loss: 0.4194\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4230 - val_loss: 0.4157\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4210 - val_loss: 0.4220\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4187 - val_loss: 0.4191\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4167 - val_loss: 0.4167\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4149 - val_loss: 0.4208\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4132 - val_loss: 0.4141\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4115 - val_loss: 0.4366\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4101 - val_loss: 0.4039\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4085 - val_loss: 0.3958\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4072 - val_loss: 0.4017\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4057 - val_loss: 0.4009\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4044 - val_loss: 0.3957\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4033 - val_loss: 0.3908\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4018 - val_loss: 0.4075\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4006 - val_loss: 0.3871\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3996 - val_loss: 0.4034\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3985 - val_loss: 0.3939\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3975 - val_loss: 0.3935\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3964 - val_loss: 0.3923\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3955 - val_loss: 0.3939\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3946 - val_loss: 0.3856\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3937 - val_loss: 0.3887\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3928 - val_loss: 0.3938\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3921 - val_loss: 0.3813\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3913 - val_loss: 0.3798\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3904 - val_loss: 0.3999\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3896 - val_loss: 0.3825\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3890 - val_loss: 0.3807\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3883 - val_loss: 0.3790\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3874 - val_loss: 0.3776\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3867 - val_loss: 0.3827\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3861 - val_loss: 0.3786\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3854 - val_loss: 0.3748\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3847 - val_loss: 0.3787\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3841 - val_loss: 0.3711\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3834 - val_loss: 0.3747\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3829 - val_loss: 0.3689\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3822 - val_loss: 0.3828\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3818 - val_loss: 0.3767\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3812 - val_loss: 0.3752\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3805 - val_loss: 0.3731\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3801 - val_loss: 0.3663\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3796 - val_loss: 0.3780\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3790 - val_loss: 0.3676\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3783 - val_loss: 0.3751\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3779 - val_loss: 0.3661\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3774 - val_loss: 0.3697\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3769 - val_loss: 0.3680\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3764 - val_loss: 0.3692\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3759 - val_loss: 0.3678\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3755 - val_loss: 0.3705\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3750 - val_loss: 0.3697\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3745 - val_loss: 0.3604\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3742 - val_loss: 0.3663\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3736 - val_loss: 0.3638\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3731 - val_loss: 0.3627\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3729 - val_loss: 0.3649\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3725 - val_loss: 0.3640\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3720 - val_loss: 0.3687\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3717 - val_loss: 0.3624\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3713 - val_loss: 0.3580\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3708 - val_loss: 0.3669\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3704 - val_loss: 0.3629\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3701 - val_loss: 0.3588\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3698 - val_loss: 0.3621\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3694 - val_loss: 0.3597\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3690 - val_loss: 0.3572\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3687 - val_loss: 0.3616\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3684 - val_loss: 0.3617\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3680 - val_loss: 0.3601\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3677 - val_loss: 0.3631\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3674 - val_loss: 0.3554\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.4106\n",
      "[CV 2/5] END learning_rate=0.0009947592837178595, n_hidden=1, n_neurons=34;, score=-0.411 total time=  59.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 113us/sample - loss: 2.4914 - val_loss: 1.0692\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.9077 - val_loss: 0.7864\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7624 - val_loss: 0.9278\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7089 - val_loss: 1.1816\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6722 - val_loss: 1.4748\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6419 - val_loss: 1.7491\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6160 - val_loss: 1.9027\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5928 - val_loss: 2.0078\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5721 - val_loss: 2.0406\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5537 - val_loss: 2.0292\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5371 - val_loss: 2.0240\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5222 - val_loss: 1.9805\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.6011\n",
      "[CV 3/5] END learning_rate=0.0009947592837178595, n_hidden=1, n_neurons=34;, score=-0.601 total time=   7.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 85us/sample - loss: 2.6959 - val_loss: 1.9194\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 1.0270 - val_loss: 1.0734\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.8213 - val_loss: 1.1693\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.7627 - val_loss: 0.6885\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7095 - val_loss: 0.8165\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6810 - val_loss: 0.8911\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6502 - val_loss: 0.6467\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6288 - val_loss: 0.6275\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6059 - val_loss: 0.5739\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5867 - val_loss: 0.5489\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5717 - val_loss: 0.5205\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5570 - val_loss: 0.5143\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5430 - val_loss: 0.5755\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5316 - val_loss: 0.7508\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5262 - val_loss: 0.5552\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5163 - val_loss: 0.5174\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5055 - val_loss: 0.4659\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4972 - val_loss: 0.4630\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4894 - val_loss: 0.4888\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4835 - val_loss: 0.4629\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4776 - val_loss: 0.4411\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4722 - val_loss: 0.4477\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4671 - val_loss: 0.4385\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4631 - val_loss: 0.4279\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4586 - val_loss: 0.4404\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4555 - val_loss: 0.4196\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4517 - val_loss: 0.4164\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4486 - val_loss: 0.4139\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4455 - val_loss: 0.4102\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4428 - val_loss: 0.4095\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4399 - val_loss: 0.4085\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4379 - val_loss: 0.4038\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4356 - val_loss: 0.4014\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4333 - val_loss: 0.3994\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4315 - val_loss: 0.3980\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4295 - val_loss: 0.4000\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4277 - val_loss: 0.3949\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4259 - val_loss: 0.3920\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4243 - val_loss: 0.3919\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4226 - val_loss: 0.3895\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4213 - val_loss: 0.3878\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4200 - val_loss: 0.3875\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4188 - val_loss: 0.3865\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4174 - val_loss: 0.3843\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4161 - val_loss: 0.3934\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4151 - val_loss: 0.3834\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4140 - val_loss: 0.3844\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4129 - val_loss: 0.3798\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4116 - val_loss: 0.3795\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4106 - val_loss: 0.3804\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4096 - val_loss: 0.3792\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4086 - val_loss: 0.3769\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4075 - val_loss: 0.3758\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4066 - val_loss: 0.3759\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4054 - val_loss: 0.3808\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4047 - val_loss: 0.3740\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4038 - val_loss: 0.3714\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4029 - val_loss: 0.3989\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4022 - val_loss: 0.3699\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4014 - val_loss: 0.3693\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4004 - val_loss: 0.3688\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3996 - val_loss: 0.3708\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3988 - val_loss: 0.3675\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3981 - val_loss: 0.3656\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3973 - val_loss: 0.3651\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3964 - val_loss: 0.3668\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3957 - val_loss: 0.3655\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3949 - val_loss: 0.3638\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3941 - val_loss: 0.3668\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3935 - val_loss: 0.3626\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3926 - val_loss: 0.3627\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3918 - val_loss: 0.3626\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3916 - val_loss: 0.3602\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3908 - val_loss: 0.3593\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3901 - val_loss: 0.3597\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3895 - val_loss: 0.3615\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3889 - val_loss: 0.3574\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3881 - val_loss: 0.3570\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3876 - val_loss: 0.3577\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3871 - val_loss: 0.3558\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3866 - val_loss: 0.3562\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3861 - val_loss: 0.3546\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3854 - val_loss: 0.3575\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3849 - val_loss: 0.3551\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3845 - val_loss: 0.3544\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3841 - val_loss: 0.3544\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3836 - val_loss: 0.3530\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3830 - val_loss: 0.3523\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3826 - val_loss: 0.3555\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3822 - val_loss: 0.3584\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3817 - val_loss: 0.3523\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3814 - val_loss: 0.3511\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3810 - val_loss: 0.3514\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3805 - val_loss: 0.3496\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3801 - val_loss: 0.3516\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3796 - val_loss: 0.3506\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3794 - val_loss: 0.3544\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3790 - val_loss: 0.3487\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3786 - val_loss: 0.3524\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3782 - val_loss: 0.3489\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3263\n",
      "[CV 4/5] END learning_rate=0.0009947592837178595, n_hidden=1, n_neurons=34;, score=-0.326 total time=  59.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 83us/sample - loss: 2.1173 - val_loss: 0.9580\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.8859 - val_loss: 0.8137\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7913 - val_loss: 0.7878\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7406 - val_loss: 0.7377\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.7009 - val_loss: 0.6771\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6677 - val_loss: 0.6502\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6390 - val_loss: 0.6250\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.6138 - val_loss: 0.5993\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5913 - val_loss: 0.5758\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5708 - val_loss: 0.5642\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5528 - val_loss: 0.5437\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5369 - val_loss: 0.5217\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5226 - val_loss: 0.5118\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5103 - val_loss: 0.4953\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4995 - val_loss: 0.4845\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4902 - val_loss: 0.4713\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4819 - val_loss: 0.4655\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4746 - val_loss: 0.4542\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4682 - val_loss: 0.4531\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4623 - val_loss: 0.4415\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4570 - val_loss: 0.4373\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4523 - val_loss: 0.4312\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4479 - val_loss: 0.4273\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4443 - val_loss: 0.4237\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4406 - val_loss: 0.4198\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4372 - val_loss: 0.4153\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4344 - val_loss: 0.4155\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4314 - val_loss: 0.4117\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4292 - val_loss: 0.4068\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4265 - val_loss: 0.4045\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4242 - val_loss: 0.4024\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4214 - val_loss: 0.4033\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4199 - val_loss: 0.3994\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4177 - val_loss: 0.3957\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4158 - val_loss: 0.3942\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4140 - val_loss: 0.3936\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4119 - val_loss: 0.3911\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4102 - val_loss: 0.3901\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4084 - val_loss: 0.3893\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4069 - val_loss: 0.3891\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4052 - val_loss: 0.3841\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4035 - val_loss: 0.3833\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4019 - val_loss: 0.3825\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4006 - val_loss: 0.3812\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3988 - val_loss: 0.3806\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3974 - val_loss: 0.3778\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3961 - val_loss: 0.3758\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3945 - val_loss: 0.3752\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3932 - val_loss: 0.3734\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3919 - val_loss: 0.3758\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3908 - val_loss: 0.3716\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3893 - val_loss: 0.3875\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3886 - val_loss: 0.3689\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3871 - val_loss: 0.3686\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3864 - val_loss: 0.3670\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3851 - val_loss: 0.3665\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3843 - val_loss: 0.3678\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3833 - val_loss: 0.3712\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3823 - val_loss: 0.3656\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3814 - val_loss: 0.3687\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3807 - val_loss: 0.3622\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3798 - val_loss: 0.3671\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3789 - val_loss: 0.3730\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3782 - val_loss: 0.3665\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3773 - val_loss: 0.3657\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3767 - val_loss: 0.3596\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3761 - val_loss: 0.3588\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3753 - val_loss: 0.3588\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3744 - val_loss: 0.3616\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3740 - val_loss: 0.3642\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3733 - val_loss: 0.3558\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3725 - val_loss: 0.3659\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3720 - val_loss: 0.3554\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3715 - val_loss: 0.3542\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3707 - val_loss: 0.3628\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3703 - val_loss: 0.3597\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3696 - val_loss: 0.3534\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3691 - val_loss: 0.3536\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3686 - val_loss: 0.3641\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3681 - val_loss: 0.3623\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3677 - val_loss: 0.3570\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3673 - val_loss: 0.3508\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3668 - val_loss: 0.3512\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3662 - val_loss: 0.3506\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3660 - val_loss: 0.3504\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3655 - val_loss: 0.3572\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3652 - val_loss: 0.3515\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3647 - val_loss: 0.3486\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3642 - val_loss: 0.3562\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3639 - val_loss: 0.3533\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3633 - val_loss: 0.3477\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3628 - val_loss: 0.3570\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3626 - val_loss: 0.3481\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3621 - val_loss: 0.3467\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3617 - val_loss: 0.3463\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3615 - val_loss: 0.3582\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3612 - val_loss: 0.3534\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3607 - val_loss: 0.3513\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3604 - val_loss: 0.3467\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3600 - val_loss: 0.3491\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3985\n",
      "[CV 5/5] END learning_rate=0.0009947592837178595, n_hidden=1, n_neurons=34;, score=-0.398 total time=  59.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 97us/sample - loss: 3.1424 - val_loss: 2.0475\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.1505 - val_loss: 1.7735\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.8707 - val_loss: 1.2243\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7616 - val_loss: 0.8485\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7037 - val_loss: 0.6966\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.6696 - val_loss: 0.6308\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.6453 - val_loss: 0.5963\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.6259 - val_loss: 0.5743\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6091 - val_loss: 0.5583\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5939 - val_loss: 0.5443\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5799 - val_loss: 0.5316\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5667 - val_loss: 0.5188\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5546 - val_loss: 0.5073\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5430 - val_loss: 0.4953\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5319 - val_loss: 0.4848\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5218 - val_loss: 0.4753\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5123 - val_loss: 0.4665\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5034 - val_loss: 0.4580\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4949 - val_loss: 0.4500\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4872 - val_loss: 0.4435\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4800 - val_loss: 0.4379\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4730 - val_loss: 0.4307\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4668 - val_loss: 0.4258\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4608 - val_loss: 0.4252\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4552 - val_loss: 0.4190\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4500 - val_loss: 0.4157\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4452 - val_loss: 0.4109\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4406 - val_loss: 0.4060\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4361 - val_loss: 0.4081\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4322 - val_loss: 0.4015\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4287 - val_loss: 0.4012\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4251 - val_loss: 0.4030\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4217 - val_loss: 0.4012\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4185 - val_loss: 0.3997\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4155 - val_loss: 0.3974\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4128 - val_loss: 0.3987\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4101 - val_loss: 0.4004\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4078 - val_loss: 0.3936\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4054 - val_loss: 0.3890\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4031 - val_loss: 0.3928\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4011 - val_loss: 0.3949\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3992 - val_loss: 0.3973\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3972 - val_loss: 0.3996\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3954 - val_loss: 0.4029\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3938 - val_loss: 0.3926\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3921 - val_loss: 0.3873\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3908 - val_loss: 0.3966\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3893 - val_loss: 0.3961\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3879 - val_loss: 0.3879\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3866 - val_loss: 0.3864\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3854 - val_loss: 0.3881\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3840 - val_loss: 0.3992\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3829 - val_loss: 0.3923\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3818 - val_loss: 0.3853\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3808 - val_loss: 0.3838\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3796 - val_loss: 0.3846\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3786 - val_loss: 0.3995\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3777 - val_loss: 0.3899\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3767 - val_loss: 0.3895\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3756 - val_loss: 0.3997\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3750 - val_loss: 0.3924\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3741 - val_loss: 0.3844\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3732 - val_loss: 0.3828\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3721 - val_loss: 0.3729\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3715 - val_loss: 0.3719\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3706 - val_loss: 0.3789\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3699 - val_loss: 0.3840\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3692 - val_loss: 0.3816\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3684 - val_loss: 0.3745\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3677 - val_loss: 0.3674\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3671 - val_loss: 0.3673\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3664 - val_loss: 0.3718\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3657 - val_loss: 0.3755\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3649 - val_loss: 0.3781\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3644 - val_loss: 0.3778\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3637 - val_loss: 0.3635\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3630 - val_loss: 0.3793\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3624 - val_loss: 0.3757\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3618 - val_loss: 0.3745\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3612 - val_loss: 0.3828\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3607 - val_loss: 0.3659\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3600 - val_loss: 0.3630\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3595 - val_loss: 0.3629\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3590 - val_loss: 0.3776\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3585 - val_loss: 0.3741\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3581 - val_loss: 0.3659\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3576 - val_loss: 0.3594\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3570 - val_loss: 0.3542\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3566 - val_loss: 0.3539\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3562 - val_loss: 0.3550\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3558 - val_loss: 0.3648\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3554 - val_loss: 0.3511\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3550 - val_loss: 0.3457\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3545 - val_loss: 0.3586\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3542 - val_loss: 0.3585\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3537 - val_loss: 0.3519\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3533 - val_loss: 0.3612\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3530 - val_loss: 0.3632\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3526 - val_loss: 0.3538\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3523 - val_loss: 0.3610\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3341\n",
      "[CV 1/5] END learning_rate=0.0003757926670385381, n_hidden=3, n_neurons=46;, score=-0.334 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 96us/sample - loss: 3.2526 - val_loss: 6.2451\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 1.5013 - val_loss: 6.2905\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.0655 - val_loss: 3.6811\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.8770 - val_loss: 2.2240\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7766 - val_loss: 1.3779\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7192 - val_loss: 1.0175\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6809 - val_loss: 0.8063\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6530 - val_loss: 0.6918\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6302 - val_loss: 0.6176\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6105 - val_loss: 0.5781\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5931 - val_loss: 0.5585\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5771 - val_loss: 0.5473\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.5623 - val_loss: 0.5365\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: 0.5485 - val_loss: 0.5258\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5356 - val_loss: 0.5149\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5236 - val_loss: 0.5058\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5127 - val_loss: 0.4942\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.5023 - val_loss: 0.4818\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4927 - val_loss: 0.4718\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4837 - val_loss: 0.4630\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4755 - val_loss: 0.4548\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4678 - val_loss: 0.4476\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4609 - val_loss: 0.4409\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4543 - val_loss: 0.4351\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4481 - val_loss: 0.4296\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4426 - val_loss: 0.4250\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4374 - val_loss: 0.4219\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4325 - val_loss: 0.4188\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4277 - val_loss: 0.4153\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4234 - val_loss: 0.4128\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4193 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4154 - val_loss: 0.4087\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4119 - val_loss: 0.4057\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4085 - val_loss: 0.4041\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4053 - val_loss: 0.4037\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4022 - val_loss: 0.4039\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3994 - val_loss: 0.4035\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3967 - val_loss: 0.4027\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3942 - val_loss: 0.4022\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3915 - val_loss: 0.4085\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3895 - val_loss: 0.4000\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3870 - val_loss: 0.4048\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3849 - val_loss: 0.3992\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3828 - val_loss: 0.3975\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3808 - val_loss: 0.3977\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3788 - val_loss: 0.3977\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3771 - val_loss: 0.3944\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3752 - val_loss: 0.3909\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3736 - val_loss: 0.3917\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3719 - val_loss: 0.3948\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3702 - val_loss: 0.3904\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3688 - val_loss: 0.3910\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3670 - val_loss: 0.3935\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3659 - val_loss: 0.3904\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3644 - val_loss: 0.3883\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3630 - val_loss: 0.3886\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3617 - val_loss: 0.3872\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3604 - val_loss: 0.3869\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3592 - val_loss: 0.3852\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3580 - val_loss: 0.3882\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3568 - val_loss: 0.3839\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3557 - val_loss: 0.3884\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3546 - val_loss: 0.3839\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3536 - val_loss: 0.3888\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3526 - val_loss: 0.3844\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3516 - val_loss: 0.3856\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3506 - val_loss: 0.3825\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3496 - val_loss: 0.3855\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3488 - val_loss: 0.3834\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3479 - val_loss: 0.3816\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3470 - val_loss: 0.3767\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3462 - val_loss: 0.3804\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3454 - val_loss: 0.3785\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3446 - val_loss: 0.3780\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3440 - val_loss: 0.3817\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3433 - val_loss: 0.3784\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3424 - val_loss: 0.3836\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3419 - val_loss: 0.3847\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3412 - val_loss: 0.3849\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3405 - val_loss: 0.3793\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3399 - val_loss: 0.3815\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3955\n",
      "[CV 2/5] END learning_rate=0.0003757926670385381, n_hidden=3, n_neurons=46;, score=-0.396 total time=  54.0s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 3.7578 - val_loss: 12.2723\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.5034 - val_loss: 19.0929\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.0885 - val_loss: 17.7877\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.9335 - val_loss: 14.5695\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.8448 - val_loss: 11.7944\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7898 - val_loss: 9.4783\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7523 - val_loss: 7.4336\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7243 - val_loss: 5.7268\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.7012 - val_loss: 4.3122\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6814 - val_loss: 3.2732\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6638 - val_loss: 2.6465\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6476 - val_loss: 2.1232\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6322 - val_loss: 1.6721\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6179 - val_loss: 1.3478\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6038 - val_loss: 1.0895\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5904 - val_loss: 0.8956\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5773 - val_loss: 0.7463\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5647 - val_loss: 0.6402\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5521 - val_loss: 0.5724\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5402 - val_loss: 0.5304\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5284 - val_loss: 0.5032\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5174 - val_loss: 0.4853\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5064 - val_loss: 0.4826\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4961 - val_loss: 0.4813\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4866 - val_loss: 0.4918\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4773 - val_loss: 0.4985\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4688 - val_loss: 0.5075\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4609 - val_loss: 0.5220\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4537 - val_loss: 0.5202\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4471 - val_loss: 0.5258\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4410 - val_loss: 0.5212\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4354 - val_loss: 0.5160\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4305 - val_loss: 0.5084\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4259 - val_loss: 0.4937\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.4509\n",
      "[CV 3/5] END learning_rate=0.0003757926670385381, n_hidden=3, n_neurons=46;, score=-0.451 total time=  22.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 2.9729 - val_loss: 11.3288\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.2783 - val_loss: 3.9917\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.9108 - val_loss: 1.9872\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.8177 - val_loss: 1.0224\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7698 - val_loss: 0.7494\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7391 - val_loss: 0.6855\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7146 - val_loss: 0.6601\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6926 - val_loss: 0.6434\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6728 - val_loss: 0.6259\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6543 - val_loss: 0.6161\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6372 - val_loss: 0.6052\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6215 - val_loss: 0.5899\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6062 - val_loss: 0.5728\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5919 - val_loss: 0.5526\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5784 - val_loss: 0.5550\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5664 - val_loss: 0.5356\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5548 - val_loss: 0.5118\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5437 - val_loss: 0.5190\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5335 - val_loss: 0.4941\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5241 - val_loss: 0.4837\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5153 - val_loss: 0.4735\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5069 - val_loss: 0.4636\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4991 - val_loss: 0.4557\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4921 - val_loss: 0.4463\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4854 - val_loss: 0.4392\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4789 - val_loss: 0.4345\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4733 - val_loss: 0.4272\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4678 - val_loss: 0.4233\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4628 - val_loss: 0.4171\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4580 - val_loss: 0.4146\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4537 - val_loss: 0.4110\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4496 - val_loss: 0.4063\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4457 - val_loss: 0.4022\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4421 - val_loss: 0.4075\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4383 - val_loss: 0.3975\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4358 - val_loss: 0.4071\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4331 - val_loss: 0.4009\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4304 - val_loss: 0.3993\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4278 - val_loss: 0.3983\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4255 - val_loss: 0.3944\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4232 - val_loss: 0.3964\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4212 - val_loss: 0.3936\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4192 - val_loss: 0.3883\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4172 - val_loss: 0.3922\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4154 - val_loss: 0.3941\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4136 - val_loss: 0.3895\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4121 - val_loss: 0.3932\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4106 - val_loss: 0.3933\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4091 - val_loss: 0.3822\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4076 - val_loss: 0.3763\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4063 - val_loss: 0.3857\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4049 - val_loss: 0.3810\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4035 - val_loss: 0.3908\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4022 - val_loss: 0.3762\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4008 - val_loss: 0.3788\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3999 - val_loss: 0.3741\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3986 - val_loss: 0.3844\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3973 - val_loss: 0.3862\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3965 - val_loss: 0.3763\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3953 - val_loss: 0.3821\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3944 - val_loss: 0.3673\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3932 - val_loss: 0.3880\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3923 - val_loss: 0.3864\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3914 - val_loss: 0.3862\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3905 - val_loss: 0.3843\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3896 - val_loss: 0.3814\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3887 - val_loss: 0.4116\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3889 - val_loss: 0.3572\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3870 - val_loss: 0.3599\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3862 - val_loss: 0.3607\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3854 - val_loss: 0.3698\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3847 - val_loss: 0.3780\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3838 - val_loss: 0.3676\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3832 - val_loss: 0.3597\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3825 - val_loss: 0.3670\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3815 - val_loss: 0.3789\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3809 - val_loss: 0.3622\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3803 - val_loss: 0.3792\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3269\n",
      "[CV 4/5] END learning_rate=0.0003757926670385381, n_hidden=3, n_neurons=46;, score=-0.327 total time=  51.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 128us/sample - loss: 2.6715 - val_loss: 6.0889\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 1.1907 - val_loss: 5.0747\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.9108 - val_loss: 2.4989\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7739 - val_loss: 1.5167\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7004 - val_loss: 1.0110\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6592 - val_loss: 0.7336\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6330 - val_loss: 0.6423\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6133 - val_loss: 0.5918\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5974 - val_loss: 0.5754\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5837 - val_loss: 0.5616\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5716 - val_loss: 0.5510\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5607 - val_loss: 0.5394\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.5505 - val_loss: 0.5297\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5411 - val_loss: 0.5213\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5323 - val_loss: 0.5151\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5239 - val_loss: 0.5075\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5161 - val_loss: 0.5027\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5086 - val_loss: 0.4908\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5012 - val_loss: 0.4833\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4944 - val_loss: 0.4767\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4879 - val_loss: 0.4673\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4814 - val_loss: 0.4612\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4754 - val_loss: 0.4567\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4697 - val_loss: 0.4492\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4642 - val_loss: 0.4434\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4588 - val_loss: 0.4386\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4539 - val_loss: 0.4337\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4489 - val_loss: 0.4300\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4445 - val_loss: 0.4248\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4403 - val_loss: 0.4208\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4359 - val_loss: 0.4231\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4321 - val_loss: 0.4160\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4281 - val_loss: 0.4103\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4247 - val_loss: 0.4105\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4213 - val_loss: 0.4117\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4178 - val_loss: 0.4073\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4148 - val_loss: 0.4058\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4117 - val_loss: 0.4052\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4090 - val_loss: 0.4014\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4062 - val_loss: 0.3987\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4035 - val_loss: 0.4087\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4012 - val_loss: 0.4055\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.3987 - val_loss: 0.3892\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3967 - val_loss: 0.3952\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3944 - val_loss: 0.3956\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3923 - val_loss: 0.3920\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3904 - val_loss: 0.3964\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3884 - val_loss: 0.3983\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3867 - val_loss: 0.4053\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3850 - val_loss: 0.3972\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3832 - val_loss: 0.4029\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3817 - val_loss: 0.4010\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3802 - val_loss: 0.3981\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.4210\n",
      "[CV 5/5] END learning_rate=0.0003757926670385381, n_hidden=3, n_neurons=46;, score=-0.421 total time=  34.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 106us/sample - loss: 0.8786 - val_loss: 0.6390\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5562 - val_loss: 0.5054\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4647 - val_loss: 0.5191\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4162 - val_loss: 0.4131\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3969 - val_loss: 0.4284\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3877 - val_loss: 0.4219\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3806 - val_loss: 0.4929\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3751 - val_loss: 0.4078\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3706 - val_loss: 0.3778\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3657 - val_loss: 0.4081\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3619 - val_loss: 0.4744\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3586 - val_loss: 0.4479\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3579 - val_loss: 0.3454\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3546 - val_loss: 0.4289\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3510 - val_loss: 0.3686\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3487 - val_loss: 0.3748\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3470 - val_loss: 0.3849\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3450 - val_loss: 0.4680\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3419 - val_loss: 0.3345\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3404 - val_loss: 0.3892\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3378 - val_loss: 0.3204\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3365 - val_loss: 0.3995\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3341 - val_loss: 0.4020\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3318 - val_loss: 0.4607\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3295 - val_loss: 0.3705\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3279 - val_loss: 0.3126\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3261 - val_loss: 0.3960\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3244 - val_loss: 0.3756\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3221 - val_loss: 0.4434\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3214 - val_loss: 0.3113\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3197 - val_loss: 0.3809\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3172 - val_loss: 0.3990\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3166 - val_loss: 0.3673\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3139 - val_loss: 0.4016\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3134 - val_loss: 0.3135\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3128 - val_loss: 0.3535\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3111 - val_loss: 0.3282\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3095 - val_loss: 0.3355\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3074 - val_loss: 0.5113\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3100 - val_loss: 0.3815\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3001\n",
      "[CV 1/5] END learning_rate=0.006290498650728214, n_hidden=3, n_neurons=25;, score=-0.300 total time=  26.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 98us/sample - loss: 1.0169 - val_loss: 1.3723\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5457 - val_loss: 0.6043\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4518 - val_loss: 0.4647\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4146 - val_loss: 0.5848\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3946 - val_loss: 0.9454\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3813 - val_loss: 2.2783\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3795 - val_loss: 1.4231\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3731 - val_loss: 0.4013\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3547 - val_loss: 0.3437\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3478 - val_loss: 0.3854\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3436 - val_loss: 0.3644\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3411 - val_loss: 0.3444\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3377 - val_loss: 0.3539\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3352 - val_loss: 0.3623\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3340 - val_loss: 0.3712\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3305 - val_loss: 0.3291\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3283 - val_loss: 0.4711\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3267 - val_loss: 0.3293\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3243 - val_loss: 0.4490\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3224 - val_loss: 0.3553\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3206 - val_loss: 0.7616\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3213 - val_loss: 0.5134\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3205 - val_loss: 0.3176\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3153 - val_loss: 0.3127\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3142 - val_loss: 0.3156\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3113 - val_loss: 0.3239\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3105 - val_loss: 0.3170\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3087 - val_loss: 0.3169\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3069 - val_loss: 0.3542\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3047 - val_loss: 0.3087\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3031 - val_loss: 0.3126\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3012 - val_loss: 0.3426\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2991 - val_loss: 0.2973\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2979 - val_loss: 0.3469\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2960 - val_loss: 0.3558\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2947 - val_loss: 0.2924\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2931 - val_loss: 0.3033\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2908 - val_loss: 0.2972\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2900 - val_loss: 0.3122\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2880 - val_loss: 0.2898\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2876 - val_loss: 0.3052\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2860 - val_loss: 0.2844\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2856 - val_loss: 0.2902\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2839 - val_loss: 0.3046\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2825 - val_loss: 0.2899\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2828 - val_loss: 0.2988\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2816 - val_loss: 0.2969\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2809 - val_loss: 0.3848\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2812 - val_loss: 0.2836\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2784 - val_loss: 0.2912\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2783 - val_loss: 0.2900\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2773 - val_loss: 0.2930\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2781 - val_loss: 0.2843\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2756 - val_loss: 0.3163\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2756 - val_loss: 0.2843\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2738 - val_loss: 0.2868\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2751 - val_loss: 0.2921\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2729 - val_loss: 0.2786\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2723 - val_loss: 0.2862\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2719 - val_loss: 0.2964\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2710 - val_loss: 0.2894\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2708 - val_loss: 0.2825\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2690 - val_loss: 0.2772\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.2695 - val_loss: 0.2824\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2695 - val_loss: 0.2777\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2685 - val_loss: 0.2796\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2681 - val_loss: 0.2874\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2675 - val_loss: 0.2772\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2666 - val_loss: 0.2783\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2661 - val_loss: 0.3002\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2653 - val_loss: 0.2816\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.2655 - val_loss: 0.2740\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2662 - val_loss: 0.2724\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2653 - val_loss: 0.3021\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2652 - val_loss: 0.2921\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2637 - val_loss: 0.2885\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2621 - val_loss: 0.2937\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2624 - val_loss: 0.2989\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2624 - val_loss: 0.3022\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2619 - val_loss: 0.2910\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2610 - val_loss: 0.2928\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.2602 - val_loss: 0.2877\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.2598 - val_loss: 0.2830\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3404\n",
      "[CV 2/5] END learning_rate=0.006290498650728214, n_hidden=3, n_neurons=25;, score=-0.340 total time=  55.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 97us/sample - loss: 1.0152 - val_loss: 1.0998\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6106 - val_loss: 0.5950\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5175 - val_loss: 0.4457\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4342 - val_loss: 0.3988\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3996 - val_loss: 0.4437\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3838 - val_loss: 0.4952\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3770 - val_loss: 0.6425\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3671 - val_loss: 0.7931\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3601 - val_loss: 0.9267\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3549 - val_loss: 0.7986\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3504 - val_loss: 0.8451\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3457 - val_loss: 0.9346\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3415 - val_loss: 1.2400\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3394 - val_loss: 1.2780\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3823\n",
      "[CV 3/5] END learning_rate=0.006290498650728214, n_hidden=3, n_neurons=25;, score=-0.382 total time=   9.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 94us/sample - loss: 0.9311 - val_loss: 1.2831\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5440 - val_loss: 1.0912\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4644 - val_loss: 0.4768\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4303 - val_loss: 0.4683\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4154 - val_loss: 0.4269\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4031 - val_loss: 0.3679\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3941 - val_loss: 0.5403\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3889 - val_loss: 0.4325\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3848 - val_loss: 0.3912\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3783 - val_loss: 0.3489\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3745 - val_loss: 0.4863\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3703 - val_loss: 0.4597\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3678 - val_loss: 0.4102\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3639 - val_loss: 0.3726\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3596 - val_loss: 0.3622\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3575 - val_loss: 0.4022\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3552 - val_loss: 0.4031\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3522 - val_loss: 0.3478\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3502 - val_loss: 0.3751\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3467 - val_loss: 0.3418\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3441 - val_loss: 0.4017\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3426 - val_loss: 0.3341\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3406 - val_loss: 0.3415\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3361 - val_loss: 0.3646\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3355 - val_loss: 0.3578\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3330 - val_loss: 0.3573\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3309 - val_loss: 0.3303\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3274 - val_loss: 0.3193\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3275 - val_loss: 0.3407\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3246 - val_loss: 0.3576\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3252 - val_loss: 0.3175\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3228 - val_loss: 0.3129\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3200 - val_loss: 0.3300\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3187 - val_loss: 0.3261\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3171 - val_loss: 0.3280\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3144 - val_loss: 0.3137\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3138 - val_loss: 0.3015\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3127 - val_loss: 0.3147\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3113 - val_loss: 0.3146\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3099 - val_loss: 0.3231\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3085 - val_loss: 0.3057\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3073 - val_loss: 0.3154\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3056 - val_loss: 0.3030\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3063 - val_loss: 0.3538\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3045 - val_loss: 0.3051\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3039 - val_loss: 0.3106\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3066 - val_loss: 0.3021\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.2836\n",
      "[CV 4/5] END learning_rate=0.006290498650728214, n_hidden=3, n_neurons=25;, score=-0.284 total time=  30.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 1.0508 - val_loss: 5.0760\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6059 - val_loss: 0.6241\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4705 - val_loss: 0.4365\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4285 - val_loss: 0.4375\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4022 - val_loss: 0.4742\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3889 - val_loss: 0.3768\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3764 - val_loss: 0.3876\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3710 - val_loss: 0.4115\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3659 - val_loss: 0.4169\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3593 - val_loss: 0.3826\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3552 - val_loss: 0.3646\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3506 - val_loss: 0.3505\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3483 - val_loss: 0.4855\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3452 - val_loss: 0.3836\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3420 - val_loss: 0.3431\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3405 - val_loss: 0.3620\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3365 - val_loss: 0.3906\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3340 - val_loss: 0.3848\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3315 - val_loss: 0.3279\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3298 - val_loss: 0.3640\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3276 - val_loss: 0.3805\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3268 - val_loss: 0.3921\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3220 - val_loss: 0.3723\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3202 - val_loss: 0.3538\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3182 - val_loss: 0.4342\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3239 - val_loss: 0.3250\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3195 - val_loss: 0.3256\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3160 - val_loss: 0.3315\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3122 - val_loss: 0.3854\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3117 - val_loss: 0.3348\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3110 - val_loss: 0.3170\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3085 - val_loss: 0.3244\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3060 - val_loss: 0.3202\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3059 - val_loss: 0.3688\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3027 - val_loss: 0.3213\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3035 - val_loss: 0.3342\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3018 - val_loss: 0.4016\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3003 - val_loss: 0.3436\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.2985 - val_loss: 0.3793\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.2984 - val_loss: 0.4055\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.2975 - val_loss: 1.4931\n",
      "2322/2322 [==============================] - 0s 36us/sample - loss: 0.3476\n",
      "[CV 5/5] END learning_rate=0.006290498650728214, n_hidden=3, n_neurons=25;, score=-0.348 total time=  27.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 3.1202 - val_loss: 3.0495\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.3288 - val_loss: 1.9511\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.9088 - val_loss: 1.2945\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7721 - val_loss: 0.9110\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7129 - val_loss: 0.7361\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6806 - val_loss: 0.6618\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6585 - val_loss: 0.6303\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6408 - val_loss: 0.6128\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6257 - val_loss: 0.6012\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6120 - val_loss: 0.5859\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5993 - val_loss: 0.5770\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5877 - val_loss: 0.5650\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5769 - val_loss: 0.5575\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5667 - val_loss: 0.5426\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5573 - val_loss: 0.5316\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5483 - val_loss: 0.5240\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5400 - val_loss: 0.5123\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5319 - val_loss: 0.5038\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5244 - val_loss: 0.4975\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5174 - val_loss: 0.4868\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5109 - val_loss: 0.4789\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5044 - val_loss: 0.4740\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4987 - val_loss: 0.4663\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4931 - val_loss: 0.4601\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4878 - val_loss: 0.4548\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4828 - val_loss: 0.4501\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4782 - val_loss: 0.4445\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4738 - val_loss: 0.4398\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4693 - val_loss: 0.4358\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4655 - val_loss: 0.4323\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4617 - val_loss: 0.4281\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4581 - val_loss: 0.4246\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4547 - val_loss: 0.4216\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4515 - val_loss: 0.4192\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4483 - val_loss: 0.4168\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4455 - val_loss: 0.4153\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4425 - val_loss: 0.4120\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4400 - val_loss: 0.4101\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4373 - val_loss: 0.4069\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4349 - val_loss: 0.4065\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4326 - val_loss: 0.4059\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4303 - val_loss: 0.4032\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4283 - val_loss: 0.4016\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4261 - val_loss: 0.4047\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4241 - val_loss: 0.3982\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4224 - val_loss: 0.3987\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4205 - val_loss: 0.4004\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4188 - val_loss: 0.3997\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - ETA: 0s - loss: 0.416 - 1s 71us/sample - loss: 0.4170 - val_loss: 0.3983\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4155 - val_loss: 0.3956\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4138 - val_loss: 0.3942\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4123 - val_loss: 0.3925\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4108 - val_loss: 0.3964\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4094 - val_loss: 0.3957\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4079 - val_loss: 0.3960\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4066 - val_loss: 0.3964\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4054 - val_loss: 0.3927\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4041 - val_loss: 0.3969\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4025 - val_loss: 0.3949\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4016 - val_loss: 0.3970\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4004 - val_loss: 0.3944\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3992 - val_loss: 0.4016\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3760\n",
      "[CV 1/5] END learning_rate=0.00036501893201479633, n_hidden=2, n_neurons=74;, score=-0.376 total time=  40.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 3.1458 - val_loss: 4.3964\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.2801 - val_loss: 2.8695\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.9061 - val_loss: 1.4524\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7604 - val_loss: 0.8703\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6866 - val_loss: 0.6842\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6444 - val_loss: 0.6160\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6164 - val_loss: 0.5891\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5956 - val_loss: 0.5721\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5781 - val_loss: 0.5610\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5633 - val_loss: 0.5408\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5495 - val_loss: 0.5264\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5373 - val_loss: 0.5129\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5259 - val_loss: 0.5020\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5155 - val_loss: 0.4909\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5058 - val_loss: 0.4813\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4969 - val_loss: 0.4723\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4887 - val_loss: 0.4652\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4811 - val_loss: 0.4573\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4742 - val_loss: 0.4513\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4677 - val_loss: 0.4452\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4617 - val_loss: 0.4393\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4561 - val_loss: 0.4341\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4510 - val_loss: 0.4298\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4463 - val_loss: 0.4262\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4419 - val_loss: 0.4237\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4378 - val_loss: 0.4197\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4341 - val_loss: 0.4172\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4304 - val_loss: 0.4170\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4271 - val_loss: 0.4174\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4240 - val_loss: 0.4124\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4211 - val_loss: 0.4104\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4183 - val_loss: 0.4109\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4159 - val_loss: 0.4098\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4134 - val_loss: 0.4081\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4112 - val_loss: 0.4086\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4091 - val_loss: 0.4066\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4069 - val_loss: 0.4057\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4050 - val_loss: 0.4058\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4031 - val_loss: 0.4038\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4014 - val_loss: 0.4018\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3995 - val_loss: 0.4055\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3981 - val_loss: 0.3994\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3965 - val_loss: 0.4006\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3949 - val_loss: 0.3973\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3935 - val_loss: 0.3998\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3921 - val_loss: 0.3961\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3908 - val_loss: 0.3970\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3895 - val_loss: 0.3968\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3883 - val_loss: 0.3974\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3871 - val_loss: 0.3938\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3859 - val_loss: 0.3943\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3847 - val_loss: 0.3927\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3835 - val_loss: 0.3913\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3826 - val_loss: 0.3922\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3814 - val_loss: 0.3932\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3804 - val_loss: 0.3894\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3795 - val_loss: 0.3903\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3784 - val_loss: 0.3924\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3775 - val_loss: 0.3887\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3766 - val_loss: 0.3873\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3756 - val_loss: 0.3906\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3748 - val_loss: 0.3885\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3739 - val_loss: 0.3883\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3731 - val_loss: 0.3843\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3723 - val_loss: 0.3859\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3715 - val_loss: 0.3852\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3707 - val_loss: 0.3852\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3700 - val_loss: 0.3833\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3692 - val_loss: 0.3828\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3685 - val_loss: 0.3813\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3678 - val_loss: 0.3780\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3672 - val_loss: 0.3795\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3664 - val_loss: 0.3789\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3658 - val_loss: 0.3784\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3652 - val_loss: 0.3804\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3646 - val_loss: 0.3804\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3640 - val_loss: 0.3773\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3634 - val_loss: 0.3816\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3628 - val_loss: 0.3780\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3622 - val_loss: 0.3785\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3617 - val_loss: 0.3792\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3611 - val_loss: 0.3793\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3607 - val_loss: 0.3750\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3601 - val_loss: 0.3770\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3596 - val_loss: 0.3769\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3590 - val_loss: 0.3749\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3586 - val_loss: 0.3749\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3581 - val_loss: 0.3754\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3576 - val_loss: 0.3702\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3572 - val_loss: 0.3737\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3567 - val_loss: 0.3703\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3562 - val_loss: 0.3705\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3558 - val_loss: 0.3690\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3553 - val_loss: 0.3726\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3550 - val_loss: 0.3704\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3546 - val_loss: 0.3716\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3541 - val_loss: 0.3723\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3537 - val_loss: 0.3715\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3533 - val_loss: 0.3684\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3529 - val_loss: 0.3710\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4007\n",
      "[CV 2/5] END learning_rate=0.00036501893201479633, n_hidden=2, n_neurons=74;, score=-0.401 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 3.6655 - val_loss: 8.4892\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 1.3883 - val_loss: 12.3828\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.9508 - val_loss: 11.1325\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.8051 - val_loss: 8.8708\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7420 - val_loss: 7.1806\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.7077 - val_loss: 5.7148\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6846 - val_loss: 4.8027\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.6667 - val_loss: 3.8442\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6512 - val_loss: 3.1848\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6372 - val_loss: 2.6774\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6245 - val_loss: 2.2002\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6124 - val_loss: 1.8822\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6011 - val_loss: 1.5622\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5904 - val_loss: 1.3392\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5803 - val_loss: 1.1895\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5708 - val_loss: 1.0099\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5615 - val_loss: 0.9074\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5529 - val_loss: 0.8275\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5447 - val_loss: 0.7244\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5367 - val_loss: 0.6705\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5290 - val_loss: 0.6124\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5219 - val_loss: 0.5747\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5151 - val_loss: 0.5466\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5085 - val_loss: 0.5338\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5024 - val_loss: 0.5121\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4964 - val_loss: 0.4932\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4906 - val_loss: 0.4785\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4852 - val_loss: 0.4748\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4801 - val_loss: 0.4625\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4751 - val_loss: 0.4589\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4704 - val_loss: 0.4540\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4659 - val_loss: 0.4487\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4615 - val_loss: 0.4426\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4574 - val_loss: 0.4396\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.4535 - val_loss: 0.4355\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4497 - val_loss: 0.4376\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4460 - val_loss: 0.4368\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4426 - val_loss: 0.4407\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4393 - val_loss: 0.4409\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4363 - val_loss: 0.4465\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4332 - val_loss: 0.4516\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4303 - val_loss: 0.4677\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4276 - val_loss: 0.4700\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4249 - val_loss: 0.4825\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4224 - val_loss: 0.4921\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.4401\n",
      "[CV 3/5] END learning_rate=0.00036501893201479633, n_hidden=2, n_neurons=74;, score=-0.440 total time=  29.4s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 91us/sample - loss: 4.0302 - val_loss: 4.1141\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.6029 - val_loss: 2.3772\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.0311 - val_loss: 1.1306\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.8450 - val_loss: 0.8050\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.7700 - val_loss: 0.7194\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7321 - val_loss: 0.6913\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7080 - val_loss: 0.6726\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6892 - val_loss: 0.6582\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6731 - val_loss: 0.6422\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6583 - val_loss: 0.6328\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6445 - val_loss: 0.6137\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6313 - val_loss: 0.6051\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6190 - val_loss: 0.5918\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.6068 - val_loss: 0.5902\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5954 - val_loss: 0.5806\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5842 - val_loss: 0.5595\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5741 - val_loss: 0.5464\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5640 - val_loss: 0.5412\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5544 - val_loss: 0.5211\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5454 - val_loss: 0.5153\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5368 - val_loss: 0.5127\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5289 - val_loss: 0.5065\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5211 - val_loss: 0.4996\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5140 - val_loss: 0.4867\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5072 - val_loss: 0.4739\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5005 - val_loss: 0.4673\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4946 - val_loss: 0.4628\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4889 - val_loss: 0.4574\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4834 - val_loss: 0.4572\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4785 - val_loss: 0.4500\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4737 - val_loss: 0.4428\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4692 - val_loss: 0.4377\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4650 - val_loss: 0.4319\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4610 - val_loss: 0.4298\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4572 - val_loss: 0.4246\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4539 - val_loss: 0.4241\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4506 - val_loss: 0.4170\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4475 - val_loss: 0.4150\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4446 - val_loss: 0.4120\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4418 - val_loss: 0.4087\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4393 - val_loss: 0.4054\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4369 - val_loss: 0.4020\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4346 - val_loss: 0.4004\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4324 - val_loss: 0.4004\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4304 - val_loss: 0.3965\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4284 - val_loss: 0.3948\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4266 - val_loss: 0.3928\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4248 - val_loss: 0.3912\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4231 - val_loss: 0.3900\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4215 - val_loss: 0.3883\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4199 - val_loss: 0.3868\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4184 - val_loss: 0.3859\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4170 - val_loss: 0.3858\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4155 - val_loss: 0.3869\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4142 - val_loss: 0.3830\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4128 - val_loss: 0.3831\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4116 - val_loss: 0.3833\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4103 - val_loss: 0.3823\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4092 - val_loss: 0.3823\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4079 - val_loss: 0.3800\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4068 - val_loss: 0.3805\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4057 - val_loss: 0.3786\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4046 - val_loss: 0.3787\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4035 - val_loss: 0.3809\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4025 - val_loss: 0.3774\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4014 - val_loss: 0.3744\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4005 - val_loss: 0.3758\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3995 - val_loss: 0.3785\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3986 - val_loss: 0.3751\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3978 - val_loss: 0.3767\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3968 - val_loss: 0.3733\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3960 - val_loss: 0.3759\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3952 - val_loss: 0.3731\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3945 - val_loss: 0.3730\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3936 - val_loss: 0.3705\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3928 - val_loss: 0.3712\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3920 - val_loss: 0.3728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3914 - val_loss: 0.3751\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3906 - val_loss: 0.3745\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3898 - val_loss: 0.3751\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3892 - val_loss: 0.3785\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3883 - val_loss: 0.3689\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3879 - val_loss: 0.3744\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3871 - val_loss: 0.3764\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3865 - val_loss: 0.3774\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3858 - val_loss: 0.3741\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3851 - val_loss: 0.3772\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3845 - val_loss: 0.3781\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3839 - val_loss: 0.3685\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3832 - val_loss: 0.3691\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3826 - val_loss: 0.3746\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3820 - val_loss: 0.3676\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3814 - val_loss: 0.3688\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3807 - val_loss: 0.3711\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3803 - val_loss: 0.3716\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3797 - val_loss: 0.3683\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3791 - val_loss: 0.3750\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3786 - val_loss: 0.3767\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3781 - val_loss: 0.3846\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3775 - val_loss: 0.3780\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3369\n",
      "[CV 4/5] END learning_rate=0.00036501893201479633, n_hidden=2, n_neurons=74;, score=-0.337 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 3.3016 - val_loss: 2.6419\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 1.2992 - val_loss: 2.2798\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.8854 - val_loss: 1.4603\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.7439 - val_loss: 0.9906\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6818 - val_loss: 0.7656\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6485 - val_loss: 0.6750\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6265 - val_loss: 0.6239\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6089 - val_loss: 0.5895\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5937 - val_loss: 0.5711\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5800 - val_loss: 0.5562\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5680 - val_loss: 0.5444\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5566 - val_loss: 0.5343\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5463 - val_loss: 0.5229\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5367 - val_loss: 0.5133\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5279 - val_loss: 0.5075\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5201 - val_loss: 0.4964\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5126 - val_loss: 0.4900\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5056 - val_loss: 0.4849\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4990 - val_loss: 0.4754\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4931 - val_loss: 0.4699\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4873 - val_loss: 0.4650\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4821 - val_loss: 0.4623\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4771 - val_loss: 0.4559\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4723 - val_loss: 0.4503\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4680 - val_loss: 0.4479\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4637 - val_loss: 0.4455\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4596 - val_loss: 0.4398\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4559 - val_loss: 0.4430\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4524 - val_loss: 0.4420\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4490 - val_loss: 0.4348\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4459 - val_loss: 0.4344\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4427 - val_loss: 0.4296\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4399 - val_loss: 0.4298\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4372 - val_loss: 0.4321\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4345 - val_loss: 0.4251\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4320 - val_loss: 0.4254\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4295 - val_loss: 0.4228\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4272 - val_loss: 0.4199\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4250 - val_loss: 0.4201\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4228 - val_loss: 0.4161\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4207 - val_loss: 0.4162\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4186 - val_loss: 0.4105\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4169 - val_loss: 0.4103\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4150 - val_loss: 0.4084\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4132 - val_loss: 0.4069\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4116 - val_loss: 0.4078\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4098 - val_loss: 0.4059\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4082 - val_loss: 0.4042\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4066 - val_loss: 0.4081\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4051 - val_loss: 0.4064\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4035 - val_loss: 0.4045\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4021 - val_loss: 0.4047\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4007 - val_loss: 0.4028\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3993 - val_loss: 0.4043\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3980 - val_loss: 0.3995\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3967 - val_loss: 0.4024\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3955 - val_loss: 0.4009\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3943 - val_loss: 0.3953\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3931 - val_loss: 0.4029\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3919 - val_loss: 0.4003\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3908 - val_loss: 0.4030\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3897 - val_loss: 0.3967\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3886 - val_loss: 0.3973\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3876 - val_loss: 0.3938\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3866 - val_loss: 0.3987\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3856 - val_loss: 0.3939\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3847 - val_loss: 0.3940\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3837 - val_loss: 0.3901\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3829 - val_loss: 0.3910\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3819 - val_loss: 0.3932\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3810 - val_loss: 0.3908\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3803 - val_loss: 0.3916\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3793 - val_loss: 0.3924\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3784 - val_loss: 0.3821\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3777 - val_loss: 0.3843\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3768 - val_loss: 0.3890\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3761 - val_loss: 0.3877\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3753 - val_loss: 0.3900\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3746 - val_loss: 0.3817\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3738 - val_loss: 0.3829\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3731 - val_loss: 0.3794\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3724 - val_loss: 0.3798\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3716 - val_loss: 0.3858\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3710 - val_loss: 0.3871\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3703 - val_loss: 0.3825\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3696 - val_loss: 0.3836\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3690 - val_loss: 0.3834\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3683 - val_loss: 0.3753\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3677 - val_loss: 0.3812\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3671 - val_loss: 0.3758\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3665 - val_loss: 0.3743\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3659 - val_loss: 0.3765\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3653 - val_loss: 0.3742\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3647 - val_loss: 0.3794\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3642 - val_loss: 0.3798\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3636 - val_loss: 0.3761\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3628 - val_loss: 0.3908\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3626 - val_loss: 0.3796\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3620 - val_loss: 0.3803\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3614 - val_loss: 0.3750\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3954\n",
      "[CV 5/5] END learning_rate=0.00036501893201479633, n_hidden=2, n_neurons=74;, score=-0.395 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 96us/sample - loss: 1.8883 - val_loss: 1.3151\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3516 - val_loss: 1.3150\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3515 - val_loss: 1.3182\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3518 - val_loss: 1.3163\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3515 - val_loss: 1.3154\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.3515 - val_loss: 1.3174\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 1.3516 - val_loss: 1.3174\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.3516 - val_loss: 1.3175\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.3512 - val_loss: 1.3220\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 1.3520 - val_loss: 1.3171\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.3516 - val_loss: 1.3156\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 1.3516 - val_loss: 1.3193\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 1.2966\n",
      "[CV 1/5] END learning_rate=0.007126128725820573, n_hidden=3, n_neurons=3;, score=-1.297 total time=   8.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 95us/sample - loss: 1.1972 - val_loss: 0.6708\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6653 - val_loss: 0.5963\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5901 - val_loss: 0.5242\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5165 - val_loss: 0.4514\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4630 - val_loss: 0.4129\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4406 - val_loss: 0.3999\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4335 - val_loss: 0.3964\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4302 - val_loss: 0.3943\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4266 - val_loss: 0.3911\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4243 - val_loss: 0.3934\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4232 - val_loss: 0.3913\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4202 - val_loss: 0.3904\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4202 - val_loss: 0.3924\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4192 - val_loss: 0.3906\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4179 - val_loss: 0.3854\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4170 - val_loss: 0.3848\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4158 - val_loss: 0.3872\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4153 - val_loss: 0.3897\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4144 - val_loss: 0.3903\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4127 - val_loss: 0.3845\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4125 - val_loss: 0.3836\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.4114 - val_loss: 0.3817\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4113 - val_loss: 0.3830\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4098 - val_loss: 0.3863\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4093 - val_loss: 0.3899\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4086 - val_loss: 0.3837\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4085 - val_loss: 0.3805\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4072 - val_loss: 0.3798\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4066 - val_loss: 0.3787\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4058 - val_loss: 0.3784\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4056 - val_loss: 0.3913\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4053 - val_loss: 0.3800\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.4027 - val_loss: 0.3796\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4044 - val_loss: 0.3795\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4040 - val_loss: 0.3820\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4025 - val_loss: 0.3805\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4028 - val_loss: 0.3829\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4023 - val_loss: 0.3798\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4016 - val_loss: 0.3779\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4012 - val_loss: 0.4033\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4017 - val_loss: 0.3798\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4007 - val_loss: 0.3777\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3996 - val_loss: 0.3761\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3995 - val_loss: 0.3745\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3992 - val_loss: 0.3764\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3987 - val_loss: 0.3739\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3977 - val_loss: 0.3721\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3963 - val_loss: 0.3727\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3957 - val_loss: 0.3727\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3955 - val_loss: 0.3743\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3944 - val_loss: 0.3701\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3943 - val_loss: 0.3726\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3928 - val_loss: 0.3742\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3941 - val_loss: 0.3688\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3930 - val_loss: 0.3742\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3917 - val_loss: 0.3699\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3916 - val_loss: 0.3724\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3909 - val_loss: 0.3858\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3913 - val_loss: 0.3719\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 0.3904 - val_loss: 0.3835\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3898 - val_loss: 0.3731\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3895 - val_loss: 0.3675\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3882 - val_loss: 0.3852\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3879 - val_loss: 0.3693\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3868 - val_loss: 0.3665\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3866 - val_loss: 0.3654\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3863 - val_loss: 0.3654\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3865 - val_loss: 0.3645\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3841 - val_loss: 0.3663\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3846 - val_loss: 0.3687\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3838 - val_loss: 0.3666\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3830 - val_loss: 0.3702\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3836 - val_loss: 0.3670\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3826 - val_loss: 0.3693\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3821 - val_loss: 0.3671\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3822 - val_loss: 0.3688\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3804 - val_loss: 0.3718\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3802 - val_loss: 0.3647\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4355\n",
      "[CV 2/5] END learning_rate=0.007126128725820573, n_hidden=3, n_neurons=3;, score=-0.435 total time=  50.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 89us/sample - loss: 1.2541 - val_loss: 5.7908\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7776 - val_loss: 1.1990\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6933 - val_loss: 0.6208\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6124 - val_loss: 0.5323\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5195 - val_loss: 0.4613\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4554 - val_loss: 0.4098\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4328 - val_loss: 0.3998\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4235 - val_loss: 0.3977\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4183 - val_loss: 0.3905\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4153 - val_loss: 0.3868\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4146 - val_loss: 0.3867\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4124 - val_loss: 0.3873\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4122 - val_loss: 0.3840\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4104 - val_loss: 0.3888\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4111 - val_loss: 0.3823\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4093 - val_loss: 0.3912\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4086 - val_loss: 0.3796\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4072 - val_loss: 0.3925\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4080 - val_loss: 0.3900\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4054 - val_loss: 0.3864\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4059 - val_loss: 0.3904\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4058 - val_loss: 0.3891\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4018 - val_loss: 0.3901\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4036 - val_loss: 0.3927\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4007 - val_loss: 0.3929\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4005 - val_loss: 0.3965\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4000 - val_loss: 0.3938\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.4333\n",
      "[CV 3/5] END learning_rate=0.007126128725820573, n_hidden=3, n_neurons=3;, score=-0.433 total time=  16.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 1.2402 - val_loss: 0.7135\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6478 - val_loss: 0.5328\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5388 - val_loss: 0.4620\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4772 - val_loss: 0.4192\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4569 - val_loss: 0.4248\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4501 - val_loss: 0.4052\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4439 - val_loss: 0.4362\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4417 - val_loss: 0.3949\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4383 - val_loss: 0.4101\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4370 - val_loss: 0.4003\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4388 - val_loss: 0.3897\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4361 - val_loss: 0.3924\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4358 - val_loss: 0.3966\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4349 - val_loss: 0.3898\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4345 - val_loss: 0.4327\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4336 - val_loss: 0.3944\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4309 - val_loss: 0.3883\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4308 - val_loss: 0.3881\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4320 - val_loss: 0.3885\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4300 - val_loss: 0.3933\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4297 - val_loss: 0.3869\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4279 - val_loss: 0.3909\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4277 - val_loss: 0.3828\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4274 - val_loss: 0.3835\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4284 - val_loss: 0.3836\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4274 - val_loss: 0.3867\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4252 - val_loss: 0.3956\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4247 - val_loss: 0.3817\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4241 - val_loss: 0.3837\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4238 - val_loss: 0.3845\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4234 - val_loss: 0.3849\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4233 - val_loss: 0.3816\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4233 - val_loss: 0.3788\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4214 - val_loss: 0.3856\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4221 - val_loss: 0.3803\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4230 - val_loss: 0.3794\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4216 - val_loss: 0.3786\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4202 - val_loss: 0.3810\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4202 - val_loss: 0.3900\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4210 - val_loss: 0.3774\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4196 - val_loss: 0.3802\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4199 - val_loss: 0.3894\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4205 - val_loss: 0.3791\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4197 - val_loss: 0.3873\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4190 - val_loss: 0.3835\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4207 - val_loss: 0.3801\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4173 - val_loss: 0.3972\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4201 - val_loss: 0.3775\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4181 - val_loss: 0.3835\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4187 - val_loss: 0.3999\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.3788\n",
      "[CV 4/5] END learning_rate=0.007126128725820573, n_hidden=3, n_neurons=3;, score=-0.379 total time=  31.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 121us/sample - loss: 1.0763 - val_loss: 0.9476\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5697 - val_loss: 0.7545\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4812 - val_loss: 0.6775\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4497 - val_loss: 0.6199\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4380 - val_loss: 0.5660\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4238 - val_loss: 0.5447\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4148 - val_loss: 0.5274\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4100 - val_loss: 0.5187\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4071 - val_loss: 0.5270\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4036 - val_loss: 0.5383\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4001 - val_loss: 0.5270\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3985 - val_loss: 0.5147\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3949 - val_loss: 0.5071\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3917 - val_loss: 0.4871\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3900 - val_loss: 0.4664\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3900 - val_loss: 0.4639\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3887 - val_loss: 0.4437\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3872 - val_loss: 0.4467\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3885 - val_loss: 0.4194\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3880 - val_loss: 0.4257\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3872 - val_loss: 0.4186\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3845 - val_loss: 0.4073\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3827 - val_loss: 0.4012\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3808 - val_loss: 0.3997\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3803 - val_loss: 0.3944\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3846 - val_loss: 0.3937\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3818 - val_loss: 0.3856\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3822 - val_loss: 0.3953\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3817 - val_loss: 0.3848\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3826 - val_loss: 0.3810\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3793 - val_loss: 0.3750\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3794 - val_loss: 0.3879\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3816 - val_loss: 0.3797\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3801 - val_loss: 0.3790\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3770 - val_loss: 0.3722\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3789 - val_loss: 0.3760\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3788 - val_loss: 0.3719\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3775 - val_loss: 0.3698\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3776 - val_loss: 0.3705\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3781 - val_loss: 0.3732\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3777 - val_loss: 0.3869\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3753 - val_loss: 0.3829\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3761 - val_loss: 0.3753\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3757 - val_loss: 0.3724\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3756 - val_loss: 0.3723\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3758 - val_loss: 0.3739\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3769 - val_loss: 0.3672\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3751 - val_loss: 0.3649\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3747 - val_loss: 0.3658\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3752 - val_loss: 0.3598\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3743 - val_loss: 0.3637\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3733 - val_loss: 0.3654\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3730 - val_loss: 0.3658\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3742 - val_loss: 0.3656\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3733 - val_loss: 0.3625\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3734 - val_loss: 0.3563\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3748 - val_loss: 0.3587\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3724 - val_loss: 0.3750\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3731 - val_loss: 0.3583\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3745 - val_loss: 0.3629\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3760 - val_loss: 0.3528\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3784 - val_loss: 0.3586\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3732 - val_loss: 0.3719\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3711 - val_loss: 0.3577\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3730 - val_loss: 0.3643\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3716 - val_loss: 0.3527\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3701 - val_loss: 0.3511\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3686 - val_loss: 0.3538\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3696 - val_loss: 0.3519\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3680 - val_loss: 0.3522\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3668 - val_loss: 0.3459\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3675 - val_loss: 0.3668\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3659 - val_loss: 0.3460\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3671 - val_loss: 0.3522\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3635 - val_loss: 0.3484\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3653 - val_loss: 0.3440\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3642 - val_loss: 0.3593\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3649 - val_loss: 0.3567\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3654 - val_loss: 0.3495\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3657 - val_loss: 0.3435\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3643 - val_loss: 0.3424\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3639 - val_loss: 0.3485\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3629 - val_loss: 0.3447\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3628 - val_loss: 0.3447\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3675 - val_loss: 0.3424\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3650 - val_loss: 0.3431\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3785 - val_loss: 0.3412\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3650 - val_loss: 0.3453\n",
      "Epoch 89/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3669 - val_loss: 0.3501\n",
      "Epoch 90/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3640 - val_loss: 0.3778\n",
      "Epoch 91/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3657 - val_loss: 0.3437\n",
      "Epoch 92/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3650 - val_loss: 0.3390\n",
      "Epoch 93/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3635 - val_loss: 0.3731\n",
      "Epoch 94/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3634 - val_loss: 0.3349\n",
      "Epoch 95/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3617 - val_loss: 0.3375\n",
      "Epoch 96/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3614 - val_loss: 0.3386\n",
      "Epoch 97/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3627 - val_loss: 0.3423\n",
      "Epoch 98/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3608 - val_loss: 0.3424\n",
      "Epoch 99/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3599 - val_loss: 0.3346\n",
      "Epoch 100/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3595 - val_loss: 0.3385\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3824\n",
      "[CV 5/5] END learning_rate=0.007126128725820573, n_hidden=3, n_neurons=3;, score=-0.382 total time= 1.1min\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 4.3507 - val_loss: 2.7629\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 1.3788 - val_loss: 1.3430\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.8167 - val_loss: 1.2372\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6954 - val_loss: 0.9433\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6580 - val_loss: 0.7980\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6374 - val_loss: 1.1020\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6279 - val_loss: 1.1172\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6202 - val_loss: 0.6866\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6074 - val_loss: 0.8942\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.6022 - val_loss: 0.7203\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5939 - val_loss: 0.7439\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5874 - val_loss: 0.8011\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5800 - val_loss: 1.0209\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5775 - val_loss: 0.9648\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5743 - val_loss: 0.6346\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5684 - val_loss: 0.6541\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5651 - val_loss: 0.5988\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5616 - val_loss: 0.6295\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5573 - val_loss: 0.8216\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5560 - val_loss: 0.8580\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5553 - val_loss: 0.6452\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5523 - val_loss: 0.5976\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5502 - val_loss: 0.5929\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5483 - val_loss: 0.6349\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5472 - val_loss: 0.6083\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5433 - val_loss: 0.8721\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5456 - val_loss: 0.7502\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5418 - val_loss: 0.9294\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5436 - val_loss: 0.8115\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5424 - val_loss: 0.7276\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5417 - val_loss: 0.5916\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5375 - val_loss: 0.8868\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5413 - val_loss: 0.7094\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5398 - val_loss: 0.6402\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5388 - val_loss: 0.6447\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5384 - val_loss: 0.5360\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5372 - val_loss: 0.6599\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5354 - val_loss: 0.8987\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5390 - val_loss: 0.5944\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5354 - val_loss: 0.7875\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5365 - val_loss: 0.8168\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5378 - val_loss: 0.6321\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5355 - val_loss: 0.7410\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.5359 - val_loss: 0.7691\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5364 - val_loss: 0.7343\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5345 - val_loss: 0.8779\n",
      "2322/2322 [==============================] - 0s 30us/sample - loss: 0.4999\n",
      "[CV 1/5] END learning_rate=0.0013145364874065165, n_hidden=0, n_neurons=78;, score=-0.500 total time=  23.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 3.6892 - val_loss: 2.4070\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 1.3447 - val_loss: 1.1015\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.8313 - val_loss: 0.8993\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.7006 - val_loss: 0.9628\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6607 - val_loss: 0.7464\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6393 - val_loss: 0.8964\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.6283 - val_loss: 0.6758\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.6168 - val_loss: 0.8116\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6081 - val_loss: 0.9024\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6017 - val_loss: 0.7430\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5941 - val_loss: 0.6333\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5876 - val_loss: 0.6413\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5816 - val_loss: 0.7122\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5770 - val_loss: 0.6409\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5693 - val_loss: 0.9715\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5685 - val_loss: 0.8881\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5645 - val_loss: 0.8252\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5612 - val_loss: 0.6920\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5575 - val_loss: 0.5870\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5544 - val_loss: 0.5726\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5501 - val_loss: 0.8155\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5476 - val_loss: 0.9983\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5475 - val_loss: 0.9617\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5462 - val_loss: 0.8523\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5437 - val_loss: 0.8201\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5414 - val_loss: 0.8540\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5408 - val_loss: 0.7699\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5382 - val_loss: 0.8645\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5365 - val_loss: 0.9871\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5383 - val_loss: 0.7008\n",
      "2322/2322 [==============================] - 0s 29us/sample - loss: 0.5806\n",
      "[CV 2/5] END learning_rate=0.0013145364874065165, n_hidden=0, n_neurons=78;, score=-0.581 total time=  15.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 3.4110 - val_loss: 51.4478\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 1.2094 - val_loss: 44.5959\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.7643 - val_loss: 39.7628\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.6532 - val_loss: 36.1752\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.6160 - val_loss: 33.2050\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5976 - val_loss: 30.9034\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5852 - val_loss: 29.0561\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5751 - val_loss: 27.3774\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5663 - val_loss: 26.0197\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5589 - val_loss: 24.9385\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5520 - val_loss: 23.9394\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5462 - val_loss: 22.9863\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5409 - val_loss: 22.2918\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5362 - val_loss: 21.5819\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5320 - val_loss: 20.9725\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5283 - val_loss: 20.4580\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5252 - val_loss: 19.9735\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5223 - val_loss: 19.5331\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5197 - val_loss: 19.1859\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5173 - val_loss: 18.8492\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5154 - val_loss: 18.5673\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5137 - val_loss: 18.2498\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5118 - val_loss: 18.0354\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5104 - val_loss: 17.9087\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5091 - val_loss: 17.7069\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.5079 - val_loss: 17.4695\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5071 - val_loss: 17.3215\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5062 - val_loss: 17.2638\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5054 - val_loss: 17.0237\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5046 - val_loss: 16.9469\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5039 - val_loss: 16.7512\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5036 - val_loss: 16.6182\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5029 - val_loss: 16.4334\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5027 - val_loss: 16.2815\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5022 - val_loss: 16.2134\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5019 - val_loss: 16.1425\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5016 - val_loss: 16.0275\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5012 - val_loss: 15.9797\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5011 - val_loss: 15.9893\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5008 - val_loss: 15.9760\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5005 - val_loss: 15.7623\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5001 - val_loss: 15.7762\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5002 - val_loss: 15.6562\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4999 - val_loss: 15.6183\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4999 - val_loss: 15.6247\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5000 - val_loss: 15.5867\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4999 - val_loss: 15.4751\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4996 - val_loss: 15.3824\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4998 - val_loss: 15.3840\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4994 - val_loss: 15.3565\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4996 - val_loss: 15.2711\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.4996 - val_loss: 15.2324\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4994 - val_loss: 15.3067\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4994 - val_loss: 15.3158\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4994 - val_loss: 15.2917\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4991 - val_loss: 15.3030\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4992 - val_loss: 15.3388\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5013 - val_loss: 15.2079\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4994 - val_loss: 15.2159\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4992 - val_loss: 15.2710\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4991 - val_loss: 15.1423\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4990 - val_loss: 15.1585\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4991 - val_loss: 15.2094\n",
      "Epoch 64/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4992 - val_loss: 15.1977\n",
      "Epoch 65/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4992 - val_loss: 15.1882\n",
      "Epoch 66/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4993 - val_loss: 15.1640\n",
      "Epoch 67/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4990 - val_loss: 15.1001\n",
      "Epoch 68/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.4991 - val_loss: 15.1288\n",
      "Epoch 69/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4992 - val_loss: 15.1262\n",
      "Epoch 70/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4991 - val_loss: 15.2100\n",
      "Epoch 71/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.4991 - val_loss: 15.1944\n",
      "Epoch 72/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.4990 - val_loss: 15.1982\n",
      "Epoch 73/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4993 - val_loss: 15.1182\n",
      "Epoch 74/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4992 - val_loss: 15.1438\n",
      "Epoch 75/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4989 - val_loss: 15.1717\n",
      "Epoch 76/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4992 - val_loss: 15.1279\n",
      "Epoch 77/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4991 - val_loss: 15.0434\n",
      "Epoch 78/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4989 - val_loss: 14.9929\n",
      "Epoch 79/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4991 - val_loss: 15.0504\n",
      "Epoch 80/100\n",
      "9288/9288 [==============================] - 0s 49us/sample - loss: 0.4989 - val_loss: 15.1084\n",
      "Epoch 81/100\n",
      "9288/9288 [==============================] - 0s 46us/sample - loss: 0.4989 - val_loss: 15.1386\n",
      "Epoch 82/100\n",
      "9288/9288 [==============================] - 0s 48us/sample - loss: 0.4992 - val_loss: 15.0600\n",
      "Epoch 83/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.4988 - val_loss: 15.0894\n",
      "Epoch 84/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.4992 - val_loss: 15.1078\n",
      "Epoch 85/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4991 - val_loss: 15.1111\n",
      "Epoch 86/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4991 - val_loss: 15.1096\n",
      "Epoch 87/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4992 - val_loss: 15.1211\n",
      "Epoch 88/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4990 - val_loss: 15.1069\n",
      "2322/2322 [==============================] - 0s 45us/sample - loss: 1.0944\n",
      "[CV 3/5] END learning_rate=0.0013145364874065165, n_hidden=0, n_neurons=78;, score=-1.094 total time=  46.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 88us/sample - loss: 4.3445 - val_loss: 2.4256\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 1.1682 - val_loss: 0.7384\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6885 - val_loss: 0.8815\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5920 - val_loss: 0.7793\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5676 - val_loss: 0.7705\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5614 - val_loss: 0.6272\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5553 - val_loss: 0.8903\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5569 - val_loss: 0.7123\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.5546 - val_loss: 0.6057\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5526 - val_loss: 0.6552\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5516 - val_loss: 0.6234\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5497 - val_loss: 0.7339\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5485 - val_loss: 0.8563\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5487 - val_loss: 0.8618\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5480 - val_loss: 0.8651\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5484 - val_loss: 0.7210\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5475 - val_loss: 0.6122\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5461 - val_loss: 0.6231\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5451 - val_loss: 0.7336\n",
      "2322/2322 [==============================] - 0s 48us/sample - loss: 0.4798\n",
      "[CV 4/5] END learning_rate=0.0013145364874065165, n_hidden=0, n_neurons=78;, score=-0.480 total time=  11.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 91us/sample - loss: 3.3396 - val_loss: 1.6538\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 1.1739 - val_loss: 0.9424\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.7604 - val_loss: 1.0314\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.6653 - val_loss: 0.8192\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6334 - val_loss: 0.6843\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.6175 - val_loss: 0.6348\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6043 - val_loss: 0.8643\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5966 - val_loss: 0.7343\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5877 - val_loss: 0.6937\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5778 - val_loss: 0.9658\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5748 - val_loss: 0.7311\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5663 - val_loss: 0.9141\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5621 - val_loss: 0.9491\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5590 - val_loss: 0.7542\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5542 - val_loss: 0.6296\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5489 - val_loss: 0.7967\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5475 - val_loss: 0.6842\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.5428 - val_loss: 0.8417\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5424 - val_loss: 0.6838\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5397 - val_loss: 0.5972\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5363 - val_loss: 0.7308\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5357 - val_loss: 0.6828\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5318 - val_loss: 0.9263\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5339 - val_loss: 0.7295\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5316 - val_loss: 0.6051\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5285 - val_loss: 0.8002\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5297 - val_loss: 0.6805\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5275 - val_loss: 0.7702\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5280 - val_loss: 0.5711\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.5248 - val_loss: 0.8003\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5271 - val_loss: 0.6093\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5233 - val_loss: 0.8597\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.5261 - val_loss: 0.7170\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5251 - val_loss: 0.5722\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.5234 - val_loss: 0.6913\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5225 - val_loss: 0.8657\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.5246 - val_loss: 0.7392\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.5227 - val_loss: 0.8240\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.5227 - val_loss: 0.8468\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.5621\n",
      "[CV 5/5] END learning_rate=0.0013145364874065165, n_hidden=0, n_neurons=78;, score=-0.562 total time=  22.9s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.5711 - val_loss: 2.0936\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4673 - val_loss: 70.5263\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.7520 - val_loss: 1.8100\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6652 - val_loss: 76.6668\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.6673 - val_loss: 0.4188\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3890 - val_loss: 0.4508\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3935 - val_loss: 0.3977\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3691 - val_loss: 0.4024\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3629 - val_loss: 0.3677\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3569 - val_loss: 0.3787\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3550 - val_loss: 0.3524\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3465 - val_loss: 0.3435\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3450 - val_loss: 0.3835\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3378 - val_loss: 0.4468\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3374 - val_loss: 0.3603\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3301 - val_loss: 0.3529\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3285 - val_loss: 0.3567\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3233 - val_loss: 0.4957\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3226 - val_loss: 0.3111\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3218 - val_loss: 0.3258\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3176 - val_loss: 0.3405\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3201 - val_loss: 0.3148\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3154 - val_loss: 0.5385\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3156 - val_loss: 0.3199\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3159 - val_loss: 0.3137\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3151 - val_loss: 0.5402\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3188 - val_loss: 0.3636\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3171 - val_loss: 0.2901\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3100 - val_loss: 0.3709\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3134 - val_loss: 0.3031\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3084 - val_loss: 0.3309\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3105 - val_loss: 0.3719\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3104 - val_loss: 0.3913\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3124 - val_loss: 0.2938\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 84us/sample - loss: 0.3103 - val_loss: 0.3185\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3067 - val_loss: 0.2860\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3032 - val_loss: 0.3553\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3057 - val_loss: 0.3198\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3015 - val_loss: 0.2965\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3048 - val_loss: 0.2919\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3015 - val_loss: 0.3845\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3502 - val_loss: 0.2917\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3083 - val_loss: 0.4518\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3078 - val_loss: 0.3104\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3064 - val_loss: 0.2879\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3026 - val_loss: 0.3703\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.3599\n",
      "[CV 1/5] END learning_rate=0.026729541632066298, n_hidden=1, n_neurons=81;, score=-0.360 total time=  29.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.6521 - val_loss: 0.4487\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4093 - val_loss: 2.6091\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4203 - val_loss: 9.5663\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4265 - val_loss: 21.5771\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4260 - val_loss: 2.0938\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3709 - val_loss: 0.3503\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3546 - val_loss: 0.3577\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3453 - val_loss: 0.3600\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3398 - val_loss: 0.3500\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3363 - val_loss: 0.3430\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3336 - val_loss: 0.3334\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3308 - val_loss: 0.3572\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3250 - val_loss: 0.3238\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3254 - val_loss: 0.3321\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3246 - val_loss: 0.3545\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3240 - val_loss: 0.3595\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3210 - val_loss: 0.3662\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3166 - val_loss: 0.4092\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3167 - val_loss: 0.5253\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3157 - val_loss: 0.3095\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3117 - val_loss: 0.3601\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3113 - val_loss: 0.3822\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3107 - val_loss: 0.3164\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3081 - val_loss: 0.3762\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3076 - val_loss: 0.3903\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3069 - val_loss: 0.3293\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3081 - val_loss: 0.3150\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3039 - val_loss: 0.3423\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3036 - val_loss: 0.3099\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3011 - val_loss: 0.3175\n",
      "2322/2322 [==============================] - 0s 32us/sample - loss: 0.3771\n",
      "[CV 2/5] END learning_rate=0.026729541632066298, n_hidden=1, n_neurons=81;, score=-0.377 total time=  18.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 88us/sample - loss: 0.7284 - val_loss: 3.2010\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4291 - val_loss: 1.2450\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.4020 - val_loss: 1.3751\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3797 - val_loss: 2.3466\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3895 - val_loss: 2.1240\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3665 - val_loss: 1.3960\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3582 - val_loss: 1.0327\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3640 - val_loss: 1.3266\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3521 - val_loss: 1.5601\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3507 - val_loss: 1.5423\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3546 - val_loss: 0.5986\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3469 - val_loss: 0.3252\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3514 - val_loss: 1.0059\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3593 - val_loss: 0.6066\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3347 - val_loss: 1.9024\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3331 - val_loss: 0.5124\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3333 - val_loss: 0.3314\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3336 - val_loss: 0.7729\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3392 - val_loss: 0.6413\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3299 - val_loss: 2.4553\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3255 - val_loss: 1.8127\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3396 - val_loss: 0.4150\n",
      "2322/2322 [==============================] - 0s 42us/sample - loss: 0.3540\n",
      "[CV 3/5] END learning_rate=0.026729541632066298, n_hidden=1, n_neurons=81;, score=-0.354 total time=  13.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 123us/sample - loss: 0.8104 - val_loss: 690.2780\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 76us/sample - loss: nan - val_loss: nan\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: nan\n",
      "[CV 4/5] END learning_rate=0.026729541632066298, n_hidden=1, n_neurons=81;, score=nan total time=   7.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.8949 - val_loss: 1.1986\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.6018 - val_loss: 1.0905\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4066 - val_loss: 0.9429\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3703 - val_loss: 0.5999\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3645 - val_loss: 4.3350\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3921 - val_loss: 1.1494\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3644 - val_loss: 0.3928\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3479 - val_loss: 0.3687\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3434 - val_loss: 0.3278\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3394 - val_loss: 0.3514\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3362 - val_loss: 0.3524\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.3324 - val_loss: 0.3276\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3381 - val_loss: 0.3535\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3290 - val_loss: 0.3361\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3274 - val_loss: 0.3715\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 57us/sample - loss: 0.3227 - val_loss: 0.3253\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.3216 - val_loss: 0.3125\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3234 - val_loss: 0.3884\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3199 - val_loss: 0.3090\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3157 - val_loss: 0.4295\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3162 - val_loss: 0.3363\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3126 - val_loss: 0.3428\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3119 - val_loss: 0.3261\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3124 - val_loss: 0.3997\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3107 - val_loss: 0.3284\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3074 - val_loss: 0.2950\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3102 - val_loss: 0.3000\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3104 - val_loss: 0.4008\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3085 - val_loss: 0.3111\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3070 - val_loss: 0.3263\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3087 - val_loss: 0.3120\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3048 - val_loss: 0.2986\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3048 - val_loss: 0.3522\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3036 - val_loss: 0.3236\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 58us/sample - loss: 0.3034 - val_loss: 0.2971\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3018 - val_loss: 0.3065\n",
      "2322/2322 [==============================] - 0s 47us/sample - loss: 0.3502\n",
      "[CV 5/5] END learning_rate=0.026729541632066298, n_hidden=1, n_neurons=81;, score=-0.350 total time=  21.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 2.2250 - val_loss: 2.1581\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.9030 - val_loss: 0.8525\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.7419 - val_loss: 0.6654\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.6643 - val_loss: 0.6200\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.6178 - val_loss: 0.5636\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5811 - val_loss: 0.5943\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5520 - val_loss: 0.5726\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5320 - val_loss: 0.5849\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5122 - val_loss: 0.4982\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4973 - val_loss: 0.6338\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4841 - val_loss: 0.4420\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4721 - val_loss: 0.4779\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4625 - val_loss: 0.4217\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4543 - val_loss: 0.4145\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4467 - val_loss: 0.4067\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4407 - val_loss: 0.4447\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4352 - val_loss: 0.4019\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4300 - val_loss: 0.3915\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4260 - val_loss: 0.3864\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4214 - val_loss: 0.4300\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4176 - val_loss: 0.3906\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4141 - val_loss: 0.6195\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4124 - val_loss: 0.5522\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4109 - val_loss: 1.0076\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4122 - val_loss: 0.4531\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.4046 - val_loss: 0.9618\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4045 - val_loss: 0.4869\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4000 - val_loss: 1.0036\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4000 - val_loss: 0.3626\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3946 - val_loss: 0.6199\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3935 - val_loss: 0.4339\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3922 - val_loss: 0.7743\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3922 - val_loss: 0.3570\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3883 - val_loss: 0.8013\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3894 - val_loss: 0.3560\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3848 - val_loss: 0.6540\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3849 - val_loss: 0.3898\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3829 - val_loss: 0.3577\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3817 - val_loss: 0.5514\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3804 - val_loss: 0.4225\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3790 - val_loss: 0.4487\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3778 - val_loss: 0.5061\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3773 - val_loss: 0.3593\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3756 - val_loss: 0.3941\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3749 - val_loss: 0.3968\n",
      "2322/2322 [==============================] - 0s 39us/sample - loss: 0.3539\n",
      "[CV 1/5] END learning_rate=0.000981832824996176, n_hidden=2, n_neurons=29;, score=-0.354 total time=  27.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 95us/sample - loss: 1.6938 - val_loss: 0.8103\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6513 - val_loss: 0.7349\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.5680 - val_loss: 0.5365\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5329 - val_loss: 0.5035\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.5070 - val_loss: 0.4955\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4867 - val_loss: 0.4656\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4715 - val_loss: 0.4959\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4579 - val_loss: 0.4575\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4469 - val_loss: 0.5172\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4384 - val_loss: 0.4331\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4311 - val_loss: 0.4922\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4247 - val_loss: 0.4646\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4191 - val_loss: 0.4218\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4147 - val_loss: 0.4476\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4105 - val_loss: 0.4195\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4068 - val_loss: 0.4113\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4035 - val_loss: 0.4451\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4004 - val_loss: 0.4416\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3978 - val_loss: 0.4102\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3951 - val_loss: 0.4441\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3927 - val_loss: 0.4526\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3904 - val_loss: 0.4227\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3885 - val_loss: 0.3964\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3867 - val_loss: 0.4102\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3848 - val_loss: 0.4081\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3834 - val_loss: 0.4233\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3815 - val_loss: 0.4133\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3800 - val_loss: 0.4126\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3786 - val_loss: 0.4103\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3772 - val_loss: 0.3890\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3757 - val_loss: 0.4059\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3747 - val_loss: 0.4049\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.3733 - val_loss: 0.3866\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3722 - val_loss: 0.4252\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3712 - val_loss: 0.4107\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3701 - val_loss: 0.3980\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3691 - val_loss: 0.4017\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3677 - val_loss: 0.4228\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3672 - val_loss: 0.3945\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3656 - val_loss: 0.3584\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3653 - val_loss: 0.4147\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3643 - val_loss: 0.3791\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3635 - val_loss: 0.3804\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3624 - val_loss: 0.4157\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3616 - val_loss: 0.3974\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3609 - val_loss: 0.3820\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3601 - val_loss: 0.3932\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3592 - val_loss: 0.3659\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3585 - val_loss: 0.3671\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3578 - val_loss: 0.4077\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4033\n",
      "[CV 2/5] END learning_rate=0.000981832824996176, n_hidden=2, n_neurons=29;, score=-0.403 total time=  31.3s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 94us/sample - loss: 1.9120 - val_loss: 2.8888\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.8318 - val_loss: 1.0414\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.7148 - val_loss: 0.6848\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.6662 - val_loss: 0.6818\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.6342 - val_loss: 0.7498\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6084 - val_loss: 0.8524\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5864 - val_loss: 0.9047\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5668 - val_loss: 0.9485\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5496 - val_loss: 0.9971\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5338 - val_loss: 0.9183\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5197 - val_loss: 0.8865\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5068 - val_loss: 0.8739\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4953 - val_loss: 0.7973\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4840 - val_loss: 0.7182\n",
      "2322/2322 [==============================] - 0s 38us/sample - loss: 0.5038\n",
      "[CV 3/5] END learning_rate=0.000981832824996176, n_hidden=2, n_neurons=29;, score=-0.504 total time=   9.1s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 93us/sample - loss: 2.3335 - val_loss: 0.9601\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.7811 - val_loss: 0.7209\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.6709 - val_loss: 0.7351\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.6307 - val_loss: 0.5841\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5985 - val_loss: 0.5611\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.5714 - val_loss: 0.5218\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5486 - val_loss: 0.5043\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5281 - val_loss: 0.4872\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.5104 - val_loss: 0.4611\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4946 - val_loss: 0.4577\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 77us/sample - loss: 0.4820 - val_loss: 0.4348\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4703 - val_loss: 0.4242\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4603 - val_loss: 0.4199\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4517 - val_loss: 0.4070\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4443 - val_loss: 0.4005\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4380 - val_loss: 0.3942\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4322 - val_loss: 0.3936\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4275 - val_loss: 0.3848\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4225 - val_loss: 0.3898\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4191 - val_loss: 0.3873\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4155 - val_loss: 0.3741\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4122 - val_loss: 0.3890\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.4094 - val_loss: 0.3775\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.4068 - val_loss: 0.3711\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4043 - val_loss: 0.3801\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.4023 - val_loss: 0.3627\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3999 - val_loss: 0.3659\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3981 - val_loss: 0.3769\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3963 - val_loss: 0.3861\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3950 - val_loss: 0.3618\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3934 - val_loss: 0.3642\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3916 - val_loss: 0.3801\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3906 - val_loss: 0.3593\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3889 - val_loss: 0.3804\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3881 - val_loss: 0.3532\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3863 - val_loss: 0.3671\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3854 - val_loss: 0.3508\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3840 - val_loss: 0.3743\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3833 - val_loss: 0.3513\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3821 - val_loss: 0.3786\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3812 - val_loss: 0.3554\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3802 - val_loss: 0.3923\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3796 - val_loss: 0.3575\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3787 - val_loss: 0.3509\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3777 - val_loss: 0.3481\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3766 - val_loss: 0.3960\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3763 - val_loss: 0.3457\n",
      "Epoch 48/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3757 - val_loss: 0.3554\n",
      "Epoch 49/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3747 - val_loss: 0.4066\n",
      "Epoch 50/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3744 - val_loss: 0.3770\n",
      "Epoch 51/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3737 - val_loss: 0.3528\n",
      "Epoch 52/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3727 - val_loss: 0.3624\n",
      "Epoch 53/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3725 - val_loss: 0.3419\n",
      "Epoch 54/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3714 - val_loss: 0.3838\n",
      "Epoch 55/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3711 - val_loss: 0.3642\n",
      "Epoch 56/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3704 - val_loss: 0.3482\n",
      "Epoch 57/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3700 - val_loss: 0.3781\n",
      "Epoch 58/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3693 - val_loss: 0.3914\n",
      "Epoch 59/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3689 - val_loss: 0.3774\n",
      "Epoch 60/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3683 - val_loss: 0.3732\n",
      "Epoch 61/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3679 - val_loss: 0.3584\n",
      "Epoch 62/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3673 - val_loss: 0.3506\n",
      "Epoch 63/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3666 - val_loss: 0.3817\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3180\n",
      "[CV 4/5] END learning_rate=0.000981832824996176, n_hidden=2, n_neurons=29;, score=-0.318 total time=  39.7s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 87us/sample - loss: 2.0338 - val_loss: 5.2690\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.8195 - val_loss: 0.7913\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.6885 - val_loss: 0.6470\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.6471 - val_loss: 0.6104\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.6143 - val_loss: 0.5811\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5854 - val_loss: 0.5603\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.5612 - val_loss: 0.5284\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5391 - val_loss: 0.5085\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5201 - val_loss: 0.4919\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.5031 - val_loss: 0.4819\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4886 - val_loss: 0.4710\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4755 - val_loss: 0.4586\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.4645 - val_loss: 0.4532\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4548 - val_loss: 0.4421\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4465 - val_loss: 0.4559\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4396 - val_loss: 0.4551\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4329 - val_loss: 0.4499\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.4277 - val_loss: 0.4537\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4227 - val_loss: 0.4517\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4184 - val_loss: 0.4513\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4146 - val_loss: 0.4873\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4109 - val_loss: 0.4419\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4085 - val_loss: 0.4310\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4052 - val_loss: 0.4485\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4031 - val_loss: 0.4487\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4009 - val_loss: 0.4552\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3985 - val_loss: 0.4793\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3969 - val_loss: 0.4710\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3951 - val_loss: 0.4153\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3929 - val_loss: 0.4591\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3914 - val_loss: 0.4994\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3900 - val_loss: 0.4981\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3884 - val_loss: 0.4843\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3874 - val_loss: 0.4574\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3858 - val_loss: 0.4230\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3846 - val_loss: 0.4675\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3836 - val_loss: 0.4678\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3820 - val_loss: 0.4543\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3812 - val_loss: 0.4777\n",
      "2322/2322 [==============================] - 0s 31us/sample - loss: 0.4117\n",
      "[CV 5/5] END learning_rate=0.000981832824996176, n_hidden=2, n_neurons=29;, score=-0.412 total time=  24.2s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 86us/sample - loss: 1.1189 - val_loss: 0.9933\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5826 - val_loss: 8.1917\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.5123 - val_loss: 1.1444\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4434 - val_loss: 0.3791\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4131 - val_loss: 0.3645\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3999 - val_loss: 0.3669\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3894 - val_loss: 0.3906\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3843 - val_loss: 0.3805\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3812 - val_loss: 0.3659\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3781 - val_loss: 0.3727\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3767 - val_loss: 0.3600\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3750 - val_loss: 0.3601\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3729 - val_loss: 0.3815\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3717 - val_loss: 0.3517\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3710 - val_loss: 0.3829\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3703 - val_loss: 0.3610\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3681 - val_loss: 0.3789\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3680 - val_loss: 0.3608\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3667 - val_loss: 0.3600\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3668 - val_loss: 0.3594\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3651 - val_loss: 0.3540\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3640 - val_loss: 0.3580\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3636 - val_loss: 0.3491\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3623 - val_loss: 0.3651\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3619 - val_loss: 0.3475\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3607 - val_loss: 0.3487\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3598 - val_loss: 0.3515\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3587 - val_loss: 0.3608\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3583 - val_loss: 0.3592\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3576 - val_loss: 0.3383\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3566 - val_loss: 0.3531\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 80us/sample - loss: 0.3565 - val_loss: 0.3425\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.3556 - val_loss: 0.3605\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3546 - val_loss: 0.3532\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3542 - val_loss: 0.3689\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.3534 - val_loss: 0.3614\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3527 - val_loss: 0.3516\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3512 - val_loss: 0.3492\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3506 - val_loss: 0.3469\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3502 - val_loss: 0.3484\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.3362\n",
      "[CV 1/5] END learning_rate=0.006865791385321327, n_hidden=2, n_neurons=10;, score=-0.336 total time=  25.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 119us/sample - loss: 0.8376 - val_loss: 6.3363\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5764 - val_loss: 0.4695\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4614 - val_loss: 0.4278\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.4328 - val_loss: 0.4092\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.4150 - val_loss: 0.4032\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4026 - val_loss: 0.3798\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3936 - val_loss: 0.3757\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3866 - val_loss: 0.3718\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3838 - val_loss: 0.3705\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3773 - val_loss: 0.3687\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3753 - val_loss: 0.3733\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3725 - val_loss: 0.3586\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3697 - val_loss: 0.3623\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3677 - val_loss: 0.3581\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3645 - val_loss: 0.3546\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3631 - val_loss: 0.3608\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3610 - val_loss: 0.3580\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3594 - val_loss: 0.3482\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3582 - val_loss: 0.3525\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3560 - val_loss: 0.3544\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3545 - val_loss: 0.3562\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3533 - val_loss: 0.3508\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3518 - val_loss: 0.3617\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 82us/sample - loss: 0.3512 - val_loss: 0.3579\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3505 - val_loss: 0.3572\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3490 - val_loss: 0.3759\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3481 - val_loss: 0.3664\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3475 - val_loss: 0.3542\n",
      "2322/2322 [==============================] - 0s 33us/sample - loss: 0.4044\n",
      "[CV 2/5] END learning_rate=0.006865791385321327, n_hidden=2, n_neurons=10;, score=-0.404 total time=  18.5s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.9041 - val_loss: 0.6714\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5023 - val_loss: 0.5630\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.4346 - val_loss: 0.4556\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4159 - val_loss: 0.3947\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.4005 - val_loss: 0.5426\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3938 - val_loss: 0.8295\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3873 - val_loss: 1.0459\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3855 - val_loss: 1.3762\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3822 - val_loss: 1.8776\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3831 - val_loss: 1.8767\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3743 - val_loss: 2.2381\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3722 - val_loss: 2.3459\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3708 - val_loss: 2.6220\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3680 - val_loss: 2.6516\n",
      "2322/2322 [==============================] - 0s 35us/sample - loss: 0.4825\n",
      "[CV 3/5] END learning_rate=0.006865791385321327, n_hidden=2, n_neurons=10;, score=-0.482 total time=   8.8s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 92us/sample - loss: 1.5519 - val_loss: 1.5798\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5997 - val_loss: 0.5265\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.5411 - val_loss: 0.5137\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.5083 - val_loss: 0.4946\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4849 - val_loss: 0.4348\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4669 - val_loss: 0.4227\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4532 - val_loss: 0.4010\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.4396 - val_loss: 0.3893\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.4291 - val_loss: 0.3818\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 75us/sample - loss: 0.4228 - val_loss: 0.3863\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.4166 - val_loss: 0.3948\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4109 - val_loss: 0.3875\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4063 - val_loss: 0.3805\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4019 - val_loss: 0.3802\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3994 - val_loss: 0.3718\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 1s 74us/sample - loss: 0.3937 - val_loss: 0.3777\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3918 - val_loss: 0.3822\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3892 - val_loss: 0.3865\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3869 - val_loss: 0.3689\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3835 - val_loss: 0.3614\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3839 - val_loss: 0.3688\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3805 - val_loss: 0.4010\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 79us/sample - loss: 0.3782 - val_loss: 0.3933\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3773 - val_loss: 0.3735\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 54us/sample - loss: 0.3756 - val_loss: 0.3453\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.3729 - val_loss: 0.3904\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3715 - val_loss: 0.3538\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3702 - val_loss: 0.3529\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3687 - val_loss: 0.3542\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3684 - val_loss: 0.3452\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3655 - val_loss: 0.3468\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3662 - val_loss: 0.3461\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3642 - val_loss: 0.3853\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 63us/sample - loss: 0.3630 - val_loss: 0.3317\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3628 - val_loss: 0.3407\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3608 - val_loss: 0.3305\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3623 - val_loss: 0.3607\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3602 - val_loss: 0.3480\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3591 - val_loss: 0.3546\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3580 - val_loss: 0.3564\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3578 - val_loss: 0.3703\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3562 - val_loss: 0.3548\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3547 - val_loss: 0.3462\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3542 - val_loss: 0.3410\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3529 - val_loss: 0.3423\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.3511 - val_loss: 0.3395\n",
      "2322/2322 [==============================] - 0s 34us/sample - loss: 0.3009\n",
      "[CV 4/5] END learning_rate=0.006865791385321327, n_hidden=2, n_neurons=10;, score=-0.301 total time=  28.6s\n",
      "Train on 9288 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "9288/9288 [==============================] - 1s 90us/sample - loss: 0.9783 - val_loss: 0.7325\n",
      "Epoch 2/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.5211 - val_loss: 0.5619\n",
      "Epoch 3/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4536 - val_loss: 0.5391\n",
      "Epoch 4/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4281 - val_loss: 0.5312\n",
      "Epoch 5/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.4182 - val_loss: 0.4679\n",
      "Epoch 6/100\n",
      "9288/9288 [==============================] - 1s 62us/sample - loss: 0.4105 - val_loss: 0.4795\n",
      "Epoch 7/100\n",
      "9288/9288 [==============================] - 1s 65us/sample - loss: 0.4054 - val_loss: 0.4550\n",
      "Epoch 8/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.4012 - val_loss: 0.4390\n",
      "Epoch 9/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3974 - val_loss: 0.4463\n",
      "Epoch 10/100\n",
      "9288/9288 [==============================] - 1s 55us/sample - loss: 0.3942 - val_loss: 0.4208\n",
      "Epoch 11/100\n",
      "9288/9288 [==============================] - 0s 50us/sample - loss: 0.3913 - val_loss: 0.4116\n",
      "Epoch 12/100\n",
      "9288/9288 [==============================] - 0s 51us/sample - loss: 0.3891 - val_loss: 0.4002\n",
      "Epoch 13/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3871 - val_loss: 0.3924\n",
      "Epoch 14/100\n",
      "9288/9288 [==============================] - 0s 53us/sample - loss: 0.3843 - val_loss: 0.3913\n",
      "Epoch 15/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3814 - val_loss: 0.3844\n",
      "Epoch 16/100\n",
      "9288/9288 [==============================] - 0s 52us/sample - loss: 0.3794 - val_loss: 0.3851\n",
      "Epoch 17/100\n",
      "9288/9288 [==============================] - 1s 56us/sample - loss: 0.3777 - val_loss: 0.3813\n",
      "Epoch 18/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3773 - val_loss: 0.3684\n",
      "Epoch 19/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3724 - val_loss: 0.3685\n",
      "Epoch 20/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3708 - val_loss: 0.3689\n",
      "Epoch 21/100\n",
      "9288/9288 [==============================] - 1s 59us/sample - loss: 0.3691 - val_loss: 0.3678\n",
      "Epoch 22/100\n",
      "9288/9288 [==============================] - 1s 60us/sample - loss: 0.3683 - val_loss: 0.3662\n",
      "Epoch 23/100\n",
      "9288/9288 [==============================] - 1s 61us/sample - loss: 0.3649 - val_loss: 0.3607\n",
      "Epoch 24/100\n",
      "9288/9288 [==============================] - 1s 64us/sample - loss: 0.3648 - val_loss: 0.3567\n",
      "Epoch 25/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3617 - val_loss: 0.3605\n",
      "Epoch 26/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3621 - val_loss: 0.3492\n",
      "Epoch 27/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3589 - val_loss: 0.3521\n",
      "Epoch 28/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3593 - val_loss: 0.3539\n",
      "Epoch 29/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3574 - val_loss: 0.3572\n",
      "Epoch 30/100\n",
      "9288/9288 [==============================] - 1s 67us/sample - loss: 0.3571 - val_loss: 0.3935\n",
      "Epoch 31/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3566 - val_loss: 0.3449\n",
      "Epoch 32/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3547 - val_loss: 0.3611\n",
      "Epoch 33/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3538 - val_loss: 0.3595\n",
      "Epoch 34/100\n",
      "9288/9288 [==============================] - 1s 81us/sample - loss: 0.3532 - val_loss: 0.3692\n",
      "Epoch 35/100\n",
      "9288/9288 [==============================] - 1s 78us/sample - loss: 0.3522 - val_loss: 0.3464\n",
      "Epoch 36/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3507 - val_loss: 0.3810\n",
      "Epoch 37/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3501 - val_loss: 0.3409\n",
      "Epoch 38/100\n",
      "9288/9288 [==============================] - 1s 73us/sample - loss: 0.3484 - val_loss: 0.3512\n",
      "Epoch 39/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3480 - val_loss: 0.3627\n",
      "Epoch 40/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3471 - val_loss: 0.3485\n",
      "Epoch 41/100\n",
      "9288/9288 [==============================] - 1s 66us/sample - loss: 0.3482 - val_loss: 0.3483\n",
      "Epoch 42/100\n",
      "9288/9288 [==============================] - 1s 72us/sample - loss: 0.3457 - val_loss: 0.3457\n",
      "Epoch 43/100\n",
      "9288/9288 [==============================] - 1s 69us/sample - loss: 0.3437 - val_loss: 0.3511\n",
      "Epoch 44/100\n",
      "9288/9288 [==============================] - 1s 68us/sample - loss: 0.3424 - val_loss: 0.3461\n",
      "Epoch 45/100\n",
      "9288/9288 [==============================] - 1s 70us/sample - loss: 0.3413 - val_loss: 0.3515\n",
      "Epoch 46/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3405 - val_loss: 0.3854\n",
      "Epoch 47/100\n",
      "9288/9288 [==============================] - 1s 71us/sample - loss: 0.3399 - val_loss: 0.3546\n",
      "2322/2322 [==============================] - 0s 41us/sample - loss: 0.3708\n",
      "[CV 5/5] END learning_rate=0.006865791385321327, n_hidden=2, n_neurons=10;, score=-0.371 total time=  28.6s\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 94us/sample - loss: 0.5327 - val_loss: 0.7776\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3954 - val_loss: 0.3721\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.3777 - val_loss: 1.0698\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.4111 - val_loss: 1.1860\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.3683 - val_loss: 0.9038\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 78us/sample - loss: 0.3386 - val_loss: 0.8148\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3378 - val_loss: 0.7363\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3245 - val_loss: 0.3221\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3156 - val_loss: 0.2955\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.3083 - val_loss: 0.3190\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.3037 - val_loss: 0.2982\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.2981 - val_loss: 0.3108\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2946 - val_loss: 0.3093\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2904 - val_loss: 0.3197\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.2914 - val_loss: 0.2832\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.2883 - val_loss: 0.2806\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.2825 - val_loss: 0.3021\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.2809 - val_loss: 0.3011\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.2799 - val_loss: 0.3376\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.2759 - val_loss: 0.2917\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 66us/sample - loss: 0.2734 - val_loss: 0.3003\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2707 - val_loss: 0.5070\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2711 - val_loss: 0.3749\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.2697 - val_loss: 0.2605\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2645 - val_loss: 0.2706\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.2627 - val_loss: 0.2710\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2640 - val_loss: 0.2807\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 69us/sample - loss: 0.2620 - val_loss: 0.2511\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 73us/sample - loss: 0.2588 - val_loss: 0.2821\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2566 - val_loss: 0.2729\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2524 - val_loss: 0.3030\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 71us/sample - loss: 0.2531 - val_loss: 0.2808\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.2531 - val_loss: 0.3723\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.2556 - val_loss: 0.3207\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.2556 - val_loss: 0.2740\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.2515 - val_loss: 0.2572\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 67us/sample - loss: 0.2494 - val_loss: 0.2796\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 68us/sample - loss: 0.2488 - val_loss: 0.3111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001CF76D31F98>,\n",
       "                   n_iter=50,\n",
       "                   param_distributions={'learning_rate': [0.0003088305950596277,\n",
       "                                                          0.002495602424607858,\n",
       "                                                          0.0071838790433400605,\n",
       "                                                          0.015780747125008227,\n",
       "                                                          0.003860894096331268,\n",
       "                                                          0.0005997160123002173,\n",
       "                                                          0.003560718354812314,\n",
       "                                                          0.0034861900012054334,\n",
       "                                                          0.00463...\n",
       "                                                          0.00033368338318311494,\n",
       "                                                          0.0024500633844252886,\n",
       "                                                          0.010651700979207746,\n",
       "                                                          0.020626570171785423,\n",
       "                                                          0.017921282292549218,\n",
       "                                                          0.011351612134119027,\n",
       "                                                          0.012275653075878143,\n",
       "                                                          0.012717747408516621,\n",
       "                                                          0.004951446186184391,\n",
       "                                                          0.0003221023401251936,\n",
       "                                                          0.0010526297927427543, ...],\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20, 21,\n",
       "                                                      22, 23, 24, 25, 26, 27,\n",
       "                                                      28, 29, 30, ...]},\n",
       "                   verbose=3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100)               .tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2)      .rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=50, cv=5, verbose=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "74daf9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 95, 'n_hidden': 3, 'learning_rate': 0.02339766725509075}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40f5ecd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3150370457478375"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69a3f536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor at 0x1cf01369f60>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fef5b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 34us/sample - loss: 0.2860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.2860388092869936"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00c098c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1cf78621d30>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = rnd_search_cv.best_estimator_.model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8448322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 35us/sample - loss: 0.2860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2860388092869936"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78b7040",
   "metadata": {},
   "source": [
    "# End of Chapter Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a71f5b",
   "metadata": {},
   "source": [
    "1. The TensorFlow Playground is a handy neural network simulator built by the\n",
    "   TensorFlow team. In this exercise, you will train several binary classifiers in just a\n",
    "   few clicks, and tweak the models architecture and its hyperparameters to gain\n",
    "   some intuition on how neural networks work and what their hyperparameters\n",
    "   do. Take some time to explore the following:\n",
    "   \n",
    "   \n",
    "    a. The patterns learned by a neural net. Try training the default neural network\n",
    "    by clicking the Run button (top left). Notice how it quickly finds a good solu\n",
    "    tion for the classification task. The neurons in the first hidden layer have\n",
    "    learned simple patterns, while the neurons in the second hidden layer have\n",
    "    learned to combine the simple patterns of the first hidden layer into more\n",
    "    complex patterns. In general, the more layers there are, the more complex the\n",
    "    patterns can be.\n",
    "\n",
    "\n",
    "    b. Activation functions. Try replacing the tanh activation function with a ReLU\n",
    "    activation function, and train the network again. Notice that it finds a solution\n",
    "    even faster, but this time the boundaries are linear. This is due to the shape of\n",
    "    the ReLU function.\n",
    "\n",
    "\n",
    "    c. The risk of local minima. Modify the network architecture to have just one\n",
    "    hidden layer with three neurons. Train it multiple times (to reset the network\n",
    "    weights, click the Reset button next to the Play button). Notice that the train\n",
    "    ing time varies a lot, and sometimes it even gets stuck in a local minimum.\n",
    "\n",
    "\n",
    "    d. What happens when neural nets are too small. Remove one neuron to keep\n",
    "    just two. Notice that the neural network is now incapable of finding a good\n",
    "    solution, even if you try multiple times. The model has too few parameters\n",
    "    and systematically underfits the training set.\n",
    "\n",
    "\n",
    "    e. What happens when neural nets are large enough. Set the number of neurons\n",
    "    to eight, and train the network several times. Notice that it is now consistently\n",
    "    fast and never gets stuck. This highlights an important finding in neural net\n",
    "    work theory: large neural networks almost never get stuck in local minima,\n",
    "    and even when they do these local optima are almost as good as the global\n",
    "    optimum. However, they can still get stuck on long plateaus for a long time.\n",
    "\n",
    "\n",
    "    f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the\n",
    "    bottom-right dataset under DATA), and change the network architecture to\n",
    "    have four hidden layers with eight neurons each. Notice that training takes\n",
    "    much longer and often gets stuck on plateaus for long periods of time. Also\n",
    "    notice that the neurons in the highest layers (on the right) tend to evolve\n",
    "    faster than the neurons in the lowest layers (on the left). This problem, called\n",
    "    the vanishing gradients problem, can be alleviated with better weight initial\n",
    "    ization and other techniques, better optimizers (such as AdaGrad or Adam),\n",
    "    or Batch Normalization (discussed in Chapter 11).\n",
    "    \n",
    "    g. Go further. Take an hour or so to play around with other parameters and get a\n",
    "    feel for what they do, to build an intuitive understanding about neural\n",
    "    networks.\n",
    "\n",
    "2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\n",
    "   that computes A  B (where  represents the XOR operation). Hint: A  B =\n",
    "   (A   B  ( A  B).\n",
    "3. Why is it generally preferable to use a Logistic Regression classifier rather than a\n",
    "   classical Perceptron (i.e., a single layer of threshold logic units trained using the\n",
    "   Perceptron training algorithm)? How can you tweak a Perceptron to make it\n",
    "   equivalent to a Logistic Regression classifier?\n",
    "4. Why was the logistic activation function a key ingredient in training the first\n",
    "   MLPs?\n",
    "5. Name three popular activation functions. Can you draw them?\n",
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough\n",
    "   neurons, followed by one hidden layer with 50 artificial neurons, and finally one\n",
    "   output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\n",
    "   tion function.\n",
    "    What is the shape of the input matrix X?\n",
    "    What are the shapes of the hidden layers weight vector Wh\n",
    "     and its bias vector bh?\n",
    "    What are the shapes of the output layers weight vector Wo\n",
    "     and its bias vector bo?\n",
    "    What is the shape of the networks output matrix Y?\n",
    "    Write the equation that computes the networks output matrix Y as a function\n",
    "   of X, Wh, bh, Wo, and bo.\n",
    "7. How many neurons do you need in the output layer if you want to classify email\n",
    "   into spam or ham? What activation function should you use in the output layer?\n",
    "   If instead you want to tackle MNIST, how many neurons do you need in the out\n",
    "   put layer, and which activation function should you use? What about for getting\n",
    "   your network to predict housing prices, as in Chapter 2?\n",
    "8. What is backpropagation and how does it work? What is the difference between\n",
    "   backpropagation and reverse-mode autodiff?\n",
    "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP\n",
    "   overfits the training data, how could you tweak these hyperparameters to try to\n",
    "   solve the problem?\n",
    "10. Train a deep MLP on the MNIST dataset (you can load it using keras.data\n",
    "    sets.mnist.load_data(). See if you can get over 98% precision. Try searching\n",
    "    for the optimal learning rate by using the approach presented in this chapter (i.e.,\n",
    "    by growing the learning rate exponentially, plotting the loss, and finding the\n",
    "    point where the loss shoots up). Try adding all the bells and whistlessave\n",
    "    checkpoints, use early stopping, and plot learning curves using TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd0f2f",
   "metadata": {},
   "source": [
    "***1. The TensorFlow Playground is a handy neural network simulator built by the\n",
    "   TensorFlow team. In this exercise, you will train several binary classifiers in just a\n",
    "   few clicks, and tweak the models architecture and its hyperparameters to gain\n",
    "   some intuition on how neural networks work and what their hyperparameters\n",
    "   do. Take some time to explore the following:***\n",
    "   \n",
    "   \n",
    " ***a. The patterns learned by a neural net. Try training the default neural network\n",
    "    by clicking the Run button (top left). Notice how it quickly finds a good solu\n",
    "    tion for the classification task. The neurons in the first hidden layer have\n",
    "    learned simple patterns, while the neurons in the second hidden layer have\n",
    "    learned to combine the simple patterns of the first hidden layer into more\n",
    "    complex patterns. In general, the more layers there are, the more complex the\n",
    "    patterns can be.***\n",
    "\n",
    "\n",
    " ***b. Activation functions. Try replacing the tanh activation function with a ReLU\n",
    "    activation function, and train the network again. Notice that it finds a solution\n",
    "    even faster, but this time the boundaries are linear. This is due to the shape of\n",
    "    the ReLU function.***\n",
    "\n",
    "\n",
    " ***c. The risk of local minima. Modify the network architecture to have just one\n",
    "    hidden layer with three neurons. Train it multiple times (to reset the network\n",
    "    weights, click the Reset button next to the Play button). Notice that the train\n",
    "    ing time varies a lot, and sometimes it even gets stuck in a local minimum.***\n",
    "\n",
    "\n",
    " ***d. What happens when neural nets are too small. Remove one neuron to keep\n",
    "    just two. Notice that the neural network is now incapable of finding a good\n",
    "    solution, even if you try multiple times. The model has too few parameters\n",
    "    and systematically underfits the training set.***\n",
    "\n",
    "\n",
    " ***e. What happens when neural nets are large enough. Set the number of neurons\n",
    "    to eight, and train the network several times. Notice that it is now consistently\n",
    "    fast and never gets stuck. This highlights an important finding in neural net\n",
    "    work theory: large neural networks almost never get stuck in local minima,\n",
    "    and even when they do these local optima are almost as good as the global\n",
    "    optimum. However, they can still get stuck on long plateaus for a long time.***\n",
    "\n",
    "\n",
    " ***f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the\n",
    "    bottom-right dataset under DATA), and change the network architecture to\n",
    "    have four hidden layers with eight neurons each. Notice that training takes\n",
    "    much longer and often gets stuck on plateaus for long periods of time. Also\n",
    "    notice that the neurons in the highest layers (on the right) tend to evolve\n",
    "    faster than the neurons in the lowest layers (on the left). This problem, called\n",
    "    the vanishing gradients problem, can be alleviated with better weight initial\n",
    "    ization and other techniques, better optimizers (such as AdaGrad or Adam),\n",
    "    or Batch Normalization (discussed in Chapter 11).***\n",
    "    \n",
    " ***g. Go further. Take an hour or so to play around with other parameters and get a\n",
    "    feel for what they do, to build an intuitive understanding about neural\n",
    "    networks.***\n",
    "    \n",
    "## DONE IN BROWSER, SPECIFICALLY THIS LINK: https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.66473&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "attachments": {
    "EndofChapterQuestion2.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "554a6fbc",
   "metadata": {},
   "source": [
    "***2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\n",
    "   that computes A  B (where  represents the XOR operation). Hint: A  B =\n",
    "   (A   B  ( A  B).***\n",
    "   \n",
    "A:\n",
    "\n",
    "I drew this up in paint quickly where it shows both A and !B and then !A and B! in our diagram for A  B\n",
    "\n",
    "![EndofChapterQuestion2.PNG](attachment:EndofChapterQuestion2.PNG)\n",
    "\n",
    "***3. Why is it generally preferable to use a Logistic Regression classifier rather than a\n",
    "   classical Perceptron (i.e., a single layer of threshold logic units trained using the\n",
    "   Perceptron training algorithm)? How can you tweak a Perceptron to make it\n",
    "   equivalent to a Logistic Regression classifier?***\n",
    "   \n",
    "A: A classical logistic regression classifer is better than a perceptron because=perceptrons do not output a class probabilty. instead they make a prediction based on a hard threshold. This could be solved by stacking several perceptrons. This is known as a multilayer perceptron (MLP). Now with this, it would output a 0 or 1 as the perceptron fires. \n",
    "\n",
    "***4. Why was the logistic activation function a key ingredient in training the first\n",
    "   MLPs?***\n",
    "   \n",
    "A: The reason that the logistic (or sigmoid) activation function was used in training was because step functions were only associated with flat segments. With this there wasn't any actual gradients to work with. So with the sigmoid function gradient descent could actually make proress at each sucessive step. \n",
    "\n",
    "***5. Name three popular activation functions. Can you draw them?***\n",
    "\n",
    "A: \n",
    "\n",
    "1. ReLU\n",
    "2. TanH\n",
    "3. Sigmoid\n",
    "\n",
    "Drawings shown below: "
   ]
  },
  {
   "attachments": {
    "ReLU.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAABAIAAAHTCAYAAAC9erqYAAAu3klEQVR42uzdC5iddX3o+9//XTMEBuUimgjatB6tCUYf0UR6pEkQoYItKAEvp7KrFsgMpSVQT28+ex9jtKfaWrslobQzCRSfFlsvMMDYo7gNSiatZwMRsEaCl7IPgggWgtpEQma9v/PMZG6JuUzuc/l8nmceZs16FzPzX2vy/t/vei9VAABTXmfP2plGAQCmBiEAAIiqr7nk2ht7jzcSACAEAABTQjl3Wlu+0TgAgBAAAExynT1rZ2bmrGaJs40GAAgBAMAk1+jLvx389F2dN39lvhEBACEAAJjE6sjXb/skj26U1rlGBACEAABgklq6NKvI8tzhKFDqOUYFAIQAAGCSOunVXz0tom4M3S6ZrzYqACAEAACTVImWHa4UUL26s7OnzcgAgBAAAEzGEFDq3xh9O7Oe1jL9GCcMBAAhAACYbK69sff4zOrnTg7YjPh1owMAQgAAMMkccWT95tHnBxhSopxjdABACAAAJt0soHrTzr6cmbM6e9bONEAAIAQAAJNIljxjlxOEZ5u/ZoQAQAgAACaJrs+tnZN17vpd/6qca5QAQAgAACaLRp69u7sz8g1Ll6Z5AgAIAQDAZFBK/EbsvgQcd9Krv3qakQIAIQAAmOA6O3vaIuNXR2YDZdNwIKjKw8OfR8sbjRYACAEAwATXMv2Y+Zn1tIGN/VIejIytw3dmfH04BJT6N4wWAAgBAMAE14z49ZFb+fnR92WWeyOq5rbPq7nX3th7vBEDACEAAJjASpRzRm5UW6NE68jMIF9cqnx02426Ma0tHR4AAEIAADBRdfasnZmZs4ZuZ13/SdR59PACdS4efVnBZomzjRoACAEAwERd8T/b/LW9Wb6EEAAAk02LIQCAqaNUjedE1Lfs8NU3j5w8sLovIv/X8F0ZsfK21TMWv+XMx40eAEyS+YAhAICprat7zcbIPC4GriTYuOrSRfOvNioAMHk5NAAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmECEAAAAAphAhAAAAAKYQIQAAAACmkBZDAAAAMHW1d97T2rtp7suzNeZGHa+KEvNKxi/WVcx/8Irygx2Xn311fqxEXJglbtiwpHzoUP+8J/9VzsmWuDAizomIOSXiC+94Mt61bFmph5aZtSJPqjI+GhFnZcb1G64s/80zPcIeAQAAABPAycvzrP6N9gP9/+3dMvf0/g3rknFWVeKqkvGGiHhJqePyHZddujSrKNHef39kLJ19TZ5wKMdg9tXZno14b5R4tmS8sGQcExnv/OzzB8LAgDnL86VVHWsj47ci48QoccXBGLeJrBgCAJjaurrXbIzM4/o/r0rjqksXzb/aqACML3Ouyfc0m3F9lLht4bR17+jqmLf1YHyfWcvzvCrjtsGbD73jqXjZ6HfaB4PEVZnx8RJRRYlfe2BJ+fLoUPCZE+KGiDhif3+Wtmlx8bqOsnksP2uWuH7DknLJ7GvyhFLHnZmxvI74VkuJd5Uq/uf63yuf9Coa4dAAAACACRABBja8M85fs2XuZ9o77zkoMeDBJaVn9vK8vWScHREv+fTz4jURsW70Mg8sKZ94xYp8NOv4TFZx7+j7PnNsnDzwTvz+KvHY7iJAv+dMi9U/2xJ9mdFSIn5l4GF1/HVmLN9wZekaXGytV5AQAAAAMDEjwNA28kGOARnx1yUGQkC/RTuGgH51Hf9eIh7a8HvlydFfr1rimWaJ9+/vz1BFfG9Py6zrKJtnL8+1JeINmXFy/1jVdTw7KgIgBAAAAEzsCHAoYsBzpsXqTVviJyXjmFLi/Ij4uRPtlRILMgYOAdjO+iWlfwP+o4dskDL+JSLe0D9GdR0fySpe5ZWzZ04WCAAAMIEiwE5iwAE9Ed7gLvl3DG5oz5m1Ik/6+e3veEeWWHW4x6mU+Orwz5Rx4457KCAEAAAATIoIMDoG9G6Z+/cH/IfI+MLQpy0lfm30Xa9cnqeXiO/u7NKCh1rbtPjXjKgHo8AMrx4hAAAAYNJGgG3b61FX1chG+4FSmgO73A9o1rFw9H19Ee/PjHFxhZn/fDaOq0oMXdXgtV5BY+McAQAAABM0AjQacfHBuDTeA+8r609eno9FxolDZ+SPwb0B+iIe3XBl+bkTCJ68PM/KHD7J4D7LEmseXFJ6xrJsybg+I34UESdGxpzZ1+QJDg8QAgAAAESAfbMmIt5ZIma1d97TeuIP5jY/HfHBLHHRLpa/tES8c7+/a8Z3xrLY7KuzPSJ+VlXx/rq57cSFWcdpEdHjFSUEAAAAiAB7uz2ecUf/hn1mtPRumvvy6gUxLzJu39W5AbLEfVXEU/v7fauI1Xscs+X50jpiSVZxel/GtKGBKzlwGMNwCFi6NKtly0rtVSYEAAAAiAB7Nrz7fzbiV5t1vPfoaXHWrhbecEU5JJcNbO+8p3XNlvj7yHjP0GEAJ1+dD0XES6LEGUPLzb4mT/hsHR+OiMu90oQAAAAAEWAPFh657htrt8zty4yWKPGxqsSlg5cWPKRO/quck4341WjETf0b/mu2zP3byLhh9HkKssRXSsZLSsbc/uXbjo6HNm2JVSXiD7zSfp6rBgAAAIgAP6erY97WOuL+wZtf/9YV5bOHZbAa0VMiOqOOfz95eX6z/0sbrixdO4zPLcM3WuJ/bH4mvtmo4pb1S8r3vNqEAAAAABFg7L6fJX7SiLj0cI1Xlvh2bDv+/5iI+ObCaesu23GZ06et+2JEPDSwXMQLsooPHqbxmhAcGgAAACAC7FTJeHVG/OH6Kw/fO+sblpRzXrk8T2/2xX888L6y/oGdLNPVMW/r7GvydVnHaaWKf3UJQSEAAABABNhLr1iRb68z7t+wZPvd8A+Hby4pd+4xGGzb+HfpQCEAAABABNhbs1bkSXXGH0cVZ3v2Jh/nCAAAABABhs3tzLaScWNkdNjFfnKyRwAAAMAUjQBzludLmyXeXrZGzwPvK+vndmbbpi3xz1WJa7+1ZOTyfAgBAAAATPAI0K8Z8elSx9zSEh8+eXn+4+ZnYn4p0f6tK8qXPYOTl0MDAAAApmAEGHT/wPfPaImM+c2Idz+wRASY7OwRAAAAMDUjQBw9La742bPxxb46Hvv2lWWtZ08IAAAAYJJGgH7rOsrmiPisZ25qcWgAAADAFIwACAEAAACIAAgBAAAAiAAIAQAAAIgACAEAAAAigAiAEAAAACACiAAIAQAAACIACAEAAAAiAAgBAAAAIgAIAQAAACIACAEAAAAiAAgBAAAAIgBCAAAAACIAQgAAAIAIIAIgBAAAAIgAIgBCAAAAgAgAQgAAAIAIAEIAAACACABCAAAAgAgAQgAAAIAIAEIAAACACABCAAAAgAgAQgAAAIAIAEIAAAAgAogACAEAAAAigAiAEAAAACACgBAAAAAgAoAQAAAAIAKAEAAAACACgBAAAAAgAoAQAAAAIAKAEAAAACACgBAAAACIACIACAEAAIAIIAKAEAAAAIgAIAQAAACIACAEAAAAiAAgBAAAAIgAIAQAAACIACAEAAAAiAAgBAAAAIgAIAQAAACIACAEAAAAIoAIAEIAAAAgAogAIAQAAAAiACAEAAAAIgAIAQAAACIACAEAAAAiAAgBAAAAIgAIAQAAACIACAEAAAAiAAgBAACACCACgBAAAACIACIAHBwthgAAxo+u7t5zG5HT9uWxzShb2hct+LxRBEQAQAgAgIki87pm5PR9eWipylMRcYJBBEQAYHccGgAAAIgAMIXYIwAAxpFSYmNmGevSJ/TPj4cny3X2GUFABACEAACYQBYvWjh7LMut6u79nTpjxfAXqrKp1PmbRhAQAQAhAAAmma7uNe+vs/6zodulqp7KqnHO4vNPu9voACIAIAQAwCSxdGlWLz5lzV9k5v85EgHKw3Vf39kdb1uwwQgBIgAgBADAJNHeeU/rL0xfu7LOeM9wBCjlwbpRvanj/IUPGyFABACEAACYJDo7e9qq6ZtvrDPPH4kA1X3RePacjvPOfNwIASIAIAQAwCRx7Y29x1dtcWtmvWA4AlTVHVUp51/yljN/aoQAEQDYW5UhAIDx64i2+t2jI8C2CXP+Sl03v7yqu/eG629e81qjBIgAwN4ohgAAxq/2zntaXzd9879l5qxdLVOV6pPPbo7fv/yiBRv35Xt0da/ZGJnHDc4Mrs6sP2fkgX2x+scvP/PJZ47+4K7uP77lmYH/rt88QwSAw8ihAQAwjnV1zNs6r7v3DyKyJ6I8UUpszCwv6587Dy1TZ/2eI9qq6UuX5rnLlpV6f75fFdVxkTHTyAN76xubTlxQ9zUuG9rY35nnNJ6JY1ufrddvnlGJACAEAAC70L5owedX3rb6hYvfsu3EgO2d97TOm775olJiWdY5czAGvPlFp/T+cUR8ZP++W7n30gtP/5RRB/bG4OEA7Xva3fj5rZtiRvM/RQA4zJwjAAAmgKEIEIN7CbQvWnhDKa1nRilPj1rsj4wUcJgiwJjPCdBPBAAhAADYB5e+9fXfjSgfHv5C5nGrbv3ay4wMMJ4jwIuPfPp6EQAOL4cGAMA4ct2ta59bN5uvK6WademiBX+zp+Uz++4qo+fffc+eGhHfNZLAeIsAGVGfeNSPPzSn7fFHjB4cXvYIAIBxouvm3g80m7kxI1bXGSs6O3va9vSYUrdu3GGifZSRBMZjBGg04uJzjv1ej9EDIQAAGFopZ/3dkasB1I2WFx5/1h4fU5qvHn27LuVeIwmMxwjgcAAQAgCAHSfLrX2rt9uoz3rFtTf2Hr+r5Ts7e9qyKh8Yul1KtWXdj9r+zUgCIgAgBADABDBwZYCqrByeQNc5s7Ut7rj+5jWv3XHZVbd+7WXVjON6MnPW8PIZf9bVMW+rkQREAGB3nCwQAMaRfPzHV1XTj104tIGfWZ/SF9VdK7vvXJel3N//tZJldtZ9p2bW04YeV0p1391PHPURIwiIAMCe2CMAAMaRjo7zNtfN5vmlxF0jX60bmXFq1Lm4/yOzXrB9BIi7StXydnsDACIAIAQAwESMAW87Y8Mj9y18fVWqy6OUp3e5YImn+5fpX/bSt77eJQMBEQAYE4cGAMA4tGxZqSPib5Yuzc6TXvXVl7c0GnOaWeb039coub6v2Vz/g397w7cHlwMQAQAhAAAmURDYMPhxkxEBRABgfzk0AAAAEAFACAAAAEQAEQCEAAAAQAQQAUAIAAAARABACAAAAEQAQAgAAABEAEAIAAAARABACAAAAEQAQAgAAABEAEAIAAAARABACAAAABFABACEAAAAEAFEABACAAAAEUAEACEAAAAQAQAhAAAAEAEAIQAAABABACEAAAAQAQAhAAAAEAEAIQAAABABACEAAAAQAQAhAAAARAARABACAABABBABACEAAABEAEAIAAAARABACAAAAEQAQAgAAABEAEAIAAAARABACAAAAEQAQAgAAABEAEAIAAAARABACAAAABFABACEAAAAEAFEAEAIAAAAEQBACAAAABEAEAIAAAARABACAAAAEQAQAgAAABEAEAIAAAARABACAAAAEQAQAgAAQAQQAQAhAAAARAARABACAABABAAQAgAAQAQAEAIAAEAEABACAABABACEAAAAQAQAhAAAAEAEAIQAAABABACEAAAAQAQAhAAAABABRABACAAAABFABACEAAAAEAEAhAAAABABAIQAAAAQAQCEAAAAEAEAhAAAABABAIQAAAAQAQAhAAAAEAEAIQAAAEQAEQAQAgAAQAQQAQAhAAAARAAAIQAAAEQAACEAAABEAAAhAAAARAAAIQAAAEQAgH3SYggAYPxp77yndd70n50dGa8tJV8TUX6pRDyWJe7PzAcbVXXTJW+d/1MjBSIAgBAAABNc1+fWzimNn/1DZn3KwIZGbtvcyIhTIuPN/bfqrJd1dff+bvuiBZ83YiACAOwNhwYAwHiKAN1r3htV3j8UAXa5AVLnzMi6Z+UtvR8xaiACAAgBADABdfasnRklrunfvhj6WinllqpUlzdKnN3/36jKP20fBOo/6bz5K/ONHogAAGPl0AAAGCeqZv5d1nn00K0q63dfesHCT+2w2N90dffeGBm3DAWDqjRWRcRsIwgiAMCY5hyGAAAOv2tv7D0+6/qNwyvoEv9w6YWnf2pnyw6cF6DU1wxvmGTOGtibABABAIQAAJgYWo+qXzP6djP7Vu1u+ZJx2+jbLXWcYhRBBAAQAgBggshSbbchX7XU39ltCGgc8fB2j6/zRUYRRAAAIQAAJo5vl6q6o1Tl4YiqWT+2+ae7W7iu+7Y7J0Ad8X1DCCIAwFg4WSAAjAMDx/1H9H/E0qW5xw2UEnlJDt+qmtlSvmEUQQQAEAIAYAJatqzUu7u/q3vNezPz/OEoULKn/bwFDxs5EAEAxsKhAQAwgazqXntlZFk1EgGqLXXGh40MiAAAY2WPAACYAFbetnpGNFv/ts7m+dtvsORlHRcs/PqB+j51qf/3lTfd+Z9GnMli/eYZpz3yzOMX7+y+5zSeiWNbn62/9NQvVyIAIAQAAOMnAtx05yXZjL+MzOOGvlZKtSUjL2tftPCGA/m9SuajdSn3GnUmg9U/ful5j/3s2Iv3sJgIAAgBAMD40NmzdmbVzL/Lun5j5KiN9VIebGS+6+IDuCfA8P87Gt/vuGD+140+E93g4QAfKGNcXgQAphLnCACAcei67jvfVDXz3oEIMBwAqi0R1dK7n2h71cGIADDJIoBzAgDsgj0CAGCc6erufV8z4y8i68ZIBIi7slkubn/b/PVGCA5cBOgnAgBTjT0CAGAcWXXTne+KrD8eMRQBqmZVGlc9ct/C14sAcOAjwIuPfPp6EQCYauwRAADjRNfn1s6pG3VX1EMnBChPZPRdeOmiM9YaHTiwESAj6hOP+vGH5rQ9/ojRA6YaewQAwDhRGvWfRp1HD91uNKrFHReIAHAwIkCjERefc+z3eoweMBXZIwAAxoGVt62ekX3lvBi8PECpysNRN5+5rvvON43l8c1m41GHDiACjD0CDBwOcPOa1xpBQAgAAA6L0jzigoyRkwNmnTObEbePfY1er4yIdiOJCDDGCAAwhTk0AADGgYz6pUYBRAAAIQAApowiBIAIAHBIODQAAMaBxYsWLjIKIAIAHAr2CAAAQAQAEAIAAEAEABACAABABAAQAgAAQAQAEAIAAEAEABACAABABAAQAgAAQAQAEAIAABABRAAAIQAAABFABAAQAgAAEAEAEAIAABABABACAAAQAQCEAAAAEAEAhAAAABABAIQAAAAQAQCEAAAAEAEAhAAAAEQAEQBACAAAQAQQAQCEAAAARAAAhAAAAEQAAIQAAABEAACEAAAARAAAhAAAAEQAACEAAABEAAAhAAAARAAAIQAAAEQAACEAAAARQAQAEAIAABABRAAAIQAAABEAACEAAAARAAAhAAAAEQAAIQAAABEAACEAAAARAAAhAAAAEQBACAAAABEAQAgAAEAEEAEAhAAAAEQAEQBACAAAQAQwegBCAAAAIgAAQgAAACIAAEIAAAAiAABCAAAAIgAAQgAAACIAAEIAAAAiAABCAAAAIgCAEAAAgAggAgAIAQAAiAAiAIAQAACACACAEAAAgAgAgBAAAIAIAIAQAACACACAEAAAgAgAgBAAAIAIAIAQAACACACAEAAAIAKIAAAIAQAAIoAIAIAQAAAgAogAAEIAAAAiAABCAAAAIgAAQgAAACIAAEIAAAAiAABCAAAAIgAAQgAAACIAAEIAAAAiAABCAACACCACACAEAACIACIAAEIAAIAIAABCAACACADA1NNiCABg/Lvu1rXPjbp+09DtSxYtvMmoiAAiAAD7wh4BADABZN28uJn5uaGPpUvTOlwEEAEAEAIAYDK69sbe4zPK7xgJEUAEAEAIAIApEAFa2+LWzJxlNEQAEQCAA8E5AgBgnOns7GlrfcGxs/uinBul/v3MOM6oiAAiAABCAABMQitvWz0j+1p+2Ne/idf/kcZEBBABADiwHBoAACACADCF2CMAAMaRKqdtrkvz4zu559TMeoEREgFEAACEAACYRC556/yfRsQf7Pj1ru7e90WEECACiAAA7DeHBgAAiAAACAEAAIgAAExGDg0AAIbVkY3Ozp42I3HwLN967tuzjlVViXpgyEdv8OfPz81EAACEAADg4Cn5uur5x/zYQBwc6zfPOO3kZx6/eMevv+jIn9SPPnNMtX7zDBEAACEAADiEHSDK/6qz+S9G4sD7541zzt+4te3ind33nMYz1ca+I0UAAIQAAOAQh4AsP+x42xkbjMSBNXhOgP+77OL+Ga3/Gc9miwgAwCHhZIEAAAc/AjgxIABCAACACCACACAEAABMyQjQTwQAQAgAAJgiEeDEo378QREAACEAAGCSR4CMqF985NPXn3Ps93qMHgCHgqsGAAAcxgjQaMTFc9oeb4koBhCAQ8IeAQAAhzECOBwAACEAAEAEAAAhAABABACA/eccAQAwAbQvWvBXEfFXRkIEAID9ZY8AAAARAAAhAAAAEQAAIQAAQAQQAQAQAgAARAARAAAhAABABBABABACAABEAAAQAgAARAAAEAIAAEQAABACAABEAAAQAgAARAAAEAIAAEQAABACAABEAACEAAAAEUAEAEAIAAAQAUQAAIQAAAARAACEAAAAEQAAhAAAABEAAIQAAAARAACEAAAAEQAAhAAAABEAAIQAAAARAACEAAAAEQAAhAAAQAQQAQAQAgAARAARAAAhAABABAAAIQAAQAQAACEAAEAEAAAhAABABAAAIQAAQAQAACEAAEAEAAAhAABABAAAIQAAEAFEAAAQAgAAEUAEAAAhAAAQAQBACAAAEAEAQAgAABABAEAIAAAQAQBgQmqZCr/krBV5UpXxijriqJaIb61fUr7nqQcAEQAAhIBJ5pXL8/S+iI9EHb+SJe6tIo6sM+bMXp63Z4mLH7yi/MBLAABEAACYSibtoQGzr84/bWZ8dWBCUMUvbFhS5j2wpLyyGbGgijizqmPt7GvyBC8BABABAEAImOgRYHleVyL+a5b42tHT4qzR7/x/+8qyts74RES8JOr4Cy8BABABAEAImMBOXp5XlYyLS4m+RsRvresom3dcppS4fXDG8F57BQCACAAAQsAENfvqnFsiPtb/eZ3x57s6KWAV8dBAEOj/NOM3vAwAQAQAACFgglm6NKtS4pOZ0VJK9EUj/vuuln3bk9tCwEAMyDjLywAARAAAEAImmE8/P/5LZMwZnBT844bfK0/uatnPnxRHjppA/IKXAQCIAAAgBEwg7Z33tJY6Pjh0uxFx3e6Wf2ZLnDj0eYl4gZcBAIgAADBVtOxpA7t309yXZ2vMjTpeFSXmlYxfrKuYv7Nr8M++Oj9WIi7MEjdsWFI+dKh+iTu3zD2ninjJ4M2HLnwyer+5m+VLxItHzSTavAwAQAQAgKlityvc3i1zT8+WuLBknFWVuKpkvCEiXlLquHzHZZcuzSpKtA9cli9j6aE8G3+JuHh4clDiK8uWlXp3y9clpo9a/odeBgAgAgDAVLHbPQIeWFK+HBH9HzFreX62irhtcMP7XUuX5gdGb3D3f37y8lyaER8fWJnX8Zqhxw6Fgs+cEDdExBH7+0O3TYuLhy4LOLcz2362Jc7NobJR4ot7nERkvGjUzZ94GQCACAAAQsAOHlxSemYvz9tLxtkR8ZJPP29gQ3/dDuHgE69YkY9mHZ/JKu4dfd9njo2TI+O39vsnLvHYUATot3lLnBY58nvUJe4Yw//ltaM+f9TLAABEAAAQAna+Ev7rEgMhoN+iHUPAwIZ4Hf9eIh7a8az9VUs80yzx/v39gauI7+3wM51WRiLB+t1dLSCG9kyIUZcMzPifXgYAIAIAgBCwE8+ZFqs3bYmflIxjSonzI+K/7bhMKbEgY+AQgO2sX1L6N+A/eqB/gRLxiuGJQsYX9rT8p58Xryk56qoBzfgXLwMAEAEAYKrYq8sHDu6Sf8fgmnnOrBV50k5W2O/IEqsO1S+QETNG3fzOGMrBb4y69dA7fhwPeBkAgAgAAELArtfKw++6t5T4tdF3vXJ5nl4ivruzSwseRM8d/tFKPDaG5c8ZNcH41J6uMAAAiAAAMKVDwOhd6Zt1LBx9X1/E+zPj6kP5C5SII4d/mb74990tO3BJw4xfGZ5glPg7LwEAEAEAYCpp2dsHPPC+sv7k5flYZJxYYttGdQzuDdAX8eiGK8vPnUDw5OV5VubwSQb3WZZY8+CS0rPDZOGREjFn4PMj4oe7/R/U8bvDk5ASNwyetwAAEAEAQAjYgzUR8c4SMau9857WE38wt/npiA9miYt2sfylJeKd+/3T5s+fA6CK+Pcc/Hxh67qfbNjFQ/t/zt4tcdlgUPhJVPFHnn4AEAEAQAgYy4o6447+DfvMaOndNPfl1QtiXmTcvqtzA2SJ+6qIp/b3h60iVu9k0vDtoc//ZcvcmbHD5QWHrHlm7m+XGLxaQMYf7ukygwCACAAAQsCI4d3/sxG/2qzjvUdPG3Vt/h1suKJ89GD9AlnF30cdy0rGMX3bLiX4cyFg4OoGGR/rn2Vkies3LCldnnoAEAEAYCqq9uVBC49c941Som/gRomPVSX+++ClBQ+5gXf2M1YM/jJv3/H+9s57WkvG9SXjmCxx+8Jp6y7ztAOACAAAQsBe6OqYt7WOuH/w5te/dUX57OH8JY4+Mv4sS9wSGb/1ihX59qVLc+D3mrUiT1qzZe6XSsbZGfGX73wyfr3/Z/e0A4AIAABCwN77fpb4SSPi0sP9S6zrKJvf+WRcmCWWZh1//pkT4pHZy/OeRsb/FxFPZ8S8DVeWP1y2rNSecgAQAQBgKtvXcwREyXh1Rvzh+ivHxyX4BjfyP9T/MWtFntSScdTbnoyHbPwDgAgAAOxnCHjFinx7nXH/eD3p3tDVC9Z7fgFABAAA9i8EzFqRJ9UZfxxVnG34AEAEEAEAYGLZq3MEzO3MtpJxY2R0uA4/AIgAIgAATDy73CNgzvJ8abPE28vW6HngfWX93M5s27Ql/rkqce23lpR1hg4ARAARAAAmUQhoRny61DG3tMSHT16e/7j5mZhfSrR/64ryZcMGACKACAAAE9PuVuL3D6zAM1oiY34z4t0PLBEBAEAEEAEAYCLb5R4BR0+LK372bHyxr47Hvn1lWWuoAEAEEAEAYBKHgHUdZXNEfNYQAYAIIAIAwORRGQIAQAQAACEAABABRAAAEAIAABFABAAAIQAAEAFEAACYEFoMAQAw5I6nX/q6ZjN+UwQAgMnLHgEAwID1m06sv/6TF4oAADDJ2SMAABiIAF966perMsblRQAAOHi6ute8PyIeK42tX1j8ljMfFwIAgAMbAf7zhcd86alfHvNegiIAABxcJcqbM+sF2dcSK7t774sSX4y6/h93/+jo3q6OeVv3OwSc/Fc5J1viGkMNAFNxohEv+NJTIQIAwDiVWZ8SGadExJ/Mm7F506ndvWsi6tubzfr2jredsWGfQkCjJZ7fzHiD4QUARAAAGMfqPLqOfHNEvLlUVay8Zc3DGXF7I+P2qKovXfLW+T8dUwgwkgCACAAcCs2WrY82+loNBByo9XOdMyNicTNicTSzubK7918z48vR0vjCo+tev27ZslLv7HHllcvz9GbGVw0hALCrCHD2874Tc57z2E+MxkFSyrYto8ytBsO4A0RWz42oG/v3T1y1JaP+WZTyxVJtvWr0SQeFAABgt970vO/Uc45+zCWHAWACK6XadtLB6tlPODQAABABAGDSFoB4Okr5bJV5+5ZN5Y7LLzpzoxAAAIzMFUr0ZUZLRtSvPeaH//iq5zx2t1FhssqIX8hSXlRl/r9GAxhf/z6V38nMWfu2Lq+2RNT3Z8TXqoiH6kbrv7a/5bTt1udCAAAwtFFUZ8TdUSIaVXR+6rdP/OSnDAsAHHIru3svjBh7CBjY7T/q1ZHx/9z9xFG9XR3zdnvuEyEAANg2iYioMmLLO5+MM3d1lmEAYByss6vqqYz8UmTcXhpbvzD6RIBj0dJXxTdb6niDoQQAmn3xHyIAAIw3VbOUGL404CO7uTTgmELAht8rT0bEnQYWAAAAxodSlYcz4vZGxu1bNscdl1+0YKNRAQAAgElo5W2rZxgFAAAA4IBwXWAAAACYQoQAAAAAmEKEAAAAAJhChAAAAACYQoQAAAAAmEKEAAAAAJhChAAAAACYQoQAAAAAmEKEAAAAAJhChAAAAACYQoQAAAAAmEKEAAAAAJhCWgwBHDxd3b3nNiKn7ctjm1G2tC9a8HmjeOC0d97TOm/6z86OjNeWkq+JKL9UIh7LEvdn5oONqrrpkrfO/6mROjyuu3Xtc6Ou3zR0+5JFC28yKgdHZ8/amWVrvnfo76D/a0N/C3Vf3yc73nbGBqPk9W99YH1gfgSTVzEEcBBXdDeveTwip+/TH2dVPbX4/AUnGMUD9Fx8bu2c0sh/yKxP2fWYl4czy++aYBweq7rvvLLO+MTQ7UfvX9hYtqzURubAbmxmnSvqrN+z239/SrklGlsvW/yWMx83al7/1geYH8Hk49AAYPJPOLrXvDeqvH93k75+WefMyLpn5S29HzFqh9a1N/Yen1F+x0gcPCtvWz2jruu79xQBBv4WMs+PuvWuVbd+7WVGzuvf+gBACAD2QimxMaI8MbaPqrnDJKTPCO6/zp61M6PENRF1Y+R5KbdUpbq8UeLs/v9GVf5p+7Gv/6Tz5q/MN3qHbiOotS1uzcxZRuPgaO+8pzWaR3x29BiXqjxcSnw8SvntKuOiUlUfLaU8OHpDKOutnx/YZR2vf+sDzI9gUnGOADiIFi9aOHssy63q7v2dOmPF8BeqsqnU+ZtGcP9Vzfy7rPPooVtV1u++9IKFn9phsb/p6u69MTJuGZogVqWxKiJmG8GDNCHv7GlrfcGxs/uinBul/v3MOM6oHDzzpm++KDMXjN742bqpXHz5RQs37vC8fLjMOPYTUefi2LZnwKxmXf9eRHhX1Ovf+gDzI5hM/yYaAji8urrXvL/O+tqhCUepqqeiajlj8QWn32F09s/A7rZ1/caR+UP8w6UXnv6pnS07cBxoqa8Zut2/ATTw7hEH3MrbVs8oLzhmU1/kuoh6WdgIOuhK5KjdzssTVVW9+/KLFmzccbmOjvM2P3rvgstG7xlQSly2dGmaL3j9Wx9gfgRCALC/+ifWK7vv/MvI/LPhCXdVHq77+n61/S2n3W2E9l/rUfVrRt9uZt+q3W4sZdw2+nZLHacYRSb8ZPq2f31dZpw68kIvf767s6EvW1bqUueHhjeC6pw587X/cq6RxPoA8yOYPBwaAIdBe+c9rb8wfe3KOuM9I3Pz8mDdqN7Ucf7Ch43QgZGlOiVy5KTbVUv9nd1O/BpHPJzNZ2PUBtCLjOKBV+W0zXVpfnwn95yaWS8wQgdY3bd41Kv8iXzi6b/d00O+/42F//TiU3o/MHTcetZ5WcT2G0Z4/VsfYH4EQgAwRp2dPW3V9M031pnnj6zkqvui8ew5Hee5VNcB9u1SVXdE5MuyLi+qH9u822tC13Xfdscs1hHfN4QH3uC70X+w49e7unvfFxE2hA6wkuV1Gblt46eUdZd2nLd5T49ZtqzUXbesWRMZAyGgzpxrJL3+rQ8wPwIhANgH197Ye3w1cHbokXd9+icmVSnnX/KWM39qhA6swes/D1wDeizHOJfIS3L4VtXMlvINo8hEl5knDW/MlHxkzA+syyMRQ38ROb29857Wro55W40o1geYH4EQAOyFI9rqd9e5/Ts+GfkrdV1/eVV37wNV5vKLL1j4dSN14C1bVurd3d/Vvea9ud27ENnTft4CuyEysTd+Ou9pjdg0ffuN+7EpWT+aZeT23JOeOTEi/E1gfYD5EUwCThYIh9BdTxx97eizcW+bmOfRmXFqnfV7+iLXreruveHaG3uPN1qHzqrutVdGllUjk75qS53xYSPDRDe48b7dxv2YJwjV9rtCl6xmGFGsDzA/gsnBHgEwBtfduvYtdZ3L9rRcRvxfg7sf7lRXx7yt87p7/yAieyLKE6XExszysqFL42xb79XvOaKtmr50aZ67p3ctPAd7/xyMtvK21TOi2fq3dTbP3/7/kZd1eOfhsD43HBhlSzx3dPKvqvL0WB/bbLRujL6RIwEaza0m4Exa1geHl/kRCAEwLjXr+nmRuedLB5Xy/D0t0r8htPK21S9c/JZtJ75p77yndd70zReVEsuyzpmDK7s3v+iU3j+OiI8Y/QP/HAxM+m6685Jsxl9G5nEjD6229E/62hctvMGIH77nhgMYAo7Y8h/Z1zJqo6ZMH+tjG1mf2Bz9/8ryH0aUSRkBrA/GBfMjOLQcGgCHwdBKLgYreP9Eo5TWM6Ns927dHxmpA6+zZ+3Mlbf0rs4SqyJj1KSvPNjIPM2kj8nkkXVv/FH/Bs1wCCgxc6yPzbr5ktG3my1bHzWiWB9gfgRCAEwZjaq6qWoc8ct7+uhfbl+/x6Vvff13I8rIcYiZx6269WsvM/oH7jm4rvvON1XNvDfr+o0jE77+jaRq6d1PtL3KiYjG798H+2Zg99mSwxPrknHimENAqV44arrQfGTdG39kRJksrA8mDvMjODgcGgBjMHjd572+fM11t659bt1svq6Uatalixb8zR4n3tl3Vxnd5/qePTUivusZ2PfnYEhXd+/7mhl/ETlyvGEpcVc2y8Xtb5u/3ggfvueGgyzzhxHb9gTIiP9tzA+L/KXhv5UqH3VMLpOF9cHhZ34Eh589AuBgTTRu7v1As5kbM2J1nbGis7OnbU+PKXXrxu0n4nGUkdx/q266812R9cdHTjpUNavSuOqR+xa+3qSPyd8Bqn8e+TxO6+xZu8fDAwbOzJ15zqh/i243klgfYH4Ek4c9AuAgqbL+bl1icKJRN1peePxZEXHbbh9Tmq8e/ZZbXcq9RnI/JxyfWzunbtRdUefQdOKJjL4LL110xlqjw1RQWp7tzL4jPrBtw6duNPqqD0XEe3f3mNaj44+yHjlmOvqqq40k1geYH8Ek+ls0BHBwZGvf6tG366xX7O76t52dPW1ZlQ8MT95LtWXdj9r+zUju50ZQo/7TqPPooduNRrW44wKTPqaOgZNvVfnZkX+L4r90fu4rs3e1/MrbVs/IyCtG/VvU651SrA8wP4LJxR4BcBAn3123rFkZdS4eWPHVObO1rdxx/c1rLtnxJESrbv3ayzL7OrOuZw2vKDP+rKtj3lYjue8GNmj6ynkDOxEOHOdcHo66+cx13Xe+aSyPbzYbj9oAYlJMvOvmX5eo/o/BaXejVI07u7p7L2lftODz2024b/7K/KgbN47eWKoi7Q2A9YH1gfkRCAHAmCffj//4qmr6sQszc9a2lVd9Sl9Ud63svnNdlnJ/bLs29+ys+07NrKcNPa6U6r67nzjKNXL3U2kecUHGyMmg+icbzb051rmlXhkR7UaSia7jgjPWrryl96NZ138y+NcwPTJ7Vt6y5uHI2DbxLvHa/r+RHN5tOqIq1ScvWbTA1R6wPrA+MD+CScahAXAwJ98d522um83zS4m7Rr5aNzLj1IESXufizHrB9iu5uKtULW9Xuw/ARCPqlxoF2OaRe+f/1yix3bv7Axv+mecPfNS5/UkEq/JPdz1x1GIjh/UB5kcgBAB7u7J72xkbHrlv4eurUl0epTy9ywVLPN2/TP+y266Zy/4rJn4waNmyUrcvOv2qKNV520++d/irKeXBKuOi9vMX/qYJN9YHmB/BJP1X0RDAobN0aVYnveqrL29pNOY0s8zp/1qj5Pq+ZnP9D/7tDd92nW7gUOn63No5A2firsovRpbWKPlwZnyjY4djdAHMjwAAAABgAvv/AwAA//9U19tHNNPmjwAAAABJRU5ErkJggg=="
    }
   },
   "cell_type": "markdown",
   "id": "f1e5f85c",
   "metadata": {},
   "source": [
    "ReLU\n",
    "![ReLU.png](attachment:ReLU.png)"
   ]
  },
  {
   "attachments": {
    "tanh.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "0841f02a",
   "metadata": {},
   "source": [
    "TanH\n",
    "![tanh.png](attachment:tanh.png)"
   ]
  },
  {
   "attachments": {
    "Sigmoid.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "508496a1",
   "metadata": {},
   "source": [
    "Sigmoid\n",
    "![Sigmoid.png](attachment:Sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4340",
   "metadata": {},
   "source": [
    "***6. Suppose you have an MLP composed of one input layer with 10 passthrough\n",
    "   neurons, followed by one hidden layer with 50 artificial neurons, and finally one\n",
    "   output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\n",
    "   tion function.\n",
    "    What is the shape of the input matrix X?\n",
    "    What are the shapes of the hidden layers weight vector Wh\n",
    "     and its bias vector bh?\n",
    "    What are the shapes of the output layers weight vector Wo\n",
    "     and its bias vector bo?\n",
    "    What is the shape of the networks output matrix Y?\n",
    "    Write the equation that computes the networks output matrix Y as a function\n",
    "   of X, Wh, bh, Wo, and bo.***\n",
    "   \n",
    "A: Let's break this down piece by piece keeping in mind the linear algebra needed to make this work. We start with our training data and the input data we are feeding into the 10 neurons. Let's say for example the training size is represented by size *m*.\n",
    "\n",
    "Then, that *m*X10 matrix shape is passed along into the 50 articial nerons ones in the hidden layer. With that in mind the hidden layer's weight vector would be a 10X50 matrix now and the bias vector would be of size 50 for the 50 neurons\n",
    "\n",
    "From here, we move onto the output layer weight vector. We are now feeding data out of the 50 neurons so it will be a 50 by something matrix. We note the output layer has 3 neurons. So that will our shape. 50X3 with the bias vector of length 3.\n",
    "\n",
    "The shape of the output matrix is what we started with multiplid by the output neurons. So we are left with the output matrix shape *m*X3 which makes sense with matrix multiplication for our input and output matrices. Here we have *m*X10 * 10X50 which gives us a *m*X50 matrix. Then we have a *m*X50 * 50X3 which gives us *m*X3 as our output matrix!\n",
    "\n",
    "The function would be represented as: Y* = ReLU(ReLU(X Wh + bh) Wo + bo)\n",
    "\n",
    "***7. How many neurons do you need in the output layer if you want to classify email\n",
    "   into spam or ham? What activation function should you use in the output layer?\n",
    "   If instead you want to tackle MNIST, how many neurons do you need in the out\n",
    "   put layer, and which activation function should you use? What about for getting\n",
    "   your network to predict housing prices, as in Chapter 2?***\n",
    "   \n",
    "A: One neuron. The neural would still fire and it would either return a probability for spam and the probability for ham email. A simple logistic function should work fine for this case. For the MNIST case, you need 10 neurons with a probability for each case prediction for the picture representing the corresponding digit. For the MNIST dataset, I would use a softmax function because a softmax regressor (From chapter 4) can handle multiple classes for our model. For housing prices, the output would be predicting the house price, which is one output... So this would be one output neuron and you wouldn't need an activation function because it a price that can vary widely based on the inputs fed through the network. This wouldn't be a classification problem.\n",
    "\n",
    "***8. What is backpropagation and how does it work? What is the difference between\n",
    "   backpropagation and reverse-mode autodiff?***\n",
    "   \n",
    "A: Backprop us an algorithm used to train neural networks. First, the algorithm makes a prediction on the data (Also known as forward pass) and then measures the error on that prediction with regards to neuron weights. Then, it goes backwards (hence the name) to measure the errors for each connection (reverse pass) and then finally, tweaks the weights in each layer to reduce the error (AKA Gradient Descent). The different between backprop and reverse-mode autodiff is that backprob is the whole algorithm with several backprop steps involved while training, where reverse-mode autodiff is a technique to compute the gradients effectively during gradient descent (1 step of backprop). \n",
    "\n",
    "***9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP\n",
    "   overfits the training data, how could you tweak these hyperparameters to try to\n",
    "   solve the problem?***\n",
    "   \n",
    "A: List:\n",
    "\n",
    "- number of hidden layers\n",
    "- number of neuron in hidden layers\n",
    "- activation function used in hidden layers and output layer\n",
    "\n",
    "This overfitting can be fixed by tweaking the number of hidden layers and changing (reducing) the number of neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff6c0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question #10\n",
    "'''10. Train a deep MLP on the MNIST dataset (you can load it using keras.data\n",
    "    sets.mnist.load_data(). See if you can get over 98% precision. Try searching\n",
    "    for the optimal learning rate by using the approach presented in this chapter (i.e.,\n",
    "    by growing the learning rate exponentially, plotting the loss, and finding the\n",
    "    point where the loss shoots up). Try adding all the bells and whistlessave\n",
    "    checkpoints, use early stopping, and plot learning curves using TensorBoard.'''\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72cc9de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbe4f6",
   "metadata": {},
   "source": [
    "The dataset is the same size as the set from scikit-learn with 28x28 pixel images. We also can note each pixel intensity is represented as a byte (0 to 255) for pixel ratio. We can also view the datatypes of each instance below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf64cf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c316e13",
   "metadata": {},
   "source": [
    "Let's start by splitting the dataset again into a smaller subset so we can also include a validation set for X and y and then similar to what we did with the fashion dataset, we will shrink the pixel intensity between 0 and 1 by dividing by 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0c0b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_train_sub = X_train[:5000] / 255., X_train[5000:] / 255.\n",
    "y_val, y_train_sub = y_train[:5000], y_train[5000:]\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03670fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_sub[0], cmap=\"binary\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c8ca6",
   "metadata": {},
   "source": [
    "The dataset already has labels numbered 0-9 so we don't have to worry about that. The ID is self explanatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b0e59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773894a",
   "metadata": {},
   "source": [
    "Let's plot a 3X10 array of some of the sample values we can view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c245f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x259.2 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = 3\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train_sub[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(y_train_sub[index], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d860020c",
   "metadata": {},
   "source": [
    "Let's use the keras API to build a function that can store both the learning rates and the loss measures per rate in 2 simple arrays. This is also similar to using using cyclical learning rates with a range test from Smith (2015) https://arxiv.org/pdf/1506.01186v2.pdf (Only with mine we will measure loss where Leslie N. Smith did it with accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc223412",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = [] #rates array\n",
    "        self.losses = [] # losses array\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate)) # append learning rate to array\n",
    "        self.losses.append(logs[\"loss\"]) # append loss value to array\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796be2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1017c6",
   "metadata": {},
   "source": [
    "Now let's build the model. Let us build a sequential model where we flatten the inputs to a 28X28 array and then 2 relu dense layers. Then finally an output dense layer with a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aeb7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(400, activation=\"relu\"),\n",
    "    keras.layers.Dense(150, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4878fde",
   "metadata": {},
   "source": [
    "We now compile our model with a learning rate starting at 1e-5 and we can grow it by a factor of 1% each iteration using our class object we built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e413ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])\n",
    "expon_lr = ExponentialLearningRate(factor=1.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba41271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "55000/55000 [==============================] - 5s 97us/sample - loss: nan - accuracy: 0.3104 - val_loss: nan - val_accuracy: 0.0958\n"
     ]
    }
   ],
   "source": [
    "# train the model now\n",
    "history = model.fit(X_train_sub, y_train_sub, epochs=1,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[expon_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e120818",
   "metadata": {},
   "source": [
    "As we can see, our accuracy is not very good.. Let us plot the Loss function against the learning rate regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5e90b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(expon_lr.rates, expon_lr.losses)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
    "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103fa9d6",
   "metadata": {},
   "source": [
    "It looks like our graph starts to shoot back up quite a bit after 0.5. So let us use something around that value and build a new model and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c530445",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(400, activation=\"relu\"),\n",
    "    keras.layers.Dense(150, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cfc00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.2),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc635d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\my_mnist_logs\\\\run_001'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Log to library to view later with tensorboard\n",
    "import os\n",
    "run_index = 1 # increment this at every run\n",
    "run_logdir = os.path.join(os.curdir, \"my_mnist_logs\", \"run_{:03d}\".format(run_index))\n",
    "run_logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce291de9",
   "metadata": {},
   "source": [
    "Now let's fit train our model with some model checkpoints and early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d734329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 3s 52us/sample - loss: 0.2761 - accuracy: 0.9167 - val_loss: 0.1319 - val_accuracy: 0.9646\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.1061 - accuracy: 0.9681 - val_loss: 0.0816 - val_accuracy: 0.9766\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.0705 - accuracy: 0.9784 - val_loss: 0.1414 - val_accuracy: 0.9526\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0772 - val_accuracy: 0.9764\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 0.0367 - accuracy: 0.9890 - val_loss: 0.1233 - val_accuracy: 0.9564\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.0269 - accuracy: 0.9919 - val_loss: 0.0712 - val_accuracy: 0.9808\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 2s 37us/sample - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.0735 - val_accuracy: 0.9804\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0132 - accuracy: 0.9963 - val_loss: 0.0651 - val_accuracy: 0.9814\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.0631 - val_accuracy: 0.9834\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0642 - val_accuracy: 0.9840\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.0694 - val_accuracy: 0.9828\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.0664 - val_accuracy: 0.9840\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 2s 37us/sample - loss: 0.0018 - accuracy: 0.9998 - val_loss: 0.0673 - val_accuracy: 0.9852\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.0688 - val_accuracy: 0.9848\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 9.7120e-04 - accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9846\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 8.5000e-04 - accuracy: 1.0000 - val_loss: 0.0703 - val_accuracy: 0.9846\n",
      "Epoch 17/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 7.3953e-04 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 0.9848\n",
      "Epoch 18/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 6.5566e-04 - accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9848\n",
      "Epoch 19/100\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 5.8955e-04 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9852\n",
      "Epoch 20/100\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 5.3766e-04 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9844\n",
      "Epoch 21/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 4.9789e-04 - accuracy: 1.0000 - val_loss: 0.0720 - val_accuracy: 0.9848\n",
      "Epoch 22/100\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 4.6059e-04 - accuracy: 1.0000 - val_loss: 0.0721 - val_accuracy: 0.9848\n",
      "Epoch 23/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 4.2422e-04 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9848\n",
      "Epoch 24/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 4.0126e-04 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9850\n",
      "Epoch 25/100\n",
      "55000/55000 [==============================] - 2s 37us/sample - loss: 3.7617e-04 - accuracy: 1.0000 - val_loss: 0.0733 - val_accuracy: 0.9850\n",
      "Epoch 26/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 3.5414e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9850\n",
      "Epoch 27/100\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 3.3557e-04 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9848\n",
      "Epoch 28/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 3.1760e-04 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9850\n",
      "Epoch 29/100\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 3.0167e-04 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9848\n",
      "Epoch 30/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 2.8725e-04 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9848\n",
      "Epoch 31/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 2.7522e-04 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9848\n",
      "Epoch 32/100\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 2.6221e-04 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 0.9852\n",
      "Epoch 33/100\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 2.5073e-04 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9846\n",
      "Epoch 34/100\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 2.4015e-04 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=25)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_mnist_model.h5\", save_best_only=True)\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit(X_train_sub, y_train_sub, epochs=100, batch_size=64,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08af1cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0578 - accuracy: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05778030508286611, 0.9833]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_mnist_model.h5\") # rollback to best model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2f187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
