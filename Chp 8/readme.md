This chapter focuses on dimensionality reduction and how to bring extremely high dimensional data down to an interpretable vector space. This chapter talks about methods of PCA and randomized PCA to assist with dimensionality reduction and also provides great examples of the "Curse of Dimensionalty" and how humans have a tough time trying to visualize shapes and objects greater than 3 dimensions. There is a really good example with a 4D Cube and it's respective youtube video. We first discuss linear projections of data in a 2-D space to make it interpretable from high dimensionality and then move onto how to preserve variance when you shrink the dimensionality of data significantly. Typically, you will most likely sacrifice accuracy from dimensionality reduction, but if done correctly it can be minimal. It also may be impossible to train a dataset that has 60,000 feature per each data item. If there are 10 million instances, this problem now becomes a 10,000,000 x 60,000 problem... this may even fail to run without a supercomputer. This chapter ends with discussing Kernel PCA and how to select a kernel and associated hyperparameters when trying to train this type of model and then a short section on other dimensionality reduction techniques.
More of the contents of this chapter are shown in the jupyter notebook for this chapter.
