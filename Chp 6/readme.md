This chapter is all about decision trees. Decision trees are awesome for machine learning because they are very easy to understand and interpret and the visualization of decision trees are very sound logically. They can be used for classification problems as the nodes are paired in a O(log<sub>2</sub>n) fashion for computation and work very well in this regard. Decision trees can also be used for regression if needed but typically is better for classification problems. There is also a section on the CART (Classification and Regression Tree) Algorithm which is the algorithm for how decision trees are trained with a specified threshold for 2 starting subsets in the training set. There is also a section on hyperparamters and the differences between using Gini impurity or entropy inpurity for model selection. More details of the contents of this chapter can be found in the jupyter notebook in this folder for this chapter.
