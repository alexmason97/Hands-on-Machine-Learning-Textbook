This chapter is all about Support Vector Machines, the logic behind them and how they work for classification and regression problems. The chapter opens up talking about how we can view SVMs like trying to place a street that equally seperates data. Almost like a roadblock seperating one group of data from another. There is a discussion of soft margin classifications where the street isn't perfect as some data may cross over based on the characteristics and features of the data points in the vector space and how a soft margin can help with these outliers. The chapter also includes a useful sections on kernels and kernalized SVMs for computations and how to get sound results with your model when using this tool in your belt. SVM Regression is like the opposite of SVMs where the street is actually containing the data as opposed to seperating it as well. More details of the contents of this chapter can be found inside of the jupyter notebook in this folder. 
