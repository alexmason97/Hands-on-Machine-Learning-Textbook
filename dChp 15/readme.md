Chapter 15 is an extension of chapter 14 with CNNs, only now we are working with process sequencing and we use RNNs (Recurrent Neural Networks) and CNNs (Convolutional Neural Networks) to achieve this. We start by discussing what a recurrent neuron is and how it can be stacked in layers to acheive an RNN. Then the book discusses training RNNs with the backpropagation through time algorithm and how it can be computationally expensive and gives an example with time series predictions. We look at types of RNNs like LSTMs and GRUs. We end off the chapter by looking at tackling unstable gradients with RNNs and how CNNs can help with these types of problems better than some RNNs can with architectures like *WaveNet*.
The Jupyter notebook for this chapter discusses the content mentioned above in greater detail with code examples. 
